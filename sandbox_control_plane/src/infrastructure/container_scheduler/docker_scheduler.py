"""
Docker å®¹å™¨è°ƒåº¦å™¨

ä½¿ç”¨ aiodocker å®ç° Docker å®¹å™¨çš„åˆ›å»ºå’Œç®¡ç†ã€‚

æ”¯æŒ S3 workspace æŒ‚è½½ï¼šå½“ workspace_path ä»¥ s3:// å¼€å¤´æ—¶ï¼Œ
å®¹å™¨ä¼šé€šè¿‡ s3fs å°† S3 bucket æŒ‚è½½åˆ° /workspace ç›®å½•ã€‚

æ”¯æŒ Python ä¾èµ–å®‰è£…ï¼šæŒ‰ç…§ sandbox-design-v2.1.md ç« èŠ‚ 5 è®¾è®¡ã€‚
"""
import asyncio
import json
import os
from typing import Optional, List
from urllib.parse import urlparse

from aiodocker import Docker
from aiodocker.exceptions import DockerError

from src.infrastructure.container_scheduler.base import (
    IContainerScheduler,
    ContainerConfig,
    ContainerInfo,
    ContainerResult,
)
from src.infrastructure.config.settings import get_settings
from src.infrastructure.logging import get_logger

logger = get_logger(__name__)


class DockerScheduler(IContainerScheduler):
    """
    Docker å®¹å™¨è°ƒåº¦å™¨

    é€šè¿‡ Docker socket æˆ– TCP è¿æ¥ Docker daemonï¼Œç®¡ç†å®¹å™¨ç”Ÿå‘½å‘¨æœŸã€‚
    """

    def __init__(self, docker_url: str = "unix:///var/run/docker.sock"):
        """
        åˆå§‹åŒ– Docker è°ƒåº¦å™¨

        Args:
            docker_url: Docker daemon è¿æ¥URL
                - unix:///var/run/docker.sock (Unix socket)
                - tcp://localhost:2375 (TCP)
        """
        self._docker_url = docker_url
        self._docker: Optional[Docker] = None
        self._initialized = False

    async def _ensure_docker(self) -> Docker:
        """ç¡®ä¿ Docker å®¢æˆ·ç«¯å·²åˆå§‹åŒ–"""
        if not self._initialized:
            self._docker = Docker(url=self._docker_url)
            self._initialized = True
        return self._docker

    async def close(self) -> None:
        """å…³é—­ Docker è¿æ¥"""
        if self._docker:
            await self._docker.close()
            self._initialized = False

    def _parse_s3_workspace(self, workspace_path: str) -> Optional[dict]:
        """
        è§£æ S3 workspace è·¯å¾„

        Args:
            workspace_path: S3 è·¯å¾„ï¼Œæ ¼å¼: s3://bucket/sessions/{session_id}/

        Returns:
            åŒ…å« bucket, prefix çš„å­—å…¸ï¼Œå¦‚æœä¸æ˜¯ S3 è·¯å¾„åˆ™è¿”å› None
        """
        if not workspace_path or not workspace_path.startswith("s3://"):
            return None

        parsed = urlparse(workspace_path)
        return {
            "bucket": parsed.netloc,
            "prefix": parsed.path.lstrip('/'),
        }

    def _build_s3_mount_entrypoint(
        self,
        s3_bucket: str,
        s3_prefix: str,
        s3_endpoint_url: str,
        s3_access_key: str,
        s3_secret_key: str,
        dependencies: Optional[List[str]] = None,
    ) -> str:
        """
        æ„å»ºå®¹å™¨å¯åŠ¨è„šæœ¬ï¼Œç”¨äºæŒ‚è½½ S3 bucket å¹¶å®‰è£…ä¾èµ–

        Args:
            s3_bucket: S3 bucket åç§°
            s3_prefix: S3 è·¯å¾„å‰ç¼€
            s3_endpoint_url: S3 ç«¯ç‚¹ URL
            s3_access_key: S3 è®¿é—®å¯†é’¥ ID
            s3_secret_key: S3 è®¿é—®å¯†é’¥
            dependencies: pip åŒ…è§„èŒƒåˆ—è¡¨ï¼ˆå¦‚ ["requests==2.31.0", "pandas>=2.0"]ï¼‰

        Returns:
            Shell è„šæœ¬å­—ç¬¦ä¸²

        å·¥ä½œåŸç†:
        1. æŒ‚è½½ S3 bucket åˆ° /workspace/s3-root
        2. åˆ›å»ºç¬¦å·é“¾æ¥ /workspace -> /workspace/s3-root/sessions/{session_id}
        3. å®‰è£…ä¾èµ–åˆ° /workspace/.venv/ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        4. ä½¿ç”¨ gosu åˆ‡æ¢åˆ° sandbox ç”¨æˆ·è¿è¡Œ executor
        """
        # å¯¹äº MinIOï¼Œéœ€è¦ä½¿ç”¨ use_path_request_style
        path_style_option = "-o use_path_request_style" if s3_endpoint_url else ""

        # ä¾èµ–å®‰è£…è„šæœ¬ç‰‡æ®µ
        dependency_install_script = ""
        if dependencies:
            # è½¬æ¢ä¾èµ–æ ¼å¼ï¼š[{"name": "requests", "version": "==2.31.0"}] -> ["requests==2.31.0"]
            pip_specs = []
            for dep in dependencies:
                if isinstance(dep, dict):
                    name = dep.get("name", "")
                    version = dep.get("version", "")
                    if version:
                        pip_specs.append(f"{name}{version}")
                    else:
                        pip_specs.append(name)
                elif isinstance(dep, str):
                    pip_specs.append(dep)

            deps_json = json.dumps(dependencies)
            deps_list = " ".join(f'"{spec}"' for spec in pip_specs)
            dependency_install_script = f"""
# ========== å®‰è£… Python ä¾èµ– ==========
echo "ğŸ“¦ Installing dependencies: {deps_json}"
echo "ğŸ“¦ Pip specs: {pip_specs}"
mkdir -p /workspace/.venv/

if pip3 install \\
    --target /workspace/.venv/ \\
    --isolated \\
    --no-warn-script-location \\
    --disable-pip-version-check \\
    --index-url https://pypi.org/simple/ \\
    {deps_list}; then
    echo "âœ… Dependencies installed successfully"
    # ä¿®æ”¹å±ä¸»ä¸º sandbox ç”¨æˆ·ï¼ˆgosu åˆ‡æ¢å‰ä»¥ root å®‰è£…ï¼‰
    chown -R sandbox:sandbox /workspace/.venv/
else
    echo "âŒ Failed to install dependencies"
    exit 1
fi
"""

        return f"""#!/bin/bash
set -e

# åˆ›å»º s3fs å‡­è¯æ–‡ä»¶
echo "{s3_access_key}:{s3_secret_key}" > /tmp/.passwd-s3fs
chmod 600 /tmp/.passwd-s3fs

# 1. åˆ›å»º S3 æŒ‚è½½ç‚¹ï¼ˆæ³¨æ„ï¼šä¸æ˜¯ç›´æ¥æŒ‚åˆ° /workspaceï¼‰
echo "Mounting S3 bucket {s3_bucket}..."
mkdir -p /mnt/s3-root
s3fs {s3_bucket} /mnt/s3-root \\
    -o passwd_file=/tmp/.passwd-s3fs \\
    -o url={s3_endpoint_url or "https://s3.amazonaws.com"} \\
    {path_style_option} \\
    -o allow_other \\
    -o umask=000

# 2. åˆ›å»º session workspace ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
SESSION_PATH="/mnt/s3-root/{s3_prefix}"
echo "Ensuring session workspace exists: $SESSION_PATH"
mkdir -p "$SESSION_PATH"

# 3. å°† /workspace ç§»åŠ¨åˆ°ä¸´æ—¶ä½ç½®
mv /workspace /workspace-old 2>/dev/null || true

# 4. åˆ›å»ºç¬¦å·é“¾æ¥ä» /workspace åˆ° session ç›®å½•
ln -s "$SESSION_PATH" /workspace

# 5. éªŒè¯ç¬¦å·é“¾æ¥
echo "Workspace symlink: $(ls -la /workspace)"

# ========== âœ… æ–°å¢ï¼šå®‰è£…ä¾èµ– ==========
{dependency_install_script}

# 6. ä½¿ç”¨ gosu åˆ‡æ¢åˆ° sandbox ç”¨æˆ·è¿è¡Œ executor
# é€šè¿‡ bash -c åœ¨ gosu ä¹‹åè®¾ç½®ç¯å¢ƒå˜é‡
echo "Starting sandbox executor as sandbox user..."
exec gosu sandbox bash -c 'export PYTHONPATH=/app:/workspace/.venv:/workspace; export SANDBOX_VENV_PATH=/workspace/.venv/; exec python -m executor.interfaces.http.rest'
"""

    def _build_dependency_install_entrypoint(
        self,
        dependencies: Optional[List[str]] = None,
    ) -> str:
        """
        æ„å»ºä¾èµ–å®‰è£…è„šæœ¬ï¼ˆé S3 æ¨¡å¼ï¼‰

        Args:
            dependencies: pip åŒ…è§„èŒƒåˆ—è¡¨ï¼ˆå¦‚ ["requests==2.31.0", "pandas>=2.0"]ï¼‰

        Returns:
            Shell è„šæœ¬å­—ç¬¦ä¸²

        å·¥ä½œåŸç†:
        1. ä»¥ sandbox ç”¨æˆ·è¿è¡Œ
        2. å®‰è£…ä¾èµ–åˆ° /workspace/.venv/
        3. å¯åŠ¨ executor
        """
        # ä¾èµ–å®‰è£…è„šæœ¬ç‰‡æ®µ
        dependency_install_script = ""
        if dependencies:
            # è½¬æ¢ä¾èµ–æ ¼å¼ï¼š[{"name": "requests", "version": "==2.31.0"}] -> ["requests==2.31.0"]
            pip_specs = []
            for dep in dependencies:
                if isinstance(dep, dict):
                    name = dep.get("name", "")
                    version = dep.get("version", "")
                    if version:
                        pip_specs.append(f"{name}{version}")
                    else:
                        pip_specs.append(name)
                elif isinstance(dep, str):
                    pip_specs.append(dep)

            deps_json = json.dumps(dependencies)
            deps_list = " ".join(f'"{spec}"' for spec in pip_specs)
            dependency_install_script = f"""
# ========== å®‰è£… Python ä¾èµ– ==========
echo "ğŸ“¦ Installing dependencies: {deps_json}"
echo "ğŸ“¦ Pip specs: {pip_specs}"
mkdir -p /workspace/.venv/

if pip3 install \\
    --target /workspace/.venv/ \\
    --isolated \\
    --no-warn-script-location \\
    --disable-pip-version-check \\
    --index-url https://pypi.org/simple/ \\
    {deps_list}; then
    echo "âœ… Dependencies installed successfully"
else
    echo "âŒ Failed to install dependencies"
    exit 1
fi
"""

        return f"""#!/bin/bash
set -e

echo "ğŸš€ Starting sandbox executor (non-S3 mode)..."

# ========== å®‰è£…ä¾èµ– ==========
{dependency_install_script}

# å¯åŠ¨ executor
echo "ğŸ¯ Starting executor daemon..."
exec python -m executor.interfaces.http.rest
"""

    async def create_container(self, config: ContainerConfig) -> str:
        """
        åˆ›å»º Docker å®¹å™¨

        å®¹å™¨é…ç½®ï¼š
        - NetworkMode: sandbox_network (å®¹å™¨ç½‘ç»œï¼Œç”¨äº executor é€šä¿¡)
        - CAP_DROP: ALL (ç§»é™¤æ‰€æœ‰ç‰¹æƒ)
        - CAP_ADD: SYS_ADMIN (ä»…å½“ä½¿ç”¨ S3 workspace æ—¶éœ€è¦ï¼Œç”¨äº FUSE æŒ‚è½½)
        - SecurityOpt: no-new-privileges (ç¦æ­¢è·å–æ–°æƒé™)
        - User: 1000:1000 (éç‰¹æƒç”¨æˆ·)
        - ReadonlyRootfs: false (éœ€è¦å†™å…¥å·¥ä½œç©ºé—´)

        S3 Workspace æŒ‚è½½ï¼š
        å½“ workspace_path ä»¥ s3:// å¼€å¤´æ—¶ï¼Œå®¹å™¨ä¼šé€šè¿‡ s3fs å°† S3 bucket æŒ‚è½½åˆ° /workspaceï¼š
        - æ·»åŠ  /dev/fuse è®¾å¤‡ï¼ˆFUSE éœ€è¦ï¼‰
        - æ·»åŠ  SYS_ADMIN capabilityï¼ˆFUSE æŒ‚è½½éœ€è¦ï¼‰
        - åˆ›å»º entrypoint è„šæœ¬ï¼Œåœ¨å¯åŠ¨ executor ä¹‹å‰å…ˆæŒ‚è½½ S3
        - å®¹å™¨å¯åŠ¨åè‡ªåŠ¨ cd åˆ° workspace å­ç›®å½•
        """
        docker = await self._ensure_docker()

        # è§£æèµ„æºé™åˆ¶
        cpu_quota = int(float(config.cpu_limit) * 100000)
        memory_bytes = self._parse_memory_to_bytes(config.memory_limit)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦ S3 workspace æŒ‚è½½
        s3_workspace = self._parse_s3_workspace(config.workspace_path)
        use_s3_mount = s3_workspace is not None

        # åŸºç¡€ç¯å¢ƒå˜é‡
        env_vars = dict(config.env_vars)

        # åŸºç¡€å®¹å™¨é…ç½®
        container_config = {
            "Image": config.image,
            "Hostname": config.name,
            "Env": [f"{k}={v}" for k, v in env_vars.items()],
            "HostConfig": {
                "NetworkMode": config.network_name,
                # é»˜è®¤é…ç½®ï¼ŒS3 mount æ¨¡å¼ä¼šè¦†ç›–
                "CpuQuota": cpu_quota,
                "CpuPeriod": 100000,
                "Memory": memory_bytes,
                "MemorySwap": memory_bytes,
            },
            "Labels": config.labels,
            "ExposedPorts": {
                "8080/tcp": {}
            },
        }

        # å¦‚æœä¸ä½¿ç”¨ S3 workspaceï¼Œä¿æŒåŸæœ‰å®‰å…¨é…ç½®
        # æ³¨æ„: Bubblewrap éœ€è¦ç”¨æˆ·å‘½åç©ºé—´æ”¯æŒï¼Œå¦‚æœé‡åˆ°æƒé™é”™è¯¯ï¼š
        # 1. åœ¨å®¿ä¸»æœºå¯ç”¨: sudo sysctl -w kernel.unprivileged_userns_clone=1
        # 2. æˆ–è€…è®¾ç½®ç¯å¢ƒå˜é‡ DISABLE_BWRAP=true æ¥ç¦ç”¨ bubblewrap
        if not use_s3_mount:
            # ä» config.labels ä¸­æå–ä¾èµ–åˆ—è¡¨
            dependencies_json = config.labels.get("dependencies", "")
            dependencies = json.loads(dependencies_json) if dependencies_json else None

            # æ·»åŠ  PYTHONPATH ç¯å¢ƒå˜é‡ä»¥æ”¯æŒä¾èµ–å¯¼å…¥
            if dependencies:
                container_config["Env"].append("PYTHONPATH=/workspace/.venv/:/workspace:$PYTHONPATH")
                container_config["Env"].append("SANDBOX_VENV_PATH=/workspace/.venv/")

                # å¦‚æœæœ‰ä¾èµ–ï¼Œä½¿ç”¨åŠ¨æ€ entrypoint è„šæœ¬
                entrypoint_script = self._build_dependency_install_entrypoint(
                    dependencies=dependencies,
                )
                container_config["Entrypoint"] = ["/bin/sh", "-c"]
                container_config["Cmd"] = [entrypoint_script]

                logger.info(
                    f"Configuring dependency installation for {config.name}: "
                    f"dependencies={len(dependencies)}"
                )

            container_config["HostConfig"]["CapDrop"] = ["ALL"]
            container_config["HostConfig"]["SecurityOpt"] = ["no-new-privileges"]
            # æ·»åŠ  seccomp é…ç½®ä»¥å…è®¸ç”¨æˆ·å‘½åç©ºé—´
            container_config["HostConfig"]["SecurityOpt"].append("seccomp=default")
            container_config["HostConfig"]["User"] = "1000:1000"

        # å¦‚æœä½¿ç”¨ S3 workspace æŒ‚è½½ï¼Œæ·»åŠ å¿…è¦çš„é…ç½®
        if use_s3_mount:
            settings = get_settings()

            # æ–°å¢ï¼šä» config.labels ä¸­æå–ä¾èµ–åˆ—è¡¨
            dependencies_json = config.labels.get("dependencies", "")
            dependencies = json.loads(dependencies_json) if dependencies_json else None

            # ä»¥ root ç”¨æˆ·å¯åŠ¨ï¼ˆè¦†ç›– Dockerfile ä¸­çš„ USER sandboxï¼‰
            # è¿™æ · entrypoint è„šæœ¬å¯ä»¥ä»¥ root æ‰§è¡Œ s3fs æŒ‚è½½
            container_config["User"] = "root"

            # æ·»åŠ  SYS_ADMIN capabilityï¼ˆFUSE éœ€è¦ï¼‰
            container_config["HostConfig"]["CapAdd"] = ["SYS_ADMIN"]

            # æ·»åŠ  /dev/fuse è®¾å¤‡
            container_config["HostConfig"]["Devices"] = [
                {
                    "PathOnHost": "/dev/fuse",
                    "PathInContainer": "/dev/fuse",
                    "CgroupPermissions": "rwm"
                }
            ]

            # æ·»åŠ  tmpfs ç”¨äº s3fs ç¼“å­˜
            container_config["HostConfig"]["Tmpfs"] = {
                "/tmp": "size=100M,mode=1777"
            }

            # æ·»åŠ  S3 ç›¸å…³ç¯å¢ƒå˜é‡
            s3_env_vars = {
                "S3_BUCKET": s3_workspace["bucket"],
                "S3_PREFIX": s3_workspace["prefix"],
                "S3_ENDPOINT_URL": settings.s3_endpoint_url or "https://s3.amazonaws.com",
                "S3_REGION": settings.s3_region,
                "WORKSPACE_MOUNT_POINT": "/workspace",
                "WORKSPACE_PATH": "/workspace",  # å‘Šè¯‰ executor ä½¿ç”¨æœ¬åœ°æŒ‚è½½ç‚¹
            }
            for k, v in s3_env_vars.items():
                container_config["Env"].append(f"{k}={v}")

            # æ–°å¢ï¼šæ·»åŠ  PYTHONPATH ç¯å¢ƒå˜é‡ä»¥æ”¯æŒä¾èµ–å¯¼å…¥
            # /app å¿…é¡»åœ¨æœ€å‰é¢ï¼Œä»¥ä¾¿ executor æ¨¡å—èƒ½è¢«æ‰¾åˆ°
            if dependencies:
                container_config["Env"].append("PYTHONPATH=/app:/workspace/.venv/:/workspace")
                container_config["Env"].append("SANDBOX_VENV_PATH=/workspace/.venv/")

            # ä¿®æ”¹ï¼šä¼ é€’ä¾èµ–åˆ—è¡¨åˆ° entrypoint è„šæœ¬
            entrypoint_script = self._build_s3_mount_entrypoint(
                s3_bucket=s3_workspace["bucket"],
                s3_prefix=s3_workspace["prefix"],
                s3_endpoint_url=settings.s3_endpoint_url or "",
                s3_access_key=settings.s3_access_key_id,
                s3_secret_key=settings.s3_secret_access_key,
                dependencies=dependencies,  # æ–°å¢å‚æ•°
            )
            container_config["Entrypoint"] = ["/bin/sh", "-c"]
            container_config["Cmd"] = [entrypoint_script]

            logger.info(
                f"Configuring S3 workspace mount for {config.name}: "
                f"bucket={s3_workspace['bucket']}, prefix={s3_workspace['prefix']}, "
                f"dependencies={len(dependencies) if dependencies else 0}"
            )

        try:
            container = await docker.containers.create(container_config, name=config.name)
            logger.info(
                f"Created container {container.id} for session {config.name} "
                f"on network {config.network_name} (S3 mount: {use_s3_mount})"
            )
            return container.id
        except DockerError as e:
            logger.error(f"Failed to create container: {e}")
            raise

    async def start_container(self, container_id: str) -> None:
        """å¯åŠ¨å®¹å™¨"""
        docker = await self._ensure_docker()
        try:
            container = docker.containers.container(container_id)
            await container.start()
            logger.info(f"Started container {container_id}")
        except DockerError as e:
            logger.error(f"Failed to start container {container_id}: {e}")
            raise

    async def stop_container(
        self,
        container_id: str,
        timeout: int = 10
    ) -> None:
        """åœæ­¢å®¹å™¨"""
        docker = await self._ensure_docker()
        try:
            container = docker.containers.container(container_id)
            await container.stop(timeout=timeout)
            logger.info(f"Stopped container {container_id}")
        except DockerError as e:
            logger.error(f"Failed to stop container {container_id}: {e}")
            raise

    async def remove_container(
        self,
        container_id: str,
        force: bool = True
    ) -> None:
        """åˆ é™¤å®¹å™¨"""
        docker = await self._ensure_docker()
        try:
            container = docker.containers.container(container_id)
            await container.delete(force=force)
            logger.info(f"Removed container {container_id}")
        except DockerError as e:
            logger.warning(f"Failed to remove container {container_id}: {e}")

    async def get_container_status(self, container_id: str) -> ContainerInfo:
        """è·å–å®¹å™¨çŠ¶æ€"""
        docker = await self._ensure_docker()
        try:
            container = docker.containers.container(container_id)
            info = await container.show()

            status = info["State"]["Status"]
            if status == "running":
                # Docker å¯èƒ½è¿”å›è¿è¡Œä¸­ï¼Œä½†å®é™…ä¸Šæ˜¯ paused
                if info["State"].get("Paused", False):
                    status = "paused"
            elif status == "exited":
                # å¯ä»¥æ ¹æ® exit_code åˆ¤æ–­æ˜¯ completed/failed
                pass

            return ContainerInfo(
                id=container_id,
                name=info["Name"].lstrip("/"),
                image=info["Config"]["Image"],
                status=status,
                ip_address=info["NetworkSettings"].get("IPAddress"),
                created_at=info["Created"],
                started_at=info["State"].get("StartedAt"),
                exited_at=info["State"].get("FinishedAt"),
                exit_code=info["State"].get("ExitCode"),
            )
        except DockerError as e:
            logger.error(f"Failed to get container status {container_id}: {e}")
            raise

    async def is_container_running(self, container_id: str) -> bool:
        """
        æ£€æŸ¥å®¹å™¨æ˜¯å¦æ­£åœ¨è¿è¡Œ

        ç›´æ¥é€šè¿‡ Docker API æŸ¥è¯¢ï¼Œä¸ä¾èµ–æ•°æ®åº“ã€‚
        æ­¤æ–¹æ³•ä¾› StateSyncService ä½¿ç”¨ã€‚

        Args:
            container_id: å®¹å™¨ ID

        Returns:
            bool: å®¹å™¨æ˜¯å¦è¿è¡Œä¸­
        """
        try:
            container_info = await self.get_container_status(container_id)
            return container_info.status == "running"
        except Exception as e:
            logger.warning(f"Failed to check container {container_id} status: {e}")
            return False

    async def get_container_logs(
        self,
        container_id: str,
        tail: int = 100,
        since: Optional[str] = None
    ) -> str:
        """è·å–å®¹å™¨æ—¥å¿—"""
        docker = await self._ensure_docker()
        try:
            container = docker.containers.container(container_id)
            # æ„å»ºæ—¥å¿—å‚æ•°
            params = {"stdout": True, "stderr": True, "tail": tail}
            if since:
                params["since"] = since
            logs = await container.log(**params)
            return "".join(logs)
        except DockerError as e:
            logger.error(f"Failed to get logs for container {container_id}: {e}")
            raise

    async def wait_container(
        self,
        container_id: str,
        timeout: Optional[int] = None
    ) -> ContainerResult:
        """ç­‰å¾…å®¹å™¨æ‰§è¡Œå®Œæˆ"""
        docker = await self._ensure_docker()
        try:
            container = docker.containers.container(container_id)

            if timeout:
                # ä½¿ç”¨ asyncio.wait_for å®ç°è¶…æ—¶
                result = await asyncio.wait_for(
                    container.wait(),
                    timeout=timeout
                )
            else:
                result = await container.wait()

            exit_code = result["StatusCode"]
            status = "completed" if exit_code == 0 else "failed"

            # è·å–æ—¥å¿—
            logs = await self.get_container_logs(container_id, tail=-1)

            return ContainerResult(
                status=status,
                stdout=logs,
                stderr="",
                exit_code=exit_code,
            )
        except asyncio.TimeoutError:
            logger.warning(f"Container {container_id} timed out")
            return ContainerResult(
                status="timeout",
                stdout="",
                stderr=f"Container execution timed out after {timeout}s",
                exit_code=124,
            )
        except DockerError as e:
            logger.error(f"Failed to wait for container {container_id}: {e}")
            raise

    async def ping(self) -> bool:
        """æ£€æŸ¥ Docker è¿æ¥çŠ¶æ€"""
        try:
            docker = await self._ensure_docker()
            # å°è¯•è·å– Docker ç‰ˆæœ¬ä¿¡æ¯æ¥éªŒè¯è¿æ¥
            version = await docker.version()
            return version is not None
        except Exception as e:
            logger.error(f"Docker ping failed: {e}")
            return False

    def _parse_memory_to_bytes(self, value: str) -> int:
        """
        è§£æå†…å­˜é™åˆ¶ä¸ºå­—èŠ‚æ•°

        Args:
            value: å¦‚ "512Mi", "1Gi"

        Returns:
            å­—èŠ‚æ•°
        """
        value = value.strip()
        if value.endswith("Gi") or value.endswith("GB") or value.endswith("G"):
            return int(float(value[:-2]) * 1024 * 1024 * 1024)
        elif value.endswith("Mi") or value.endswith("MB") or value.endswith("M"):
            return int(float(value[:-2]) * 1024 * 1024)
        elif value.endswith("Ki") or value.endswith("KB") or value.endswith("K"):
            return int(float(value[:-2]) * 1024)
        else:
            # é»˜è®¤ä¸º MB
            return int(float(value) * 1024 * 1024)

    def _parse_disk_to_bytes(self, value: str) -> int:
        """è§£æç£ç›˜é™åˆ¶ä¸ºå­—èŠ‚æ•°"""
        return self._parse_memory_to_bytes(value)
