# 3. å…³é”®æµç¨‹è®¾è®¡ + 4. æ•°æ®æ¨¡å‹è®¾è®¡


> **æ–‡æ¡£å¯¼èˆª**: [è¿”å›é¦–é¡µ](index.md)


## 3. å…³é”®æµç¨‹è®¾è®¡
### 3.1 ä¼šè¯åˆ›å»ºæµç¨‹
![alt text](image-2.png)
```mermaid
sequenceDiagram
    participant Agent as AI Agent
    participant API as API Gateway
    participant SessionMgr as ä¼šè¯ç®¡ç†å™¨
    participant Scheduler as è°ƒåº¦å™¨
    participant WarmPool as é¢„çƒ­æ± 
    participant Runtime as è¿è¡Œæ—¶
    participant Container as å®¹å™¨å®ä¾‹
    
    Agent->>API: POST /sessions (template_id, resources)
    API->>SessionMgr: create_session()
    SessionMgr->>Scheduler: schedule(request)
    
    Scheduler->>WarmPool: acquire(template_id)
    alt é¢„çƒ­å®ä¾‹å¯ç”¨
        WarmPool-->>Scheduler: è¿”å›é¢„çƒ­å®ä¾‹
        Scheduler-->>SessionMgr: è¿”å›å®¹å™¨èŠ‚ç‚¹
    else æ— é¢„çƒ­å®ä¾‹
        Scheduler->>Runtime: é€‰æ‹©æœ€ä¼˜èŠ‚ç‚¹
        Runtime-->>Scheduler: è¿”å›èŠ‚ç‚¹ä¿¡æ¯
    end
    
    SessionMgr->>Runtime: create_container(session)
    Runtime->>Container: docker run / kubectl create pod
    Container-->>Runtime: å®¹å™¨å¯åŠ¨æˆåŠŸ
    Runtime-->>SessionMgr: è¿”å›å®¹å™¨ ID

    SessionMgr->>MariaDB: INSERT INTO sessions ...
    MariaDB-->>SessionMgr: ç¡®è®¤ä¿å­˜
    SessionMgr-->>API: è¿”å› session_id
    API-->>Agent: è¿”å›ä¼šè¯ä¿¡æ¯
```

### 3.2 ä»£ç æ‰§è¡Œæµç¨‹
![alt text](image-1.png)
```mermaid
sequenceDiagram
    participant Agent as AI Agent
    participant API as API Gateway
    participant SessionMgr as ä¼šè¯ç®¡ç†å™¨
    participant Runtime as è¿è¡Œæ—¶
    participant Executor as æ‰§è¡Œå™¨
    participant DB as MariaDB
    participant Volume as å¯¹è±¡å­˜å‚¨<br/>(Volume æŒ‚è½½)

    Agent->>API: POST /api/v1/sessions/{session_id}/execute (code, language)
    API->>SessionMgr: get_session(id)
    SessionMgr->>DB: SELECT * FROM sessions WHERE id=?
    DB-->>SessionMgr: è¿”å›ä¼šè¯ä¿¡æ¯
    SessionMgr-->>API: è¿”å›ä¼šè¯ä¿¡æ¯

    API->>DB: INSERT INTO executions (ç”Ÿæˆ execution_id)
    DB-->>API: è¿”å› execution_id

    API->>Runtime: execute_code(session_id, execution_id, request)
    Runtime->>Executor: é€šè¿‡å®¹å™¨å†… HTTP API å‘é€æ‰§è¡Œè¯·æ±‚

    Note over Executor,Volume: workspace ç›®å½•å·²é€šè¿‡ Volume æŒ‚è½½å¯¹è±¡å­˜å‚¨

    par å¼‚æ­¥æ‰§è¡Œ
        Executor->>Executor: è¿è¡Œä»£ç  (bwrap + subprocess)
        Executor->>Executor: æ”¶é›† stdout/stderr
        Executor->>Volume: ç”Ÿæˆæ–‡ä»¶å†™å…¥ workspace<br/>(è‡ªåŠ¨æŒä¹…åŒ–åˆ°å¯¹è±¡å­˜å‚¨)
        Executor->>Executor: æ‰«æç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨
    end

    Executor->>API: POST /internal/executions/{execution_id}/result
    API->>DB: UPDATE executions (ä¿å­˜ stdout/stderr/çŠ¶æ€/æ–‡ä»¶åˆ—è¡¨)
    DB-->>API: ç¡®è®¤ä¿å­˜
    API-->>Executor: 200 OK

    API-->>Agent: è¿”å› execution_id (å·²æäº¤)

    Agent->>API: GET /api/v1/executions/{execution_id}/result
    API->>DB: SELECT * FROM executions WHERE id=?
    DB-->>API: è¿”å›ç»“æœæ•°æ® (stdout/stderr/artifacts)
    API-->>Agent: è¿”å›æ‰§è¡Œç»“æœ

    Note over Agent,Volume: å¦‚éœ€ä¸‹è½½æ–‡ä»¶ï¼Œé€šè¿‡æ–‡ä»¶ API ä»å¯¹è±¡å­˜å‚¨è·å–
```
### 3.3 å¥åº·æ£€æŸ¥ä¸æ•…éšœæ¢å¤æµç¨‹
![alt text](image-4.png)
```mermaid
sequenceDiagram
    participant Probe as ç›‘æ§æ¢é’ˆ
    participant Runtime as å®¹å™¨èŠ‚ç‚¹
    participant Scheduler as è°ƒåº¦å™¨
    participant SessionMgr as ä¼šè¯ç®¡ç†å™¨
    
    loop æ¯ 10 ç§’
        Probe->>Runtime: GET /health
        
        alt å¥åº·å“åº”
            Runtime-->>Probe: 200 OK + metrics
            Probe->>Probe: æ›´æ–°èŠ‚ç‚¹çŠ¶æ€
        else è¶…æ—¶æˆ–å¤±è´¥
            Runtime--xProbe: Timeout / Error
            Probe->>Probe: failure_count++
            
            alt failure_count >= 3
                Probe->>Scheduler: mark_unhealthy(node)
                Scheduler->>SessionMgr: migrate_sessions(node)
                
                loop å¯¹è¯¥èŠ‚ç‚¹çš„æ¯ä¸ªä¼šè¯
                    SessionMgr->>Scheduler: schedule(session)
                    Scheduler->>Runtime: åœ¨æ–°èŠ‚ç‚¹åˆ›å»ºä¼šè¯
                end
                
                Probe->>Scheduler: remove_node(node)
            end
        end
    end
```

## 4. æ•°æ®æ¨¡å‹è®¾è®¡


### 4.1 æ ¸å¿ƒå®ä½“æ¨¡å‹
```python
from enum import Enum
from pydantic import BaseModel
from datetime import datetime
from typing import Optional, Dict, List

class SessionStatus(str, Enum):
    CREATING = "creating"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    TIMEOUT = "timeout"
    TERMINATED = "terminated"

class ResourceLimit(BaseModel):
    cpu: str = "1"  # CPU æ ¸å¿ƒæ•°
    memory: str = "512Mi"  # å†…å­˜é™åˆ¶
    disk: str = "1Gi"  # ç£ç›˜é™åˆ¶
    max_processes: int = 128  # æœ€å¤§è¿›ç¨‹æ•°

class Template(BaseModel):
    id: str
    name: str
    image: str  # Docker é•œåƒ
    base_image: str  # åŸºç¡€é•œåƒï¼ˆç”¨äºä¸¤é˜¶æ®µåŠ è½½ï¼‰
    pre_installed_packages: List[str]
    default_resources: ResourceLimit
    security_context: Dict[str, any]
    created_at: datetime

class Session(BaseModel):
    id: str
    template_id: str
    status: SessionStatus
    runtime_type: str  # "docker" or "kubernetes"
    runtime_node: str  # èŠ‚ç‚¹ ID
    container_id: Optional[str]
    pod_name: Optional[str]
    resources: ResourceLimit
    env_vars: Dict[str, str]
    created_at: datetime
    updated_at: datetime
    timeout: int  # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

class Execution(BaseModel):
    id: str
    session_id: str
    code: str
    language: str
    status: str  # "pending", "running", "completed", "failed"
    stdout: str
    stderr: str
    exit_code: int
    execution_time: float  # æ‰§è¡Œè€—æ—¶ï¼ˆç§’ï¼‰
    artifacts: List[Artifact]  # ç”Ÿæˆçš„æ–‡ä»¶å…ƒæ•°æ®åˆ—è¡¨
    # æ–°å¢å­—æ®µï¼šhandler è¿”å›å€¼å’Œæ€§èƒ½æŒ‡æ ‡
    return_value: Optional[dict] = None  # handler å‡½æ•°è¿”å›å€¼ï¼ˆJSON å¯åºåˆ—åŒ–ï¼‰
    metrics: Optional[dict] = None  # æ€§èƒ½æŒ‡æ ‡ï¼ˆduration_ms, cpu_time_ms, peak_memory_mb ç­‰ï¼‰
    created_at: datetime
    completed_at: Optional[datetime]

class Artifact(BaseModel):
    """æ–‡ä»¶å…ƒæ•°æ®æ¨¡å‹"""
    path: str  # ç›¸å¯¹äº workspace çš„æ–‡ä»¶è·¯å¾„ï¼Œå¦‚ "results/output.csv"
    size: int  # æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    mime_type: str  # MIME ç±»å‹ï¼Œå¦‚ "text/csv"
    type: Literal["artifact", "log", "output"]  # æ–‡ä»¶ç±»å‹
    created_at: datetime  # åˆ›å»ºæ—¶é—´
    checksum: Optional[str] = None  # SHA256 æ ¡éªŒå’Œï¼ˆå¯é€‰ï¼‰
    download_url: Optional[str] = None  # ä¸‹è½½ URLï¼ˆé¢„ç­¾å S3 URLï¼‰

class RuntimeNode(BaseModel):
    id: str
    type: str  # "docker" or "kubernetes"
    url: str  # èŠ‚ç‚¹ API åœ°å€
    status: str  # "healthy", "unhealthy", "draining"
    cpu_usage: float  # 0.0 - 1.0
    mem_usage: float  # 0.0 - 1.0
    session_count: int
    max_sessions: int
    cached_templates: List[str]
    last_heartbeat: datetime
    failure_count: int
```

### 4.2 åè®®å®šä¹‰

#### 4.2.1 æ§åˆ¶å¹³é¢ APIï¼ˆå¤–éƒ¨ APIï¼‰

**è¯´æ˜**: ç”± AI Agent æˆ–ä¸Šå±‚æœåŠ¡è°ƒç”¨çš„å…¬å¼€ API æ¥å£ã€‚

```
# ä¼šè¯ç®¡ç†
POST   /api/v1/sessions                           # åˆ›å»ºä¼šè¯
GET    /api/v1/sessions/{id}                      # è·å–ä¼šè¯è¯¦æƒ…
GET    /api/v1/sessions                           # åˆ—å‡ºä¼šè¯
DELETE /api/v1/sessions/{id}                      # ç»ˆæ­¢ä¼šè¯

# ä»£ç æ‰§è¡Œ
POST   /api/v1/sessions/{session_id}/execute      # æäº¤æ‰§è¡Œä»»åŠ¡ï¼Œè¿”å› execution_id
GET    /api/v1/sessions/{session_id}/executions   # åˆ—å‡ºè¯¥ä¼šè¯çš„æ‰€æœ‰æ‰§è¡Œè®°å½•

# æ‰§è¡Œç»“æœæŸ¥è¯¢ï¼ˆåŸºäº execution_idï¼‰
GET    /api/v1/executions/{execution_id}          # è·å–æ‰§è¡Œè¯¦æƒ…ï¼ˆåŒ…å«ç»“æœï¼‰
GET    /api/v1/executions/{execution_id}/status   # è·å–æ‰§è¡ŒçŠ¶æ€ï¼ˆpending/running/completed/failedï¼‰
GET    /api/v1/executions/{execution_id}/result   # è·å–æ‰§è¡Œç»“æœï¼ˆstdout/stderr/exit_codeï¼‰

# æ–‡ä»¶æ“ä½œ
POST   /api/v1/sessions/{id}/files/upload         # ä¸Šä¼ æ–‡ä»¶åˆ°ä¼šè¯å·¥ä½œç›®å½•
GET    /api/v1/sessions/{id}/files/{name}         # ä¸‹è½½ä¼šè¯å·¥ä½œç›®å½•ä¸­çš„æ–‡ä»¶

# æ¨¡æ¿ç®¡ç†
POST   /api/v1/templates                          # åˆ›å»ºæ¨¡æ¿
GET    /api/v1/templates                          # åˆ—å‡ºæ¨¡æ¿
GET    /api/v1/templates/{id}                     # è·å–æ¨¡æ¿è¯¦æƒ…
PUT    /api/v1/templates/{id}                     # æ›´æ–°æ¨¡æ¿
DELETE /api/v1/templates/{id}                     # åˆ é™¤æ¨¡æ¿

# è¿è¡Œæ—¶ç®¡ç†
GET    /api/v1/runtimes                           # åˆ—å‡ºå®¹å™¨èŠ‚ç‚¹
GET    /api/v1/runtimes/{id}/health               # è·å–èŠ‚ç‚¹å¥åº·çŠ¶æ€
GET    /api/v1/runtimes/{id}/metrics              # è·å–èŠ‚ç‚¹æŒ‡æ ‡
```

**è¯·æ±‚/å“åº”ç¤ºä¾‹**ï¼š

```python
# æäº¤æ‰§è¡Œä»»åŠ¡
POST /api/v1/sessions/{session_id}/execute
Request:
{
    "code": "def handler(event):\n    return {'message': 'Hello', 'input': event.get('name', 'World')}",
    "language": "python",
    "timeout": 30,
    "event": {"name": "Alice"}
}

Response:
{
    "execution_id": "exec_1234567890",
    "status": "submitted",
    "submitted_at": "2025-01-04T10:30:00Z"
}

# æŸ¥è¯¢æ‰§è¡ŒçŠ¶æ€
GET /api/v1/executions/{execution_id}/status
Response:
{
    "execution_id": "exec_1234567890",
    "session_id": "sess_abc123",
    "status": "completed",
    "created_at": "2025-01-04T10:30:00Z",
    "completed_at": "2025-01-04T10:30:02Z"
}

# è·å–æ‰§è¡Œç»“æœ
GET /api/v1/executions/{execution_id}/result
Response:
{
    "execution_id": "exec_1234567890",
    "status": "success",
    "stdout": "Processing complete.\\n",
    "stderr": "",
    "exit_code": 0,
    "execution_time": 0.07523,
    "return_value": {
        "message": "Hello",
        "input": "Alice"
    },
    "metrics": {
        "duration_ms": 75.23,
        "cpu_time_ms": 68.12,
        "peak_memory_mb": 42.5
    },
    "artifacts": ["output.txt"]
}
```

#### 4.2.2 å†…éƒ¨å›è°ƒ APIï¼ˆç”± Executor è°ƒç”¨ï¼‰

**è¯´æ˜**: æ‰§è¡Œå™¨ï¼ˆè¿è¡Œåœ¨å®¹å™¨å†…çš„ sandbox-executorï¼‰è°ƒç”¨çš„å†…éƒ¨æ¥å£ï¼Œç”¨äºä¸ŠæŠ¥æ‰§è¡Œç»“æœã€‚

```
# æ‰§è¡Œç»“æœä¸ŠæŠ¥
POST   /internal/executions/{execution_id}/result    # ä¸ŠæŠ¥æ‰§è¡Œç»“æœï¼ˆå®Œæˆ/å¤±è´¥/è¶…æ—¶ï¼‰
POST   /internal/executions/{execution_id}/status    # ä¸ŠæŠ¥æ‰§è¡ŒçŠ¶æ€å˜æ›´ï¼ˆrunning/timeoutï¼‰

# è¯·æ±‚ä½“ç¤ºä¾‹ï¼š
POST /internal/executions/{execution_id}/result
{
    "status": "success",              # success | failed | timeout | error
    "stdout": "æ‰§è¡Œè¾“å‡ºå†…å®¹",
    "stderr": "é”™è¯¯è¾“å‡ºå†…å®¹",
    "exit_code": 0,
    "execution_time": 0.07523,
    "return_value": {                 # handler å‡½æ•°è¿”å›å€¼ï¼ˆJSON å¯åºåˆ—åŒ–ï¼‰
        "message": "Hello",
        "input": "Alice"
    },
    "metrics": {                      # æ€§èƒ½æŒ‡æ ‡
        "duration_ms": 75.23,
        "cpu_time_ms": 68.12,
        "peak_memory_mb": 42.5
    },
    "artifacts": ["generated_file.txt"]
}
```

**å®‰å…¨è¯´æ˜**: å†…éƒ¨ API åº”è¯¥ï¼š
- ä»…åœ¨å†…ç½‘/å®¹å™¨ç½‘ç»œä¸­å¯è®¿é—®
- ä½¿ç”¨è®¤è¯æœºåˆ¶ï¼ˆå¦‚ JWT token æˆ– API Keyï¼‰
- é™åˆ¶ä»…å®¹å™¨èŠ‚ç‚¹å¯è®¿é—®

**è¯´æ˜**: Container Scheduler ä½œä¸º Control Plane å†…éƒ¨æ¨¡å—ï¼Œé€šè¿‡ SDK ç›´æ¥è°ƒç”¨ Docker/K8s APIï¼Œæ— éœ€ç‹¬ç«‹çš„ HTTP APIã€‚

### 4.3 æ‰§è¡Œè¯­ä¹‰ä¸å¹‚ç­‰æ€§æ¨¡å‹

#### 4.3.1 execution_id ç”Ÿå‘½å‘¨æœŸ

æ¯ä¸ªä»£ç æ‰§è¡Œè¯·æ±‚éƒ½ä¼šè¢«åˆ†é…ä¸€ä¸ªå”¯ä¸€çš„ `execution_id`ï¼Œç”¨äºè¿½è¸ªæ•´ä¸ªæ‰§è¡Œè¿‡ç¨‹ã€‚

**execution_id ç”Ÿæˆè§„åˆ™**:
```python
execution_id = f"exec_{timestamp}_{uuid4()[:8]}"
# ç¤ºä¾‹: exec_20240115_abc12345
```

**ç”Ÿå‘½å‘¨æœŸçŠ¶æ€æœº**:
```mermaid
stateDiagram-v2
    [*] --> Pending: åˆ›å»º execution_id
    Pending --> Running: Executor å¼€å§‹æ‰§è¡Œ
    Running --> Completed: æ‰§è¡ŒæˆåŠŸ
    Running --> Failed: æ‰§è¡Œå¤±è´¥ï¼ˆè¯­æ³•é”™è¯¯/è¿è¡Œæ—¶é”™è¯¯ï¼‰
    Running --> Timeout: è¶…æ—¶
    Running --> Crashed: Executor å´©æºƒ
    Crashed --> Running: è‡ªåŠ¨é‡è¯•ï¼ˆæœ€å¤š 3 æ¬¡ï¼‰
    Crashed --> Failed: é‡è¯•æ¬¡æ•°è€—å°½
    Completed --> [*]
    Failed --> [*]
    Timeout --> [*]
```

**çŠ¶æ€è¯´æ˜**:

| çŠ¶æ€ | è¯´æ˜ | å¯å¦é‡è¯• |
|------|------|----------|
| `pending` | å·²åˆ›å»ºï¼Œç­‰å¾… Executor æ¥æ”¶ | æ˜¯ |
| `running` | Executor æ­£åœ¨æ‰§è¡Œ | å¦ |
| `completed` | æ‰§è¡ŒæˆåŠŸå®Œæˆ | å¦ |
| `failed` | æ‰§è¡Œå¤±è´¥ï¼ˆç”¨æˆ·ä»£ç é”™è¯¯ï¼‰ | å¦ |
| `timeout` | æ‰§è¡Œè¶…æ—¶ | å¯é€‰ï¼ˆç”±è°ƒç”¨æ–¹å†³å®šï¼‰ |
| `crashed` | Executor è¿›ç¨‹å´©æºƒ | æ˜¯ï¼ˆè‡ªåŠ¨é‡è¯•ï¼‰ |

#### 4.3.2 å¹‚ç­‰æ€§ä¿è¯

**At-Least-Once è¯­ä¹‰**:
- ç³»ç»Ÿä¿è¯æ¯ä¸ªæ‰§è¡Œè¯·æ±‚**è‡³å°‘è¢«æ‰§è¡Œä¸€æ¬¡**
- åœ¨ç½‘ç»œæ•…éšœã€Executor å´©æºƒç­‰åœºæ™¯ä¸‹å¯èƒ½ä¼šæ‰§è¡Œå¤šæ¬¡
- è°ƒç”¨æ–¹åº”è®¾è®¡å¹‚ç­‰å¤„ç†é€»è¾‘

**Exactly-Once è¯­ä¹‰ï¼ˆæœ‰é™ä¿è¯ï¼‰**:
- åœ¨æ­£å¸¸æƒ…å†µä¸‹ï¼ˆæ— å´©æºƒã€æ— ç½‘ç»œåˆ†åŒºï¼‰ï¼Œæ¯ä¸ª execution_id åªæ‰§è¡Œä¸€æ¬¡
- é€šè¿‡æ•°æ®åº“å”¯ä¸€çº¦æŸå’ŒçŠ¶æ€æœºä¿è¯ï¼š
  ```sql
  CREATE UNIQUE INDEX idx_execution_id ON executions(id);
  ```

**å¹‚ç­‰æ€§å»ºè®®**:
1. **è°ƒç”¨æ–¹å±‚é¢**:
   - å¯¹äºæœ‰å‰¯ä½œç”¨çš„æ“ä½œï¼ˆå¦‚å†™æ–‡ä»¶ï¼‰ï¼Œåº”å…ˆæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
   - ä½¿ç”¨å¹‚ç­‰é”®ï¼ˆidempotency keyï¼‰å»é‡

2. **å¹³å°å±‚é¢**:
   - ç›¸åŒ execution_id çš„é‡å¤æäº¤è¿”å›å·²æœ‰ç»“æœ
   - æ–‡ä»¶å†™å…¥ä½¿ç”¨åŸå­æ“ä½œï¼ˆé‡å‘½åè€Œéè¦†ç›–ï¼‰

```python
# ç¤ºä¾‹ï¼šå¹‚ç­‰æ–‡ä»¶å†™å…¥
def write_output(filename: str, content: str):
    tmp_file = f"{filename}.tmp.{uuid4()}"
    with open(tmp_file, 'w') as f:
        f.write(content)
    os.rename(tmp_file, filename)  # åŸå­æ“ä½œ
```

#### 4.3.3 é‡è¯•æœºåˆ¶

**è‡ªåŠ¨é‡è¯•æ¡ä»¶**:
- Executor è¿›ç¨‹å´©æºƒï¼ˆexit_code = -1 æˆ–ä¿¡å·ç»ˆæ­¢ï¼‰
- ç½‘ç»œé€šä¿¡å¤±è´¥ï¼ˆè¶…è¿‡ 3 æ¬¡å¿ƒè·³è¶…æ—¶ï¼‰
- å®¹å™¨å¼‚å¸¸é€€å‡ºï¼ˆéç”¨æˆ·ä»£ç å¯¼è‡´çš„å¤±è´¥ï¼‰

**é‡è¯•ç­–ç•¥**:
```python
class RetryPolicy:
    max_attempts: int = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
    backoff_base: float = 1.0  # é€€é¿åŸºæ•°ï¼ˆç§’ï¼‰
    backoff_factor: float = 2.0  # é€€é¿å› å­
    max_backoff: float = 10.0  # æœ€å¤§é€€é¿æ—¶é—´

    def get_delay(attempt: int) -> float:
        """è®¡ç®—ç¬¬ N æ¬¡é‡è¯•çš„å»¶è¿Ÿæ—¶é—´"""
        delay = backoff_base * (backoff_factor ** (attempt - 1))
        return min(delay, max_backoff)

# é‡è¯•å»¶è¿Ÿåºåˆ—: 1s, 2s, 4s, 8s, 10s, 10s, ...
```

**ä¸é‡è¯•çš„åœºæ™¯**:
- ç”¨æˆ·ä»£ç é”™è¯¯ï¼ˆè¯­æ³•é”™è¯¯ã€ImportErrorã€NameError ç­‰ï¼‰
- è¶…æ—¶ï¼ˆtimeout çŠ¶æ€ï¼‰
- æ˜¾å¼å–æ¶ˆï¼ˆè°ƒç”¨æ–¹ä¸»åŠ¨ç»ˆæ­¢ï¼‰
- é‡è¯•æ¬¡æ•°å·²è¾¾ä¸Šé™

**é‡è¯•æ‰§è¡Œæµç¨‹**:
```python
async def retry_execution_if_needed(execution_id: str) -> bool:
    """åˆ¤æ–­å¹¶æ‰§è¡Œé‡è¯•"""
    execution = await db.get_execution(execution_id)

    if execution.status != ExecutionStatus.CRASHED:
        return False

    if execution.retry_count >= MAX_RETRY_ATTEMPTS:
        await mark_failed(execution_id, reason="Max retries exceeded")
        return False

    # è®¡ç®—é€€é¿å»¶è¿Ÿ
    delay = RetryPolicy.get_delay(execution.retry_count + 1)
    await asyncio.sleep(delay)

    # é‡æ–°è°ƒåº¦åˆ°ç›¸åŒ sessionï¼ˆå¤ç”¨ workspaceï¼‰
    await scheduler.resubmit(execution.session_id, execution_id)

    # æ›´æ–°é‡è¯•è®¡æ•°
    execution.retry_count += 1
    await db.commit()

    return True
```

#### 4.3.4 Executor å´©æºƒå¤„ç†

**å´©æºƒæ£€æµ‹æœºåˆ¶**:

1. **å¿ƒè·³æ£€æµ‹**:
   ```python
   # Executor æ¯ 5 ç§’å‘é€ä¸€æ¬¡å¿ƒè·³
   async def heartbeat_loop(execution_id: str):
       while True:
           await api.post(f"/internal/executions/{execution_id}/heartbeat")
           await asyncio.sleep(5)

   # Control Plane 15 ç§’æœªæ”¶åˆ°å¿ƒè·³åˆ™åˆ¤å®šä¸ºå´©æºƒ
   HEARTBEAT_TIMEOUT = 15
   ```

2. **å®¹å™¨çŠ¶æ€ç›‘æ§**:
   ```python
   # å¥åº·æ¢é’ˆæ£€æŸ¥å®¹å™¨çŠ¶æ€
   async def check_container_health(container_id: str):
       container = docker.containers.get(container_id)
       status = container.status

       if status in {"exited", "dead"}:
           return "crashed"
       elif status == "running":
           return "healthy"
       else:
           return "unknown"
   ```

**å´©æºƒæ¢å¤æµç¨‹**:
```mermaid
flowchart TD
    A[æ£€æµ‹åˆ°å´©æºƒ] --> B{å´©æºƒç±»å‹}
    B -->|å®¹å™¨é€€å‡º| C[æ ‡è®°ä¸º crashed]
    B -->|å¿ƒè·³è¶…æ—¶| D[æ£€æŸ¥å®¹å™¨çŠ¶æ€]

    D -->|å®¹å™¨æ­£å¸¸è¿è¡Œ| E[æ¢å¤å¿ƒè·³]
    D -->|å®¹å™¨å·²é€€å‡º| C

    C --> F{é‡è¯•æ¬¡æ•° < 3?}
    F -->|æ˜¯| G[å»¶è¿Ÿåé‡è¯•]
    F -->|å¦| H[æ ‡è®°ä¸º failed]

    G --> I[åˆ›å»ºæ–°å®¹å™¨/å¤ç”¨ session]
    I --> J[é‡æ–°æ‰§è¡Œç›¸åŒä»£ç ]
    J --> K{æ‰§è¡ŒæˆåŠŸ?}
    K -->|æ˜¯| L[æ ‡è®°ä¸º completed]
    K -->|å¦| C
```

**æ•°æ®ä¸€è‡´æ€§ä¿è¯**:

1. **æ‰§è¡Œç»“æœå¹‚ç­‰ä¸ŠæŠ¥**:
   ```python
   # Executor ä½¿ç”¨å¹‚ç­‰é”®ä¸ŠæŠ¥ç»“æœ
   async def report_result(execution_id: str, result: ExecutionResult):
       await api.post(
           f"/internal/executions/{execution_id}/result",
           json=result.dict(),
           headers={"Idempotency-Key": f"{execution_id}_result"}
       )
   ```

2. **Artifact æ–‡ä»¶åŸå­åŒ–**:
   - æ–‡ä»¶å…ˆå†™å…¥ä¸´æ—¶ç›®å½• `.tmp/{execution_id}/`
   - æ‰§è¡Œå®ŒæˆååŸå­ç§»åŠ¨åˆ° workspace
   - å´©æºƒæ—¶ä¸´æ—¶æ–‡ä»¶è‡ªåŠ¨æ¸…ç†

3. **æ•°æ®åº“äº‹åŠ¡éš”ç¦»**:
   ```sql
   -- ä½¿ç”¨ä¹è§‚é”é˜²æ­¢å¹¶å‘æ›´æ–°
   UPDATE executions
   SET status = 'completed',
       version = version + 1
   WHERE id = ? AND version = ?;
   ```

#### 4.3.5 æ‰§è¡Œç»“æœæŸ¥è¯¢

**æœ€ç»ˆä¸€è‡´æ€§**:
- æ‰§è¡Œå®Œæˆåç»“æœé€šå¸¸åœ¨ 100ms å†…å¯æŸ¥è¯¢
- åœ¨é«˜è´Ÿè½½ä¸‹å¯èƒ½æœ‰ 1-2 ç§’å»¶è¿Ÿ
- è°ƒç”¨æ–¹åº”ä½¿ç”¨è½®è¯¢æˆ– Webhook è·å–ç»“æœ

**æ¨èæŸ¥è¯¢æ¨¡å¼**:
```python
async def wait_for_result(execution_id: str, timeout: int = 60) -> ExecutionResult:
    """ç­‰å¾…æ‰§è¡Œç»“æœï¼ˆå¸¦è¶…æ—¶ï¼‰"""
    start = time.time()

    while True:
        result = await api.get(f"/api/v1/executions/{execution_id}")

        if result["status"] in {"completed", "failed", "timeout"}:
            return result

        if time.time() - start > timeout:
            raise TimeoutError(f"Execution {execution_id} query timeout")

        await asyncio.sleep(0.5)  # é€€é¿è½®è¯¢
```

#### 4.3.6 å¤±è´¥æ¢å¤è·¯å¾„

æœ¬èŠ‚æè¿°å„ç§æ•…éšœåœºæ™¯ä¸‹çš„æ¢å¤æœºåˆ¶ï¼Œç¡®ä¿ç³»ç»Ÿåœ¨å„ç§å¼‚å¸¸æƒ…å†µä¸‹çš„å¯ç”¨æ€§å’Œæ•°æ®ä¸€è‡´æ€§ã€‚

**æ•…éšœåˆ†ç±»**:

| æ•…éšœç±»å‹ | å½±å“èŒƒå›´ | æ¢å¤ç­–ç•¥ | æ•°æ®ä¸€è‡´æ€§ |
|----------|----------|----------|------------|
| Control Plane é‡å¯ | æ‰€æœ‰æ­£åœ¨è¿›è¡Œçš„è¯·æ±‚ | æ•°æ®åº“çŠ¶æ€æ¢å¤ + è¿è¡Œæ—¶é‡è¿ | å¼ºä¸€è‡´æ€§ |
| Executor å´©æºƒ | å•ä¸ªæ‰§è¡Œä»»åŠ¡ | è‡ªåŠ¨é‡è¯•ï¼ˆæœ€å¤š 3 æ¬¡ï¼‰ | At-Least-Once |
| Pod Eviction | æ•´ä¸ªæ²™ç®± Pod | é€æ˜é‡å»º + å¤ç”¨ workspace | At-Least-Once |
| ç½‘ç»œåˆ†åŒº | éƒ¨åˆ†èŠ‚ç‚¹ä¸å¯è¾¾ | è‡ªåŠ¨é‡è·¯ç”± + è¶…æ—¶é‡è¯• | æœ€ç»ˆä¸€è‡´æ€§ |
| èŠ‚ç‚¹æ•…éšœ | èŠ‚ç‚¹ä¸Šæ‰€æœ‰ Pod | è°ƒåº¦åˆ°å…¶ä»–èŠ‚ç‚¹ + é‡å»º | æœ€ç»ˆä¸€è‡´æ€§ |
| æ•°æ®åº“æ•…éšœ | æ‰€æœ‰å…ƒæ•°æ®æ“ä½œ | åªè¯»æ¨¡å¼ + é‡è¯• | å¼ºä¸€è‡´æ€§ |

**åœºæ™¯ 1: Control Plane é‡å¯**

```mermaid
flowchart TD
    A[Control Plane é‡å¯] --> B[ä»æ•°æ®åº“æ¢å¤çŠ¶æ€]
    B --> C{æ£€æŸ¥ running çŠ¶æ€çš„ executions}

    C --> D[æŸ¥è¯¢ last_heartbeat_at]
    D --> E{å¿ƒè·³è¶…æ—¶?}

    E -->|æ˜¯| F[æ ‡è®°ä¸º crashed]
    E -->|å¦| G[ä¿æŒ running çŠ¶æ€]

    F --> H[è§¦å‘é‡è¯•é€»è¾‘]
    G --> I[ç­‰å¾… Executor ä¸ŠæŠ¥ç»“æœ]

    H --> J[æ£€æŸ¥ session å®¹å™¨çŠ¶æ€]
    J --> K{å®¹å™¨å­˜åœ¨?}
    K -->|æ˜¯| L[å¤ç”¨ç°æœ‰å®¹å™¨]
    K -->|å¦| M[åˆ›å»ºæ–°å®¹å™¨]

    L --> N[é‡æ–°æ‰§è¡Œä»£ç ]
    M --> N
```

**æ¢å¤æµç¨‹**:

1. **å¯åŠ¨æ—¶çŠ¶æ€æ¢å¤**:
   ```python
   async def recover_on_startup():
       """Control Plane å¯åŠ¨æ—¶æ¢å¤çŠ¶æ€"""

       # 1. æŸ¥æ‰¾æ‰€æœ‰ running çŠ¶æ€çš„æ‰§è¡Œ
       running_executions = await db.query(
           SELECT * FROM executions
           WHERE status = 'running'
       )

       for execution in running_executions:
           # 2. æ£€æŸ¥å¿ƒè·³æ—¶é—´
           if execution.last_heartbeat_at < heartbeat_threshold():
               # å¿ƒè·³è¶…æ—¶ï¼Œæ ‡è®°ä¸ºå´©æºƒ
               await mark_crashed(execution.id)
               # è§¦å‘é‡è¯•
               await retry_execution_if_needed(execution.id)

       # 3. æ£€æŸ¥ session å®¹å™¨çŠ¶æ€
       sessions = await db.query(
           SELECT * FROM sessions
           WHERE status = 'running'
       )

       for session in sessions:
           is_alive = await check_container_status(session.container_id)
           if not is_alive:
               # å®¹å™¨å·²æ¶ˆå¤±ï¼Œæ ‡è®°ä¸ºå¾…é‡å»º
               await mark_session_unhealthy(session.id)
   ```

2. **è¿è¡Œæ—¶è¿æ¥æ¢å¤**:
   ```python
   async def reconnect_runtime_nodes():
       """é‡æ–°è¿æ¥æ‰€æœ‰å®¹å™¨èŠ‚ç‚¹"""
       nodes = await db.query(SELECT * FROM runtime_nodes)

       for node in nodes:
           try:
               # å‘é€å¥åº·æ£€æŸ¥
               await node.health_check()
               node.status = "healthy"
           except Exception:
               node.status = "unhealthy"

       await db.commit()
   ```

**åœºæ™¯ 2: Pod Eviction / èŠ‚ç‚¹ Drain**

å½“ Kubernetes èŠ‚ç‚¹éœ€è¦ç»´æŠ¤ï¼ˆå¦‚å‡çº§å†…æ ¸ï¼‰æ—¶ï¼ŒPod ä¼šè¢«ä¸»åŠ¨é©±é€ã€‚

```mermaid
flowchart TD
    A[Kubernetes å‘å‡º Eviction ä¿¡å·] --> B[Pod æ”¶åˆ° SIGTERM]
    B --> C[Executor å°è¯•ä¼˜é›…å…³é—­]

    C --> D{æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡?}
    D -->|æ˜¯| E[æ ‡è®°å½“å‰æ‰§è¡Œä¸º crashed]
    D -->|å¦| F[ç›´æ¥é€€å‡º]

    E --> F
    F --> G[Pod è¢«åˆ é™¤]

    G --> H[Control Plane æ£€æµ‹åˆ° Pod æ¶ˆå¤±]
    H --> I[ä»æ•°æ®åº“æŸ¥è¯¢ session ä¿¡æ¯]

    I --> J[è°ƒåº¦åˆ°æ–°èŠ‚ç‚¹]
    J --> K[åˆ›å»ºæ–° Pod + æŒ‚è½½åŒä¸€ S3 workspace]

    K --> L[æ¢å¤ crashed çŠ¶æ€çš„ executions]
    L --> M[è‡ªåŠ¨é‡è¯•æ‰§è¡Œ]
```

**æ¢å¤æœºåˆ¶**:

1. **ä¼˜é›…å…³é—­å¤„ç†**:
   ```python
   # Executor æ”¶åˆ° SIGTERM æ—¶çš„å¤„ç†
   async def handle_shutdown():
       # 1. æ ‡è®°æ‰€æœ‰æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡ä¸º crashed
       running_executions = get_running_executions()
       for exec_id in running_executions:
           await mark_crashed_via_callback(exec_id)

       # 2. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
       cleanup_temp_files()

       # 3. æœ€å¤šç­‰å¾… 10 ç§’åå¼ºåˆ¶é€€å‡º
       await asyncio.sleep(10)
       sys.exit(143)  # SIGTERM exit code
   ```

2. **è·¨èŠ‚ç‚¹ä¼šè¯æ¢å¤**:
   ```python
   async def recover_session_on_eviction(session_id: str):
       """Pod é©±é€åæ¢å¤ä¼šè¯"""
       session = await db.get_session(session_id)

       # 1. è°ƒåº¦åˆ°æ–°çš„å¥åº·èŠ‚ç‚¹
       new_node = await scheduler.select_best_node(session.template_id)

       # 2. åœ¨æ–°èŠ‚ç‚¹ä¸Šåˆ›å»ºå®¹å™¨ï¼ŒæŒ‚è½½åŒä¸€ S3 workspace
       new_container_id = await new_node.create_container(
           session_id=session_id,
           workspace_path=session.workspace_path,  # å¤ç”¨ S3 è·¯å¾„
           template_id=session.template_id
       )

       # 3. æ›´æ–° session è®°å½•
       session.runtime_node = new_node.id
       session.container_id = new_container_id
       await db.commit()

       # 4. æ¢å¤æ‰€æœ‰ crashed çŠ¶æ€çš„æ‰§è¡Œ
       crashed_executions = await db.query(
           SELECT * FROM executions
           WHERE session_id = ? AND status = 'crashed'
       )

       for execution in crashed_executions:
           await retry_execution_if_needed(execution.id)
   ```

**åœºæ™¯ 3: ç½‘ç»œåˆ†åŒº**

ç½‘ç»œåˆ†åŒºå¯èƒ½å¯¼è‡´ Control Plane ä¸ å®¹å™¨èŠ‚ç‚¹ã€Executor ä¹‹é—´é€šä¿¡ä¸­æ–­ã€‚

```mermaid
flowchart TD
    A[ç½‘ç»œåˆ†åŒºå‘ç”Ÿ] --> B{å“ªäº›èŠ‚ç‚¹å—å½±å“?}

    B -->|å®¹å™¨èŠ‚ç‚¹ä¸å¯è¾¾| C[æ ‡è®°èŠ‚ç‚¹ä¸º unhealthy]
    B -->|Executor å¿ƒè·³ä¸¢å¤±| D[æ ‡è®°æ‰§è¡Œä¸º crashed]
    B -->|Control Plane ä¸å¯è¾¾| E[Executor æœ¬åœ°æŒä¹…åŒ–ç»“æœ]

    C --> F[åœæ­¢å‘è¯¥èŠ‚ç‚¹è°ƒåº¦æ–°ä»»åŠ¡]
    D --> G[è§¦å‘é‡è¯•é€»è¾‘]
    E --> H[ç½‘ç»œæ¢å¤åé‡è¯•ä¸ŠæŠ¥]

    F --> I[ç­‰å¾…ç½‘ç»œæ¢å¤]
    G --> I
    H --> I

    I --> J{ç½‘ç»œæ¢å¤?}
    J -->|æ˜¯| K[æ¢å¤æ­£å¸¸è°ƒåº¦]
    J -->|å¦| L[è¶…æ—¶åæ ‡è®°ä¸º failed]
```

**å¤„ç†ç­–ç•¥**:

1. **è¶…æ—¶ä¸é‡è¯•é…ç½®**:
   ```python
   class NetworkConfig:
       # HTTP å®¢æˆ·ç«¯é…ç½®
       connect_timeout: float = 5.0  # è¿æ¥è¶…æ—¶
       read_timeout: float = 30.0    # è¯»å–è¶…æ—¶
       max_retries: int = 3          # æœ€å¤§é‡è¯•æ¬¡æ•°

       # å¿ƒè·³é…ç½®
       heartbeat_interval: float = 5.0    # å¿ƒè·³é—´éš”
       heartbeat_timeout: float = 15.0    # å¿ƒè·³è¶…æ—¶

       # èŠ‚ç‚¹å¥åº·æ£€æŸ¥
       health_check_interval: float = 10.0
       node_unhealthy_threshold: int = 3  # è¿ç»­å¤±è´¥æ¬¡æ•°é˜ˆå€¼
   ```

2. **Executor æœ¬åœ°æŒä¹…åŒ–**:
   ```python
   # Executor åœ¨ Control Plane ä¸å¯è¾¾æ—¶æœ¬åœ°ä¿å­˜ç»“æœ
   async def report_result_with_fallback(execution_id: str, result: ExecutionResult):
       try:
           await api.post(f"/internal/executions/{execution_id}/result", json=result)
       except Exception as e:
           # ç½‘ç»œå¤±è´¥ï¼Œæœ¬åœ°æŒä¹…åŒ–
           local_path = f"/tmp/results/{execution_id}.json"
           with open(local_path, 'w') as f:
               json.dump(result.dict(), f)

           # åå°é‡è¯•ä»»åŠ¡
           asyncio.create_task(retry_report_when_available(execution_id, local_path))

   async def retry_report_when_available(execution_id: str, local_path: str):
       while True:
           try:
               with open(local_path, 'r') as f:
                   result = json.load(f)
               await api.post(f"/internal/executions/{execution_id}/result", json=result)
               os.remove(local_path)  # ä¸ŠæŠ¥æˆåŠŸï¼Œåˆ é™¤æœ¬åœ°æ–‡ä»¶
               break
           except Exception:
               await asyncio.sleep(5)  # 5 ç§’åé‡è¯•
   ```

**åœºæ™¯ 4: æ•°æ®åº“æ•…éšœ**

```mermaid
flowchart TD
    A[æ£€æµ‹åˆ°æ•°æ®åº“æ•…éšœ] --> B{æ•…éšœç±»å‹}

    B -->|è¿æ¥å¤±è´¥| C[åˆ‡æ¢åˆ°å¤‡ç”¨æ•°æ®åº“]
    B -->|ä¸»ä»åˆ‡æ¢| D[æ›´æ–°è¿æ¥æ± æŒ‡å‘æ–°ä¸»åº“]
    B -->|å®Œå…¨ä¸å¯ç”¨| E[è¿›å…¥åªè¯»é™çº§æ¨¡å¼]

    C --> F[é‡è¯•å¤±è´¥çš„æ“ä½œ]
    D --> F
    E --> G[æ‹’ç»å†™æ“ä½œ<br/>å…è®¸è¯»ç¼“å­˜]

    F --> H{æœåŠ¡æ¢å¤?}
    H -->|æ˜¯| I[æ¢å¤æ­£å¸¸æœåŠ¡]
    H -->|å¦| J[è¿”å› 503 Service Unavailable]

    G --> K[ç­‰å¾…æ•°æ®åº“æ¢å¤]
    K --> I
```

**é™çº§ç­–ç•¥**:

1. **åªè¯»æ¨¡å¼**:
   ```python
   class DatabaseManager:
       def __init__(self):
           self.read_only_mode = False
           self.cache = TTLCache(maxsize=1000, ttl=60)  # 1 åˆ†é’Ÿç¼“å­˜

       async def execute_write(self, query, params):
           if self.read_only_mode:
               raise ServiceUnavailable("Database in read-only mode")

           return await self.db.execute(query, params)

       async def execute_read(self, query, params):
           # ä¼˜å…ˆä»ç¼“å­˜è¯»å–
           cache_key = f"{query}:{params}"
           if cached := self.cache.get(cache_key):
               return cached

           result = await self.db.execute(query, params)
           self.cache[cache_key] = result
           return result
   ```

**åœºæ™¯ 5: S3 å¯¹è±¡å­˜å‚¨æ•…éšœ**

```mermaid
flowchart TD
    A[S3 ä¸ŠæŠ¥/ä¸‹è½½å¤±è´¥] --> B{æ“ä½œç±»å‹}

    B -->|Executor ä¸ŠæŠ¥ç»“æœ| C[æœ¬åœ°æŒä¹…åŒ– + é‡è¯•]
    B -->|ä¸‹è½½ artifact| D[è¿”å›é¢„ç­¾å URL<br/>å®¢æˆ·ç«¯ç›´æ¥ä¸‹è½½]
    B -->|åˆ›å»º workspace| E[ä½¿ç”¨æœ¬åœ°ä¸´æ—¶å­˜å‚¨]

    C --> F{é‡è¯•æˆåŠŸ?}
    F -->|æ˜¯| G[æ¸…ç†æœ¬åœ°å‰¯æœ¬]
    F -->|å¦| H[ä¿ç•™ 24 å°æ—¶ååˆ é™¤]

    D --> I[ç»•è¿‡ Control Plane]
    E --> J[é™çº§è­¦å‘Š]
```

**å®¹é”™æœºåˆ¶**:

1. **æœ¬åœ°ä¸´æ—¶å­˜å‚¨**:
   ```python
   # S3 ä¸å¯ç”¨æ—¶ä½¿ç”¨æœ¬åœ°å­˜å‚¨
   class ArtifactStorage:
       def __init__(self):
           self.s3_client = boto3.client('s3')
           self.fallback_path = "/tmp/artifacts"

       async def upload(self, file_path: str, s3_path: str):
           try:
               await self.s3_client.upload_file(file_path, bucket, s3_path)
           except Exception:
               # é™çº§åˆ°æœ¬åœ°å­˜å‚¨
               local_path = os.path.join(self.fallback_path, s3_path)
               os.makedirs(os.path.dirname(local_path), exist_ok=True)
               shutil.copy(file_path, local_path)
               logger.warning(f"S3 unavailable, using local storage: {local_path}")
   ```

2. **é¢„ç­¾å URL ç›´æ¥ä¸‹è½½**:
   ```python
   # ç»•è¿‡ Control Planeï¼Œå®¢æˆ·ç«¯ç›´æ¥ä» S3 ä¸‹è½½
   async def get_artifact_download_url(session_id: str, file_path: str) -> str:
       s3_path = f"sessions/{session_id}/{file_path}"
       url = s3_client.generate_presigned_url(
           'get_object',
           Params={'Bucket': S3_BUCKET, 'Key': s3_path},
           ExpiresIn=3600  # 1 å°æ—¶æœ‰æ•ˆæœŸ
       )
       return url
   ```

**æ¢å¤æ—¶é—´ç›®æ ‡ (RTO)**:

| æ•…éšœåœºæ™¯ | RTO | RPO | è¯´æ˜ |
|----------|-----|-----|------|
| Control Plane é‡å¯ | < 30s | 0 | å†…å­˜çŠ¶æ€å¯ä»æ•°æ®åº“æ¢å¤ |
| Executor å´©æºƒ | < 10s | 0 | è‡ªåŠ¨é‡è¯•ï¼Œæœ€å¤š 3 æ¬¡ |
| Pod Eviction | < 60s | 0 | è·¨èŠ‚ç‚¹æ¢å¤ï¼Œå¤ç”¨ S3 workspace |
| ç½‘ç»œåˆ†åŒº | < 30s | 0 | è¶…æ—¶é‡è¯• + è‡ªåŠ¨é‡è·¯ç”± |
| æ•°æ®åº“æ•…éšœ | < 60s | 0 | ä¸»ä»åˆ‡æ¢ |
| S3 æ•…éšœ | N/A | > 0 | é™çº§åˆ°æœ¬åœ°å­˜å‚¨ |

**æœ€ä½³å®è·µå»ºè®®**:

1. **å®šæœŸå¥åº·æ£€æŸ¥**:
   - æ¯ 10 ç§’æ£€æŸ¥ä¸€æ¬¡ å®¹å™¨èŠ‚ç‚¹å¥åº·çŠ¶æ€
   - æ¯ 5 ç§’æ£€æŸ¥ä¸€æ¬¡ Executor å¿ƒè·³
   - ä½¿ç”¨ Kubernetes liveness/readiness probe

2. **ä¼˜é›…å…³é—­**:
   - Control Plane æ”¶åˆ° SIGTERM æ—¶ï¼š
     - åœæ­¢æ¥å—æ–°è¯·æ±‚
     - ç­‰å¾…æ­£åœ¨å¤„ç†çš„è¯·æ±‚å®Œæˆï¼ˆæœ€å¤š 30 ç§’ï¼‰
     - æŒä¹…åŒ–å†…å­˜çŠ¶æ€åˆ°æ•°æ®åº“

3. **ç›‘æ§å‘Šè­¦**:
   - ç›‘æ§å´©æºƒé‡è¯•ç‡ï¼ˆåº” < 1%ï¼‰
   - ç›‘æ§å¿ƒè·³è¶…æ—¶æ¬¡æ•°ï¼ˆåº” < 0.1%ï¼‰
   - ç›‘æ§èŠ‚ç‚¹ä¸å¥åº·æ¯”ä¾‹ï¼ˆåº” < 10%ï¼‰
   - å‘Šè­¦é˜ˆå€¼ï¼šè¿ç»­ 3 æ¬¡é‡è¯•å¤±è´¥

### 4.4 S3 Workspace æŒ‚è½½æ¶æ„

æœ¬èŠ‚è¯¦ç»†æè¿° S3 workspace æŒ‚è½½çš„å®ç°æœºåˆ¶ï¼Œä½¿å®¹å™¨å†…çš„ç”¨æˆ·ä»£ç èƒ½å¤Ÿåƒè®¿é—®æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿä¸€æ ·è®¿é—® S3 å¯¹è±¡å­˜å‚¨ã€‚

**Implementation Status**: âœ… Fully Implemented (s3fs + bind mount)

#### 4.4.1 æ¶æ„æ¦‚è¿°ï¼ˆKubernetes - s3fs + bind mountï¼‰

Kubernetes ç¯å¢ƒä½¿ç”¨ **s3fs + bind mount æ–¹å¼**å®ç° S3 workspace æŒ‚è½½ã€‚

```mermaid
flowchart LR
    subgraph ControlPlane["Control Plane"]
        FileAPI["æ–‡ä»¶ API<br/>/sessions/{id}/files/*"]
        SessionService["Session Service"]
        K8sScheduler["K8s Scheduler<br/>s3fs Mode"]
    end

    subgraph K8sCluster["Kubernetes Cluster"]
        subgraph ExecutorPod["Executor Pod"]
            InitContainer["Init Container<br/>s3fs æŒ‚è½½"]
            S3FSMount["s3fs æŒ‚è½½<br/>/mnt/s3-root"]
            BindMount["bind mount<br/>â†’ /workspace"]
            Workspace["/workspace ç›®å½•"]
            Executor["Executor<br/>(sandbox-executor)"]
        end
    end

    subgraph StorageLayer["Storage Layer"]
        MinIO["MinIO<br/>(S3-compatible)"]
    end

    FileAPI -->|"ä¸Šä¼ /ä¸‹è½½"| S3Storage["S3Storage Service"]
    SessionService -->|"åˆ›å»ºä¼šè¯<br/>workspace_path=s3://..."| K8sScheduler
    K8sScheduler -->|"åˆ›å»º Pod<br/>å¯åŠ¨è„šæœ¬åŒ…å« s3fs æŒ‚è½½"| InitContainer

    InitContainer -->|"s3fs mount"| S3FSMount
    S3FSMount <-->|"S3 API"| MinIO
    BindMount -->|"è¦†ç›–"| Workspace
    Workspace -->|"æ–‡ä»¶è¯»å†™"| Executor

    style ControlPlane fill:#bbdefb
    style K8sCluster fill:#c8e6c9
    style ExecutorPod fill:#fff9c4
    style StorageLayer fill:#f8bbd0
    style InitContainer fill:#ffccbc
```

**å…³é”®è®¾è®¡å†³ç­–**ï¼š

| ç»„ä»¶ | æ–¹æ¡ˆ | è¯´æ˜ |
|------|------|------|
| æŒ‚è½½æ–¹å¼ | **s3fs + bind mount** | å®¹å™¨å†…å¯åŠ¨è„šæœ¬æŒ‚è½½ s3fsï¼Œä½¿ç”¨ bind mount è¦†ç›– /workspace |
| æŒ‚è½½æ—¶æœº | å®¹å™¨å¯åŠ¨æ—¶ | é€šè¿‡å¯åŠ¨è„šæœ¬è‡ªåŠ¨å®Œæˆ |
| å®¹å™¨æŒ‚è½½ç‚¹ | `/workspace` | emptyDir å·ï¼Œé€šè¿‡ bind mount è¦†ç›– session ç›®å½• |
| å­˜å‚¨åç«¯ | MinIO | S3-compatible å¯¹è±¡å­˜å‚¨ï¼Œæ— éœ€é¢å¤–å…ƒæ•°æ®æ•°æ®åº“ |

#### 4.4.2 s3fs æŒ‚è½½è„šæœ¬å®ç°

**S3 Workspace æ£€æµ‹**ï¼š

```python
def _parse_s3_workspace(self, workspace_path: str) -> Optional[dict]:
    """è§£æ S3 workspace è·¯å¾„"""
    if not workspace_path or not workspace_path.startswith("s3://"):
        return None

    parsed = urlparse(workspace_path)
    return {
        "bucket": parsed.netloc,
        "prefix": parsed.path.lstrip('/'),
    }
```

**hostPath å·é…ç½®**ï¼š

```python
def _build_s3fs_mount_script(self, bucket: str, minio_url: str, s3_prefix: str) -> str:
    """æ„å»º s3fs æŒ‚è½½è„šæœ¬"""
    return f"""
# æŒ‚è½½ S3 bucket
s3fs {bucket} /mnt/s3-root \\
    -o url={minio_url} \\
    -o use_path_request_style \\
    -o allow_other \\
    -o uid=1000 -o gid=1000

# ä½¿ç”¨ bind mount è¦†ç›– /workspace
SESSION_PATH="/mnt/s3-root/{s3_prefix}"
mkdir -p "$SESSION_PATH"
mount --bind "$SESSION_PATH" /workspace
"""

# åˆ›å»º Pod æ—¶
if s3_workspace := self._parse_s3_workspace(config.workspace_path):
    host_path = self._build_juicefs_host_path(s3_workspace["prefix"])

    volumes.append(
        V1Volume(
            name="workspace",
            host_path=V1HostPathVolumeSource(
                path=host_path,
                type="DirectoryOrCreate",
            ),
        )
    )
```

**Pod æ¨¡æ¿ç¤ºä¾‹**ï¼š

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: executor-session-123
  namespace: sandbox-runtime
spec:
  containers:
    - name: executor
      image: sandbox-executor:latest
      volumeMounts:
        - name: workspace
          mountPath: /workspace
      # ä¸éœ€è¦ç‰¹æƒæ¨¡å¼æˆ–é¢å¤–èƒ½åŠ›
  volumes:
    - name: workspace
      hostPath:
        path: /mnt/jfs/sandbox-workspace/sessions/session-123
        type: DirectoryOrCreate
```

#### 4.4.3 s3fs Secret é…ç½®

**Secret å®šä¹‰**ï¼š

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: s3fs-passwd
  namespace: sandbox-system
type: Opaque
stringData:
  s3fs-passwd: |
    minioadmin:minioadmin
```

**Secret æŒ‚è½½åˆ° Executor Pod**ï¼š

```yaml
volumes:
  - name: workspace
    emptyDir: {}
  - name: s3fs-passwd
    secret:
      secretName: s3fs-passwd
      defaultMode: 0400

volumeMounts:
  - name: workspace
    mountPath: /workspace
  - name: s3fs-passwd
    mountPath: /etc/s3fs-passwd
    readOnly: true
```

#### 4.4.4 Executor é•œåƒè¦æ±‚

**Dockerfile é…ç½®ï¼ˆç®€åŒ–ï¼‰**ï¼š

```dockerfile
FROM python:3.11-slim

# å®‰è£…åŸºç¡€ä¾èµ–
RUN apt-get update && apt-get install -y \
    curl \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# åˆ›å»ºéç‰¹æƒç”¨æˆ·
RUN groupadd -g 1000 sandbox && \
    useradd -m -u 1000 -g sandbox sandbox

# åˆ›å»º workspace ç›®å½•
RUN mkdir -p /workspace && \
    chown -R sandbox:sandbox /workspace

USER sandbox
WORKDIR /workspace

EXPOSE 8080
CMD ["python", "-m", "executor.interfaces.http.rest"]
```

**å…³é”®ç‰¹æ€§**ï¼š

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| æŒ‚è½½æ–¹å¼ | s3fs åœ¨å®¹å™¨å†…é€šè¿‡å¯åŠ¨è„šæœ¬æŒ‚è½½ |
| ç‰¹æƒè¦æ±‚ | éœ€è¦ privileged æ¨¡å¼è¿è¡Œ s3fs |
| å¯åŠ¨è„šæœ¬ | è‡ªåŠ¨æŒ‚è½½ s3fs å’Œ bind mount |
| è¿è¡Œç”¨æˆ· | root (æŒ‚è½½) â†’ gosu åˆ‡æ¢åˆ° sandbox |

#### 4.4.4 æ–‡ä»¶ API å®ç°

**ä¸Šä¼ æ–‡ä»¶æµç¨‹**ï¼š

```mermaid
sequenceDiagram
    participant Client as å®¢æˆ·ç«¯
    participant API as æ–‡ä»¶ API
    participant S3Service as S3Storage
    participant S3 as S3/MinIO

    Client->>API: POST /sessions/{id}/files/upload?path=data.csv
    API->>API: éªŒè¯ä¼šè¯å­˜åœ¨ä¸”è¿è¡Œä¸­
    API->>API: æ„å»ºå®Œæ•´ S3 è·¯å¾„<br/>s3://bucket/sessions/{id}/data.csv
    API->>S3Service: upload_file(s3_path, content)
    S3Service->>S3: boto3.put_object()
    S3-->>S3Service: ä¸Šä¼ æˆåŠŸ
    S3Service-->>API: è¿”å›
    API-->>Client: 200 OK {file_path, size}
```

**ä¸‹è½½æ–‡ä»¶æµç¨‹**ï¼š

```mermaid
sequenceDiagram
    participant Client as å®¢æˆ·ç«¯
    participant API as æ–‡ä»¶ API
    participant S3Service as S3Storage
    participant S3 as S3/MinIO

    Client->>API: GET /sessions/{id}/files/data.csv
    API->>API: éªŒè¯ä¼šè¯å­˜åœ¨
    API->>S3Service: get_file_info(s3_path)
    S3Service->>S3: boto3.head_object()
    S3-->>S3Service: æ–‡ä»¶å…ƒæ•°æ®

    alt æ–‡ä»¶ < 10MB
        API->>S3Service: download_file(s3_path)
        S3Service->>S3: boto3.get_object()
        S3-->>S3Service: æ–‡ä»¶å†…å®¹
        API-->>Client: 200 OK (ç›´æ¥è¿”å›å†…å®¹)
    else æ–‡ä»¶ >= 10MB
        API->>S3Service: generate_presigned_url(s3_path)
        S3Service->>S3: boto3.generate_presigned_url()
        S3-->>S3Service: é¢„ç­¾å URL
        API-->>Client: 200 OK {presigned_url}
    end
```

**S3Storage æ ¸å¿ƒæ–¹æ³•**ï¼š

```python
class S3Storage(IStorageService):
    def __init__(self):
        settings = get_settings()
        self._client = boto3.client(
            's3',
            endpoint_url=settings.s3_endpoint_url or None,
            aws_access_key_id=settings.s3_access_key_id,
            aws_secret_access_key=settings.s3_secret_access_key,
            region_name=settings.s3_region,
        )
        self._bucket = settings.s3_bucket

    async def upload_file(self, s3_path: str, content: bytes,
                         content_type: str = "application/octet-stream"):
        """ä¸Šä¼ æ–‡ä»¶åˆ° S3"""
        bucket, key = self._parse_s3_path(s3_path)
        await asyncio.to_thread(
            self._client.put_object,
            Bucket=bucket,
            Key=key,
            Body=content,
            ContentType=content_type
        )

    async def download_file(self, s3_path: str) -> bytes:
        """ä» S3 ä¸‹è½½æ–‡ä»¶"""
        bucket, key = self._parse_s3_path(s3_path)
        response = await asyncio.to_thread(
            self._client.get_object,
            Bucket=bucket,
            Key=key
        )
        return response['Body'].read()

    async def delete_prefix(self, prefix: str) -> int:
        """åˆ é™¤æŒ‡å®šå‰ç¼€çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆç”¨äºä¼šè¯æ¸…ç†ï¼‰"""
        bucket, key_prefix = self._parse_s3_path(prefix)
        deleted_count = 0

        paginator = self._client.get_paginator('list_objects_v2')
        for page in paginator.paginate(Bucket=bucket, Prefix=key_prefix):
            if 'Contents' in page:
                objects = [{'Key': obj['Key']} for obj in page['Contents']]
                await asyncio.to_thread(
                    self._client.delete_objects,
                    Bucket=bucket,
                    Delete={'Objects': objects}
                )
                deleted_count += len(objects)

        return deleted_count
```

#### 4.4.5 ä¼šè¯æ¸…ç†

**è‡ªåŠ¨æ¸…ç† S3 æ–‡ä»¶**ï¼š

```python
async def terminate_session(self, session_id: str) -> SessionDTO:
    """ç»ˆæ­¢ä¼šè¯ï¼Œæ¸…ç† S3 æ–‡ä»¶"""
    session = await self._session_repo.find_by_id(session_id)

    # é”€æ¯ Docker å®¹å™¨
    if session.container_id:
        await self._scheduler.destroy_container(session.container_id)

    # æ¸…ç† S3 æ–‡ä»¶
    if self._storage_service and session.workspace_path.startswith("s3://"):
        deleted_count = await self._storage_service.delete_prefix(
            session.workspace_path
        )
        logger.info(f"Deleted {deleted_count} files for session {session_id}")

    # æ›´æ–°ä¼šè¯çŠ¶æ€
    session.mark_as_terminated()
    await self._session_repo.save(session)

    return SessionDTO.from_entity(session)
```

#### 4.4.6 é…ç½®è¯´æ˜

**ç¯å¢ƒå˜é‡ï¼ˆJuiceFS hostPath æ–¹å¼ï¼‰**ï¼š

```bash
# S3/MinIO é…ç½®ï¼ˆç”¨äº JuiceFS æ–‡ä»¶æ•°æ®å­˜å‚¨ï¼‰
S3_BUCKET=sandbox-workspace          # å­˜å‚¨æ¡¶åç§°
S3_REGION=us-east-1                 # åŒºåŸŸ
S3_ACCESS_KEY_ID=minioadmin         # è®¿é—®å¯†é’¥ ID
S3_SECRET_ACCESS_KEY=minioadmin     # è®¿é—®å¯†é’¥
S3_ENDPOINT_URL=http://minio:9000   # MinIO ç«¯ç‚¹ï¼ˆAWS S3 ç•™ç©ºï¼‰

# JuiceFS hostPath é…ç½®
JUICEFS_HOST_PATH=/mnt/jfs/sandbox-workspace
JUICEFS_METAURL=mysql://root:password@mariadb.sandbox-system.svc.cluster.local:3306/juicefs_metadata
JUICEFS_STORAGE_TYPE=minio
JUICEFS_BUCKET=http://minio.sandbox-system.svc.cluster.local:9000/sandbox-workspace
JUICEFS_ACCESS_KEY=minioadmin
JUICEFS_SECRET_KEY=minioadmin
```

**K8s ConfigMap é…ç½®**ï¼š

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: sandbox-config
  namespace: sandbox-system
data:
  # JuiceFS hostPath é…ç½®
  JUICEFS_HOST_PATH: "/mnt/jfs/sandbox-workspace"
  JUICEFS_METAURL: "mysql://root:password@mariadb.sandbox-system.svc.cluster.local:3306/juicefs_metadata"
  JUICEFS_STORAGE_TYPE: "minio"
  JUICEFS_BUCKET: "http://minio.sandbox-system.svc.cluster.local:9000/sandbox-workspace"
```

**K8s Secret é…ç½®**ï¼š

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: sandbox-secrets
  namespace: sandbox-system
type: Opaque
stringData:
  # S3 å‡­è¯ï¼ˆç”¨äº JuiceFSï¼‰
  S3_ACCESS_KEY_ID: minioadmin
  S3_SECRET_ACCESS_KEY: minioadmin
```

**éƒ¨ç½²é¡ºåº**ï¼š

```bash
# 1. åŸºç¡€èµ„æº
kubectl apply -f 00-namespace.yaml
kubectl apply -f 01-configmap.yaml
kubectl apply -f 02-secret.yaml
kubectl apply -f 03-serviceaccount.yaml
kubectl apply -f 04-role.yaml

# 2. å­˜å‚¨å±‚
kubectl apply -f 08-mariadb-deployment.yaml
kubectl apply -f 07-minio-deployment.yaml

# 3. JuiceFS æ•°æ®åº“åˆå§‹åŒ–
kubectl apply -f 09-juicefs-setup.yaml

# 4. JuiceFS hostPath æŒ‚è½½åŠ©æ‰‹
kubectl apply -f 10-juicefs-hostpath-setup.yaml

# 5. Control Plane
kubectl apply -f 05-control-plane-deployment.yaml
```

#### 4.4.7 éƒ¨ç½²éªŒè¯

**æ£€æŸ¥ JuiceFS æŒ‚è½½åŠ©æ‰‹**ï¼š

```bash
# æ£€æŸ¥ DaemonSet çŠ¶æ€
kubectl get ds -n sandbox-system juicefs-mount-helper

# æ£€æŸ¥ Pod çŠ¶æ€
kubectl get pods -n sandbox-system -l app=juicefs-mount-helper

# éªŒè¯æŒ‚è½½ç‚¹
MOUNT_HELPER_POD=$(kubectl get pods -n sandbox-system -l app=juicefs-mount-helper -o jsonpath='{.items[0].metadata.name}')
kubectl exec -n sandbox-system "$MOUNT_HELPER_POD" -- ls -la /jfs/sandbox-workspace
```

**æ£€æŸ¥æ•°æ®åº“**ï¼š

```bash
# éªŒè¯ JuiceFS æ•°æ®åº“
kubectl exec -n sandbox-system deployment/mariadb -- \
  mariadb -u root -p"password" -e "SHOW DATABASES LIKE 'juicefs%';"
```

**æµ‹è¯• S3 æŒ‚è½½**ï¼š

```bash
# åˆ›å»ºæµ‹è¯•ä¼šè¯ï¼ˆS3 workspaceï¼‰
curl -X POST http://localhost:8000/api/v1/sessions \
  -H "Content-Type: application/json" \
  -d '{
    "template_id": "python-basic",
    "workspace_path": "s3://sandbox-workspace/sessions/test-001/"
  }'

# æ£€æŸ¥ Pod æ˜¯å¦åˆ›å»º
kubectl get pods -n sandbox-runtime -l sandbox-session=test-001

# æ£€æŸ¥æŒ‚è½½ç‚¹
TEST_POD=$(kubectl get pods -n sandbox-runtime -l sandbox-session=test-001 -o jsonpath='{.items[0].metadata.name}')
kubectl exec -n sandbox-runtime "$TEST_POD" -- ls -la /workspace
```
      - S3_SECRET_ACCESS_KEY=minioadmin
      - S3_BUCKET=sandbox-workspace
      - S3_REGION=us-east-1
    depends_on:
      minio:
        condition: service_healthy

  minio:
    image: quay.io/minio/minio:latest
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
      MINIO_DEFAULT_BUCKETS: sandbox-workspace
    command: server /data --console-address ":9001"
```

#### 4.4.7 å®‰å…¨è€ƒè™‘

| å®‰å…¨æªæ–½ | è¯´æ˜ |
|----------|------|
| **SYS_ADMIN èƒ½åŠ›** | ä»…åœ¨ S3 workspace æ¨¡å¼ä¸‹æ·»åŠ ï¼Œå…¶ä»–æ¨¡å¼ä½¿ç”¨ CapDrop=ALL |
| **ç”¨æˆ·é™æƒ** | æŒ‚è½½åä½¿ç”¨ gosu åˆ‡æ¢åˆ° UID:GID 1000:1000 |
| **å‡­è¯ç®¡ç†** | ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ IAM Roles æˆ– Docker Secrets |
| **ç½‘ç»œéš”ç¦»** | å®¹å™¨ NetworkMode=none æˆ–ç‹¬ç«‹ç½‘ç»œ |
| **èµ„æºé™åˆ¶** | tmpfs 100M é™åˆ¶ s3fs ç¼“å­˜å¤§å° |

#### 4.4.8 æ•…éšœå¤„ç†

| æ•…éšœåœºæ™¯ | å¤„ç†æ–¹å¼ |
|----------|----------|
| **S3 ä¸å¯è¾¾** | s3fs æŒ‚è½½å¤±è´¥ï¼Œå®¹å™¨å¯åŠ¨å¤±è´¥ï¼Œsession æ ‡è®°ä¸º failed |
| **æƒé™ä¸è¶³** | æŒ‚è½½å¤±è´¥ï¼Œæ£€æŸ¥ S3 å‡­è¯é…ç½® |
| **æ–‡ä»¶è¿‡å¤§** | ä½¿ç”¨é¢„ç­¾å URLï¼Œç»•è¿‡ Control Plane |
| **å¹¶å‘å†™å…¥** | s3fs æ”¯æŒå¹¶å‘ï¼Œä½†æ€§èƒ½å—é™ï¼Œå»ºè®®æ–‡ä»¶æ“ä½œä¸²è¡ŒåŒ– |

#### 4.4.9 æ€§èƒ½ä¼˜åŒ–

| ä¼˜åŒ–é¡¹ | æ–¹æ¡ˆ |
|--------|------|
| **å°æ–‡ä»¶ä¸Šä¼ ** | ç›´æ¥ä½¿ç”¨ boto3 put_objectï¼ˆ<5MBï¼‰ |
| **å¤§æ–‡ä»¶ä¸Šä¼ ** | ä½¿ç”¨ boto3 upload_fileï¼ˆåˆ†ç‰‡ä¸Šä¼ ï¼‰ |
| **æ–‡ä»¶ä¸‹è½½** | <10MB ç›´æ¥è¿”å›ï¼Œ>10MB ä½¿ç”¨é¢„ç­¾å URL |
| **ç¼“å­˜ç­–ç•¥** | s3fs ä½¿ç”¨ /tmp ç¼“å­˜ï¼ˆ100M tmpfsï¼‰ |
| **å¹¶å‘é™åˆ¶** | é™åˆ¶å•ä¸ª session çš„å¹¶å‘æ–‡ä»¶æ“ä½œæ•° |

#### 4.4.10 æµ‹è¯•éªŒè¯

**E2E æµ‹è¯•åœºæ™¯**ï¼š

```python
async def test_e2e_s3_workspace():
    """S3 workspace æŒ‚è½½ç«¯åˆ°ç«¯æµ‹è¯•"""

    # 1. åˆ›å»ºä¼šè¯ï¼ˆS3 workspaceï¼‰
    session = await create_session(template_id="python-basic")
    assert session.workspace_path.startswith("s3://")

    # 2. ä¸Šä¼ æ–‡ä»¶
    await upload_file(session.id, "uploads/data.csv", content)
    file_info = await s3_storage.file_exists(f"{session.workspace_path}/uploads/data.csv")
    assert file_info is True

    # 3. åœ¨å®¹å™¨å†…è¯»å–ä¸Šä¼ çš„æ–‡ä»¶
    result = await execute_code(session.id, """
        import os
        with open('/workspace/uploads/data.csv', 'r') as f:
            content = f.read()
        print(content)
    """)
    assert content in result.stdout

    # 4. åœ¨å®¹å™¨å†…å†™å…¥æ–°æ–‡ä»¶
    await execute_code(session.id, """
        with open('/workspace/artifacts/output.txt', 'w') as f:
            f.write('generated content')
    """)

    # 5. ä¸‹è½½ç”Ÿæˆçš„æ–‡ä»¶
    downloaded = await download_file(session.id, "artifacts/output.txt")
    assert downloaded == "generated content"

    # 6. åˆ é™¤ä¼šè¯ï¼ŒéªŒè¯ S3 æ–‡ä»¶è¢«æ¸…ç†
    await terminate_session(session.id)
    files = await s3_storage.list_files(f"{session.workspace_path}/")
    assert len(files) == 0
```

## 5. Python ä¸‰æ–¹ä¾èµ–å®‰è£…æŠ€æœ¯æ–¹æ¡ˆ

### 5.1 æ¦‚è¿°

æœ¬ç« èŠ‚æè¿°æ²™ç®±å¹³å°çš„ **Python ä¸‰æ–¹ä¾èµ–è‡ªåŠ¨å®‰è£…åŠŸèƒ½**ã€‚è¯¥åŠŸèƒ½å…è®¸åœ¨åˆ›å»ºä¼šè¯æ—¶æŒ‡å®šéœ€è¦å®‰è£…çš„ Python åŒ…ï¼Œç³»ç»Ÿä¼šåœ¨å®¹å™¨å¯åŠ¨æ—¶è‡ªåŠ¨å®‰è£…è¿™äº›ä¾èµ–ï¼Œå¹¶æŒä¹…åŒ–åˆ° S3 workspaceã€‚

#### 5.1.1 æ ¸å¿ƒéœ€æ±‚

| éœ€æ±‚é¡¹ | è¯´æ˜ |
|-------|------|
| **å®‰è£…æ—¶æœº** | å®¹å™¨å¯åŠ¨æ—¶ï¼ˆåœ¨ entrypoint è„šæœ¬ä¸­è‡ªåŠ¨å®Œæˆï¼‰ |
| **ä½œç”¨åŸŸ** | ä¼šè¯çº§åˆ«ï¼ˆå®‰è£…åˆ°å®¹å™¨æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿï¼‰ |
| **å®‰è£…ä½ç½®** | `/opt/sandbox-venv/`ï¼ˆæœ¬åœ°ç£ç›˜ï¼ŒéæŒä¹…åŒ–ï¼‰ |
| **ç½‘ç»œè®¿é—®** | å®¹å™¨ä½¿ç”¨ `sandbox_network`ï¼ˆbridgeï¼‰ï¼Œå¯è®¿é—® PyPI |
| **ç”¨æˆ·ä»£ç éš”ç¦»** | ç”¨æˆ·ä»£ç ä»ç„¶é€šè¿‡ Bubblewrap æ‰§è¡Œï¼ˆ`--unshare-net` æ— ç½‘ç»œï¼‰ |

#### 5.1.2 æ¶æ„å…¼å®¹æ€§

**ä¿æŒä¸å˜çš„ç°æœ‰é€»è¾‘**ï¼š
- âœ… `sandbox_network` (bridge)ï¼šå®¹å™¨é—´é€šä¿¡ï¼Œexecutor â†” control_plane
- âœ… `_build_s3_mount_entrypoint()`ï¼šS3 bucket æŒ‚è½½é€»è¾‘
- âœ… `gosu sandbox`ï¼šç”¨æˆ·åˆ‡æ¢é€»è¾‘
- âœ… Bubblewrapï¼šç”¨æˆ·ä»£ç æ‰§è¡Œéš”ç¦»ï¼ˆ`--unshare-net`ï¼‰

**æ–°å¢é€»è¾‘**ï¼š
- âœ… åœ¨ `gosu` åˆ‡æ¢å‰ï¼Œä»¥ root èº«ä»½å®‰è£…ä¾èµ–åˆ° `/opt/sandbox-venv/`ï¼ˆæœ¬åœ°ç£ç›˜ï¼‰
- âœ… å®‰è£…å®Œæˆå `chown -R sandbox:sandbox /opt/sandbox-venv/`
- âœ… ç„¶åæ‰§è¡Œ `gosu sandbox` å¯åŠ¨ executor
- âœ… è®¾ç½® `PYTHONPATH=/opt/sandbox-venv:/app:/workspace` ä½¿ä¾èµ–å¯å¯¼å…¥

### 5.2 æ ¸å¿ƒæµç¨‹

```mermaid
sequenceDiagram
    participant Client as ğŸ‘¤ Client
    participant API as ğŸŒ API Gateway
    participant SM as ğŸ“¦ Session Manager
    participant Scheduler as âš™ï¸ Scheduler
    participant Docker as ğŸ³ Docker Scheduler
    participant Entrypoint as ğŸ“œ Entrypoint Script
    participant Pip as ğŸ“¦ pip install
    participant Executor as âš¡ sandbox-executor
    participant S3 as ğŸ“¦ S3 Workspace

    Client->>API: POST /api/v1/sessions<br/>{dependencies: [{"name": "requests", "version": "==2.31.0"}]}
    API->>SM: create_session(command)

    SM->>SM: è§£æä¾èµ–åˆ—è¡¨<br/>dependencies = ["requests==2.31.0"]
    SM->>SM: ç‰ˆæœ¬å†²çªæ£€æµ‹ï¼ˆTemplate vs ç”¨æˆ·è¯·æ±‚ï¼‰

    SM->>Scheduler: schedule(config)
    Scheduler-->>SM: runtime_node

    SM->>Docker: create_container(config)<br/>config.labels["dependencies"] = ["requests==2.31.0"]

    Note over Docker: å®¹å™¨å¯åŠ¨ï¼ˆroot ç”¨æˆ·ï¼‰
    Docker->>Entrypoint: æ‰§è¡Œ entrypoint è„šæœ¬

    Entrypoint->>Entrypoint: 1. æŒ‚è½½ S3 bucketï¼ˆs3fsï¼‰
    Entrypoint->>Entrypoint: 2. åˆ›å»º /workspace ç¬¦å·é“¾æ¥
    Entrypoint->>Entrypoint: 3. æ£€æŸ¥ dependencies ç¯å¢ƒå˜é‡

    alt æœ‰ä¾èµ–éœ€è¦å®‰è£…
        Entrypoint->>Entrypoint: mkdir -p /opt/sandbox-venv/
        Entrypoint->>Pip: pip3 install --target /opt/sandbox-venv/ requests==2.31.0
        Pip->>Pip: ä» PyPI ä¸‹è½½å¹¶å®‰è£…
        Pip->>Entrypoint: å†™å…¥åˆ° /opt/sandbox-venv/ï¼ˆæœ¬åœ°ç£ç›˜ï¼‰
        Pip-->>Entrypoint: å®‰è£…æˆåŠŸ
        Entrypoint->>Entrypoint: chown -R sandbox:sandbox /opt/sandbox-venv/
    end

    Entrypoint->>Entrypoint: 4. gosu sandbox å¯åŠ¨ executor
    Entrypoint->>Executor: exec gosu sandbox python -m executor.interfaces.http.rest

    Executor->>API: ä¸ŠæŠ¥ readyï¼ˆä¾èµ–å·²å®‰è£…å®Œæˆï¼‰
    API-->>Client: Session created (status: running)

    Note over Client,S3: ç”¨æˆ·ä»£ç æ‰§è¡Œæ—¶ï¼Œå¯é€šè¿‡ import<br/>import requests  # å·²å®‰è£…åˆ° .venv/
```

### 5.3 API è®¾è®¡

#### 5.3.1 æ‰©å±•åˆ›å»ºä¼šè¯æ¥å£

**è¯·æ±‚æ¨¡å‹**ï¼š

```python
from typing import List, Optional
from pydantic import BaseModel, Field, field_validator

class DependencySpec(BaseModel):
    """ä¾èµ–åŒ…è§„èŒƒ"""
    name: str = Field(..., min_length=1, max_length=100, description="åŒ…åç§°")
    version: Optional[str] = Field(None, description="ç‰ˆæœ¬çº¦æŸ (å¦‚: ==2.31.0, >=1.0)")

    @field_validator("name")
    @classmethod
    def validate_package_name(cls, v: str) -> str:
        import re
        # ç¦æ­¢è·¯å¾„ç©¿è¶Š
        if ".." in v or v.startswith("/"):
            raise ValueError("Package name cannot contain path traversal characters")
        # ç¦æ­¢ URL
        if "://" in v:
            raise ValueError("Package name cannot contain URL")
        # PyPI åŒ…åè§„èŒƒ
        if not re.match(r"^[a-zA-Z0-9._-]+$", v):
            raise ValueError("Invalid package name format")
        return v

    def to_pip_spec(self) -> str:
        """è½¬æ¢ä¸º pip å®‰è£…è§„èŒƒ"""
        if self.version:
            return f"{self.name}{self.version}"
        return self.name

class CreateSessionRequest(BaseModel):
    """åˆ›å»ºä¼šè¯è¯·æ±‚ï¼ˆæ‰©å±•ç‰ˆï¼‰"""
    template_id: str
    timeout: int = 300
    cpu: str = "1"
    memory: str = "512Mi"
    disk: str = "1Gi"
    env_vars: Dict[str, str] = {}

    # æ–°å¢å­—æ®µ
    dependencies: List[DependencySpec] = Field(
        default_factory=list,
        max_length=50,
        description="ä¼šè¯çº§ä¾èµ–åŒ…åˆ—è¡¨"
    )
    install_timeout: int = Field(
        300,
        ge=30,
        le=1800,
        description="ä¾èµ–å®‰è£…è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰"
    )
    fail_on_dependency_error: bool = Field(
        True,
        description="ä¾èµ–å®‰è£…å¤±è´¥æ—¶æ˜¯å¦ç»ˆæ­¢ä¼šè¯åˆ›å»º"
    )
    allow_version_conflicts: bool = Field(
        False,
        description="æ˜¯å¦å…è®¸ç‰ˆæœ¬å†²çªï¼ˆTemplate é¢„è£…åŒ… vs ç”¨æˆ·è¯·æ±‚åŒ…ï¼‰"
    )
```

**è¯·æ±‚ç¤ºä¾‹**ï¼š

```bash
curl -X POST http://localhost:8000/api/v1/sessions \
  -H "Content-Type: application/json" \
  -d '{
    "template_id": "python3.11-baseline",
    "dependencies": [
      {"name": "requests", "version": "==2.31.0"},
      {"name": "pandas", "version": ">=2.0"},
      {"name": "numpy"}
    ],
    "install_timeout": 600,
    "fail_on_dependency_error": true
  }'
```

### 5.4 Docker Scheduler å®ç°

#### 5.4.1 ä¿®æ”¹ `_build_s3_mount_entrypoint()` æ–¹æ³•

**æ–‡ä»¶**: `sandbox_control_plane/src/infrastructure/container_scheduler/docker_scheduler.py`

**ç°æœ‰ç­¾å**ï¼š

```python
def _build_s3_mount_entrypoint(
    self,