# æ²™ç®±å¹³å°æŠ€æœ¯æ–¹æ¡ˆè®¾è®¡- V2.1
## 1. æ¶æ„è®¾è®¡
### 1.1 æ•´ä½“æ¶æ„

ç³»ç»Ÿé‡‡ç”¨ç®¡ç†ä¸­å¿ƒï¼ˆControl Planeï¼‰ä¸å®¹å™¨è°ƒåº¦å™¨ï¼ˆContainer Schedulerï¼‰åˆ†ç¦»çš„äº‘åŸç”Ÿæ¶æ„ï¼Œæ”¯æŒ Docker å’Œ Kubernetes ä¸¤ç§éƒ¨ç½²æ¨¡å¼ã€‚
æ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼š

- æ§åˆ¶å¹³é¢æ— çŠ¶æ€ï¼Œæ”¯æŒæ°´å¹³æ‰©å±•
- å®¹å™¨è°ƒåº¦å™¨æ± åŒ–ç®¡ç†ï¼ŒåŠ¨æ€ä¼¸ç¼©
- åè®®é©±åŠ¨çš„è§£è€¦è®¾è®¡
- å¤šå±‚å®‰å…¨éš”ç¦»
- å¼‚æ­¥é«˜å¹¶å‘å¤„ç†


### 1.2 C4 æ¶æ„æ¨¡å‹
#### C4 Level 1: ç³»ç»Ÿä¸Šä¸‹æ–‡
```mermaid
graph TB
    subgraph External["å¤–éƒ¨ç³»ç»Ÿ"]
        ä¸Šå±‚æœåŠ¡["Data Agent/Operator Platform"]
        Dev["å¼€å‘è€…<br/>(é€šè¿‡ SDK/API)"]
        K8s["Kubernetes é›†ç¾¤"]
        Docker["Docker Engine"]
    end

    subgraph SandboxPlatform["Python æ²™ç®±å¹³å°"]
        ControlPlane["ç®¡ç†ä¸­å¿ƒ<br/>(Control Plane)"]
        ContainerScheduler["Container Scheduler æ¨¡å—"]
    end

    ä¸Šå±‚æœåŠ¡ -->|æ‰§è¡Œä»£ç è¯·æ±‚| ControlPlane
    Dev -->|SDK/API è°ƒç”¨| ControlPlane
    ContainerScheduler -->|ç›´æ¥è°ƒç”¨| K8s
    ContainerScheduler -->|ç›´æ¥è®¿é—®| Docker
    ContainerScheduler -->|ä¸ŠæŠ¥ç»“æœ| ControlPlane

    style SandboxPlatform fill:#e1f5ff
    style External fill:#fff4e6
```
å¤–éƒ¨äº¤äº’ï¼š

- DataAgent/Operator Platform ç³»ç»Ÿé€šè¿‡ RESTful API å‘èµ·ä»£ç æ‰§è¡Œè¯·æ±‚
- å¼€å‘è€…é€šè¿‡ Python SDK é›†æˆæ²™ç®±èƒ½åŠ›
- ä¾èµ– Kubernetes/Docker æä¾›å®¹å™¨åŸºç¡€è®¾æ–½


#### C4 Level 2: å®¹å™¨è§†å›¾
```mermaid
graph TB
    subgraph ControlPlane["ç®¡ç†ä¸­å¿ƒ (Control Plane)"]
        API["API Gateway<br/>(FastAPI)"]
        Scheduler["è°ƒåº¦å™¨<br/>(Scheduler)"]
        SessionMgr["ä¼šè¯ç®¡ç†å™¨<br/>(Session Manager)"]
        TemplateMgr["æ¨¡æ¿ç®¡ç†å™¨<br/>(Template Manager)"]
        Monitor["ç›‘æ§æ¢é’ˆ<br/>(Health Probe)"]
        ResultStore["ç»“æœå­˜å‚¨<br/>(Result Store)"]
        SessionCleanup["ä¼šè¯æ¸…ç†æœåŠ¡<br/>(Session Cleanup)"]
    end

    subgraph ContainerScheduler["Container Scheduler æ¨¡å—"]
        DockerRuntime["Docker Scheduler"]
        K8sRuntime["K8s Scheduler"]
    end

    subgraph Sandbox["æ²™ç®±å®ä¾‹"]
        Container["å®¹å™¨<br/>(Docker/Pod)"]
        BubbleWrap["Bubblewrap<br/>(è¿›ç¨‹éš”ç¦»)"]
        Executor["æ‰§è¡Œå™¨<br/>(Code Executor)"]
    end

    subgraph Storage["å­˜å‚¨å±‚"]
        MariaDB["MariaDB<br/>(ä¼šè¯çŠ¶æ€/æ¨¡æ¿)"]
        S3["å¯¹è±¡å­˜å‚¨ S3<br/>(workspace æ–‡ä»¶)"]
        Etcd["Etcd<br/>(é…ç½®ä¸­å¿ƒ)"]
    end

    API --> Scheduler
    API --> SessionMgr
    API --> TemplateMgr
    Scheduler --> Monitor
    Scheduler --> DockerRuntime
    Scheduler --> K8sRuntime
    SessionMgr --> MariaDB
    TemplateMgr --> MariaDB
    Monitor --> DockerRuntime
    Monitor --> K8sRuntime
    ResultStore --> S3
    SessionCleanup --> SessionMgr

    DockerRuntime --> Container
    K8sRuntime --> Container
    Container --> BubbleWrap
    BubbleWrap --> Executor
    Executor -->|ä¸ŠæŠ¥ç»“æœ| API

    TemplateMgr --> Etcd

    style ControlPlane fill:#bbdefb
    style ContainerScheduler fill:#c8e6c9
    style Sandbox fill:#fff9c4
    style Storage fill:#f8bbd0
```
å…³é”®å®¹å™¨ï¼š

- API Gateway: ç»Ÿä¸€å…¥å£ï¼ŒåŸºäº FastAPI å®ç°
- è°ƒåº¦å™¨: æ™ºèƒ½ä»»åŠ¡åˆ†å‘å’Œèµ„æºè°ƒåº¦
- ä¼šè¯ç®¡ç†å™¨: ä¼šè¯ç”Ÿå‘½å‘¨æœŸç®¡ç†
- Container Scheduler: Docker/K8s è¿è¡Œæ—¶å®ä¾‹ç®¡ç†
- å­˜å‚¨å±‚ï¼š
  - MariaDBï¼ˆä¼šè¯çŠ¶æ€/æ¨¡æ¿/æ‰§è¡Œè®°å½•ï¼‰
  - S3 å¯¹è±¡å­˜å‚¨ï¼ˆworkspace æ–‡ä»¶ï¼Œé€šè¿‡ Volume æŒ‚è½½åˆ°å®¹å™¨ï¼‰
  - Etcdï¼ˆé…ç½®ä¸­å¿ƒï¼‰

**å­˜å‚¨æ¶æ„è¯´æ˜**ï¼š
- workspace ç›®å½•é€šè¿‡ S3 CSI Driver æˆ–ç±»ä¼¼æœºåˆ¶æŒ‚è½½ä¸ºå®¹å™¨ Volume
- æ‰§è¡Œæ—¶ç”Ÿæˆçš„æ–‡ä»¶ç›´æ¥å†™å…¥ workspaceï¼Œè‡ªåŠ¨æŒä¹…åŒ–åˆ° S3
- MariaDB å­˜å‚¨ stdoutã€stderrã€æ‰§è¡ŒçŠ¶æ€å’Œæ–‡ä»¶åˆ—è¡¨ï¼ˆartifactsï¼‰
- ä¸‹è½½æ–‡ä»¶æ—¶é€šè¿‡æ–‡ä»¶ API ç›´æ¥ä» S3 è·å–

#### éƒ¨ç½²æ¶æ„
```mermaid
graph TB
    subgraph Internet["ğŸŒ äº’è”ç½‘"]
        User["ğŸ‘¤ å¼€å‘è€…/Agentç³»ç»Ÿ"]
    end
    
    subgraph K8sCluster["â˜¸ï¸ Kubernetes é›†ç¾¤"]
        
        subgraph IngressLayer["å…¥å£å±‚"]
            Ingress["Ingress Controller<br/>Nginx/Traefik"]
            LB["Load Balancer<br/>L4/L7"]
        end
        
        subgraph ControlPlaneNS["ğŸ“¦ Namespace: sandbox-system"]
            subgraph ControlPlaneDeployment["Deployment: control-plane"]
                CP1["Pod: control-plane-1<br/>â”œâ”€ API Gateway<br/>â”œâ”€ Scheduler<br/>â”œâ”€ Session Manager<br/>â””â”€ Health Probe"]
                CP2["Pod: control-plane-2<br/>â”œâ”€ API Gateway<br/>â”œâ”€ Scheduler<br/>â”œâ”€ Session Manager<br/>â””â”€ Health Probe"]
                CP3["Pod: control-plane-3<br/>â”œâ”€ API Gateway<br/>â”œâ”€ Scheduler<br/>â”œâ”€ Session Manager<br/>â””â”€ Health Probe"]
            end
            
            CPService["Service: control-plane-svc<br/>Type: ClusterIP<br/>Port: 8000"]
            
            HPA["HPA<br/>Min: 3, Max: 10<br/>CPU Target: 70%"]
        end
        
        subgraph RuntimeNS["ğŸ”’ Namespace: sandbox-runtime"]

            subgraph ActiveSandboxGroup["æ´»è·ƒæ²™ç®±"]
                SB1["Pod: sandbox-abc123<br/>â”œâ”€ Session: abc123<br/>â”œâ”€ Status: Executing<br/>â””â”€ CPU: 0.8, Mem: 400Mi"]
                SB2["Pod: sandbox-def456<br/>â”œâ”€ Session: def456<br/>â”œâ”€ Status: Idle<br/>â””â”€ CPU: 0.1, Mem: 200Mi"]
                SB3["Pod: sandbox-xyz789<br/>â”œâ”€ Session: xyz789<br/>â”œâ”€ Status: Executing<br/>â””â”€ CPU: 1.0, Mem: 512Mi"]
            end

            NetworkPolicy["NetworkPolicy<br/>- ç¦æ­¢ Pod é—´é€šä¿¡<br/>- ä»…å…è®¸è®¿é—®ç®¡ç†ä¸­å¿ƒ<br/>- å¯é€‰ç™½åå•å¤–éƒ¨è®¿é—®"]
        end
        
        subgraph DataLayer["ğŸ’¾ æ•°æ®å±‚ - Namespace: data"]
            
            subgraph MariaDBCluster["StatefulSet: MariaDB Cluster"]
                DB1["mariadb-0<br/>Role: Primary"]
                DB2["mariadb-1<br/>Role: Replica"]
                DB3["mariadb-2<br/>Role: Replica"]
            end
            
            subgraph EtcdCluster["StatefulSet: Etcd Cluster"]
                Etcd1["etcd-0"]
                Etcd2["etcd-1"]
                Etcd3["etcd-2"]
            end
            
            MariaDBService["Service: mariadb-svc<br/>Port: 3306"]
            EtcdService["Service: etcd-svc"]
        end
        
    end
    
    subgraph ExternalServices["â˜ï¸ å¤–éƒ¨æœåŠ¡"]
        S3["S3 / MinIO<br/>- æ‰§è¡Œç»“æœå­˜å‚¨<br/>- ç”Ÿæˆæ–‡ä»¶å­˜å‚¨<br/>- æ—¥å¿—å½’æ¡£"]
        Registry["Container Registry<br/>- Docker Hub<br/>- Harbor<br/>- ç§æœ‰é•œåƒä»“åº“"]
    end
    
    User -->|"HTTPS<br/>TLS 1.3"| Ingress
    Ingress --> LB
    LB --> CPService
    CPService --> CP1
    CPService --> CP2
    CPService --> CP3
    
    HPA -.->|ç›‘æ§å‰¯æœ¬æ•°| ControlPlaneDeployment
    
    CP1 -->|"æŸ¥è¯¢/å†™å…¥<br/>ä¼šè¯çŠ¶æ€ / å…ƒæ•°æ®"| MariaDBService
    CP2 -->|"æŸ¥è¯¢/å†™å…¥<br/>ä¼šè¯çŠ¶æ€ / å…ƒæ•°æ®"| MariaDBService
    CP3 -->|"æŸ¥è¯¢/å†™å…¥<br/>ä¼šè¯çŠ¶æ€ / å…ƒæ•°æ®"| MariaDBService
    
    MariaDBService --> DB1
    DB1 -.->|ä¸»ä»å¤åˆ¶| DB2
    DB1 -.->|ä¸»ä»å¤åˆ¶| DB3
    
    CP1 -->|"è¯»å–é…ç½®<br/>æ¨¡æ¿ä¿¡æ¯"| EtcdService
    CP2 -->|"è¯»å–é…ç½®<br/>æ¨¡æ¿ä¿¡æ¯"| EtcdService
    EtcdService --> Etcd1
    EtcdService --> Etcd2
    EtcdService --> Etcd3

    CP1 -.->|"K8s API<br/>åˆ›å»º Pod"| SB1
    CP2 -.->|"K8s API<br/>åˆ›å»º Pod"| SB2
    CP3 -.->|"K8s API<br/>åˆ›å»º Pod"| SB3
    
    NetworkPolicy -.->|é™åˆ¶| SB1
    NetworkPolicy -.->|é™åˆ¶| SB2
    NetworkPolicy -.->|é™åˆ¶| SB3
    
    SB1 -->|"ä¸ŠæŠ¥ç»“æœ<br/>S3 API"| S3
    SB2 -->|"ä¸ŠæŠ¥ç»“æœ<br/>S3 API"| S3
    SB3 -->|"ä¸ŠæŠ¥ç»“æœ<br/>S3 API"| S3
    
    CP1 -.->|"æ‹‰å–é•œåƒ"| Registry
    Warm1 -.->|"åŸºç¡€é•œåƒ"| Registry
    SB1 -.->|"ç”¨æˆ·é•œåƒ"| Registry
    
    
    classDef ingressStyle fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef controlStyle fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef runtimeStyle fill:#e8f5e9,stroke:#1b5e20,stroke-width:2px
    classDef dataStyle fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef externalStyle fill:#f1f8e9,stroke:#33691e,stroke-width:2px
    
    class Ingress,LB ingressStyle
    class CP1,CP2,CP3,CPService,HPA controlStyle
    class SB1,SB2,SB3,NetworkPolicy runtimeStyle
    class DB1,DB2,DB3,Etcd1,Etcd2,Etcd3,MariaDBService,EtcdService dataStyle
    class S3,Registry externalStyle


```
## 2. å…³é”®ç»„ä»¶è®¾è®¡
### 2.1 ç®¡ç†ä¸­å¿ƒ (Control Plane)
#### 2.1.1 API Gateway
æŠ€æœ¯æ ˆï¼š FastAPI + Uvicorn + asyncio
èŒè´£ï¼š

- æä¾›ç»Ÿä¸€çš„ RESTful API æ¥å£
- è¯·æ±‚éªŒè¯ã€é‰´æƒã€é™æµ
- åè®®è½¬æ¢å’Œè¯·æ±‚è·¯ç”±

æ ¸å¿ƒæ¥å£ï¼š

```
# ä¼šè¯ç®¡ç†
POST   /api/v1/sessions                 # åˆ›å»ºä¼šè¯
GET    /api/v1/sessions/{id}            # æŸ¥è¯¢ä¼šè¯
DELETE /api/v1/sessions/{id}            # ç»ˆæ­¢ä¼šè¯

# æ‰§è¡Œç®¡ç†
POST   /api/v1/sessions/{id}/execute    # æäº¤æ‰§è¡Œä»»åŠ¡
GET    /api/v1/sessions/{id}/status     # æŸ¥è¯¢æ‰§è¡ŒçŠ¶æ€
GET    /api/v1/sessions/{id}/result     # è·å–æ‰§è¡Œç»“æœ

# æ¨¡æ¿ç®¡ç†
POST   /api/v1/templates                # åˆ›å»ºæ¨¡æ¿
GET    /api/v1/templates                # åˆ—å‡ºæ¨¡æ¿
GET    /api/v1/templates/{id}           # è·å–æ¨¡æ¿è¯¦æƒ…
```
è¯·æ±‚æ¨¡å¼ï¼š
```
class CreateSessionRequest(BaseModel):
    template_id: str
    timeout: int = 300  # ç§’
    resources: ResourceLimit
    env_vars: Dict[str, str] = {}

class ExecuteRequest(BaseModel):
    code: str
    language: Literal["python", "javascript", "shell"]
    async_mode: bool = False
    stdin: Optional[str] = None
    timeout: int = 30
```

ç›¸åº”æ¨¡å‹ï¼š
```
class SessionResponse(BaseModel):
    session_id: str
    status: SessionStatus
    created_at: datetime
    runtime_type: str
    node_id: str

class ExecutionResult(BaseModel):
    execution_id: str
    status: Literal["success", "failed", "timeout"]
    stdout: str
    stderr: str
    exit_code: int
    execution_time: float
    artifacts: List[str]  # ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„
```

#### 2.1.2 è°ƒåº¦å™¨ (Scheduler)

è°ƒåº¦å™¨è´Ÿè´£ä¸ºä¼šè¯è¯·æ±‚é€‰æ‹©æœ€ä¼˜çš„å®¹å™¨èŠ‚ç‚¹ã€‚ç³»ç»Ÿé‡‡ç”¨**æ— çŠ¶æ€æ¶æ„**ï¼Œå®¹å™¨æœ¬èº«ä¸å­˜å‚¨ä»»ä½•æ•°æ®ï¼Œæ‰€æœ‰çŠ¶æ€å­˜å‚¨åœ¨å¤–éƒ¨ S3 workspace ä¸­ã€‚

**æ— çŠ¶æ€æ¶æ„è¯´æ˜**ï¼š

- å®¹å™¨å®Œå…¨æ— çŠ¶æ€ï¼ˆæ•°æ®åœ¨ S3 workspaceï¼‰
- å®¹å™¨éšæ—¶å¯åˆ›å»º/é”€æ¯/é‡å»º
- èŠ‚ç‚¹æ•…éšœæ—¶å¯æ— ç¼è¿ç§»åˆ°å…¶ä»–èŠ‚ç‚¹
- è°ƒåº¦ä¸ä¾èµ–å†å²ç»‘å®šï¼ŒåŸºäºå½“å‰é›†ç¾¤çŠ¶æ€åšæœ€ä¼˜å†³ç­–

**è°ƒåº¦ç­–ç•¥**ï¼š

è°ƒåº¦åŸåˆ™ï¼š
1. ä¼˜å…ˆè€ƒè™‘æ¨¡æ¿äº²å’Œæ€§ï¼ˆé•œåƒå·²ç¼“å­˜ï¼‰
2. ä½¿ç”¨è´Ÿè½½å‡è¡¡ï¼ˆæ–°å»ºå®¹å™¨ï¼‰

#### 2.1.2.1 æ¨¡æ¿äº²å’Œæ€§

ä¼˜å…ˆé€‰æ‹©å·²ç¼“å­˜é•œåƒçš„èŠ‚ç‚¹ï¼š
- é¿å…é•œåƒæ‹‰å–ï¼ŒåŠ å¿«å¯åŠ¨é€Ÿåº¦
- å¯åŠ¨æ—¶é—´ï¼š1-2sï¼ˆvs å†·å¯åŠ¨ 2-5sï¼‰

#### 2.1.2.2 è´Ÿè½½å‡è¡¡

ç»¼åˆè€ƒè™‘ CPUã€å†…å­˜ã€ä¼šè¯æ•°ï¼š
- é€‰æ‹©è´Ÿè½½æœ€ä½çš„èŠ‚ç‚¹
- ç¡®ä¿é›†ç¾¤è´Ÿè½½å‡è¡¡

**è°ƒåº¦æµç¨‹å®ç°**ï¼š

```python
class Scheduler:
    async def schedule(self, request: CreateSessionRequest) -> RuntimeNode:
        """è°ƒåº¦é€»è¾‘ï¼ˆæ— çŠ¶æ€æ¶æ„ï¼‰"""

        # 1. è·å–æ‰€æœ‰å¥åº·èŠ‚ç‚¹
        nodes = await self.health_probe.get_healthy_nodes()

        # 2. é€‰æ‹©æœ€ä¼˜èŠ‚ç‚¹ï¼ˆè´Ÿè½½ + æ¨¡æ¿äº²å’Œæ€§ï¼‰
        best_node = await self._select_best_node(nodes, request)

        logger.info(f"Selected node {best_node.id} for session")
        return best_node

    async def _select_best_node(
        self,
        nodes: List[RuntimeNode],
        req: CreateSessionRequest
    ) -> RuntimeNode:
        """ç»¼åˆè¯„åˆ†ï¼ˆè´Ÿè½½ + æ¨¡æ¿äº²å’Œæ€§ï¼‰"""
        scored_nodes = [
            (node, self._calculate_score(node, req))
            for node in nodes
        ]

        best_node = max(scored_nodes, key=lambda x: x[1])[0]

        logger.info(
            f"Selected node {best_node.id} with score "
            f"{max(scored_nodes, key=lambda x: x[1])[1]:.2f}"
        )

        return best_node

    def _calculate_score(self, node: RuntimeNode, req: CreateSessionRequest) -> float:
        """è®¡ç®—ç»¼åˆè¯„åˆ†ï¼ˆè´Ÿè½½ + æ¨¡æ¿äº²å’Œæ€§ï¼‰"""

        # åŸºç¡€è´Ÿè½½è¯„åˆ† (æƒé‡ 0.7)
        cpu_score = (1 - node.cpu_usage) * 0.28   # 40% of 70%
        mem_score = (1 - node.mem_usage) * 0.28   # 40% of 70%
        session_score = (1 - node.session_count / node.max_sessions) * 0.14  # 20% of 70%
        load_score = cpu_score + mem_score + session_score

        # æ¨¡æ¿äº²å’Œæ€§è¯„åˆ† (æƒé‡ 0.3)
        affinity_score = 0.0

        # æ¨¡æ¿äº²å’Œæ€§ï¼ˆé•œåƒå·²ç¼“å­˜ï¼Œå¯åŠ¨æ›´å¿«ï¼‰
        if req.template_id in node.cached_templates:
            affinity_score += 0.3

        return load_score + affinity_score
```

**æ€§èƒ½ä¼˜åŒ–è·¯å¾„**ï¼š

```
æœ€ä¼˜ï¼šæ¨¡æ¿äº²å’ŒèŠ‚ç‚¹ï¼ˆ1-2sï¼Œé•œåƒç¼“å­˜ä½†å®¹å™¨æœªé¢„çƒ­ï¼‰
     â†“ æ— ç¼“å­˜
   æ¬¡ä¼˜ï¼šå†·å¯åŠ¨ï¼ˆ2-5sï¼‰
```

**æ— çŠ¶æ€æ¶æ„ä¼˜åŠ¿**ï¼š

- èŠ‚ç‚¹æ•…éšœæ—¶å¯åœ¨å…¶ä»–èŠ‚ç‚¹é‡å»ºå®¹å™¨
- è°ƒåº¦æ›´çµæ´»ï¼Œæ— å†å²ç»‘å®š
- æ”¯æŒä¼šè¯è¿ç§»
- å®Œå…¨å¼¹æ€§æ‰©å±•

#### 2.1.3 ä¼šè¯ç®¡ç†å™¨ (Session Manager)
çŠ¶æ€ç®¡ç†ï¼š

- ä½¿ç”¨ MariaDB å­˜å‚¨ä¼šè¯çŠ¶æ€å’Œæ¨¡æ¿ï¼ˆæ”¯æŒäº‹åŠ¡ã€å…³ç³»æŸ¥è¯¢ã€æ•°æ®ä¸€è‡´æ€§ï¼‰
- ä¼šè¯çŠ¶æ€æœºï¼šCreating â†’ Running â†’ Completed/Failed/Timeout
- ä½¿ç”¨ SQLAlchemy ORM + asyncpg (å¼‚æ­¥ PostgreSQL/MariaDB é©±åŠ¨)

æ•°æ®åº“è¡¨è®¾è®¡ï¼š
```sql
-- ä¼šè¯è¡¨
CREATE TABLE sessions (
    id VARCHAR(64) PRIMARY KEY,
    template_id VARCHAR(64) NOT NULL,
    status ENUM('creating', 'running', 'completed', 'failed', 'timeout', 'terminated') NOT NULL,
    runtime_type ENUM('docker', 'kubernetes') NOT NULL,
    runtime_node VARCHAR(128),           -- å½“å‰è¿è¡Œçš„èŠ‚ç‚¹ï¼ˆå¯ä¸ºç©ºï¼Œæ”¯æŒä¼šè¯è¿ç§»ï¼‰
    container_id VARCHAR(128),           -- å½“å‰å®¹å™¨ ID
    pod_name VARCHAR(128),               -- å½“å‰ Pod åç§°
    workspace_path VARCHAR(256),         -- S3 è·¯å¾„ï¼šs3://bucket/sessions/{session_id}/
    resources_cpu VARCHAR(16),
    resources_memory VARCHAR(16),
    resources_disk VARCHAR(16),
    env_vars JSON,
    timeout INT NOT NULL DEFAULT 300,
    last_activity_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,  -- æœ€åæ´»åŠ¨æ—¶é—´ï¼ˆç”¨äºè‡ªåŠ¨æ¸…ç†ï¼‰
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    completed_at TIMESTAMP NULL,
    INDEX idx_status (status),
    INDEX idx_template (template_id),
    INDEX idx_created (created_at),
    INDEX idx_runtime_node (runtime_node),  -- æ”¯æŒèŠ‚ç‚¹æ•…éšœæ—¶æŸ¥è¯¢ä¼šè¯
    INDEX idx_last_activity (last_activity_at)  -- æ”¯æŒè‡ªåŠ¨æ¸…ç†æŸ¥è¯¢
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

-- æ‰§è¡Œè®°å½•è¡¨
CREATE TABLE executions (
    id VARCHAR(64) PRIMARY KEY,
    session_id VARCHAR(64) NOT NULL,
    code TEXT NOT NULL,
    language VARCHAR(16) NOT NULL,
    status ENUM('pending', 'running', 'completed', 'failed', 'timeout') NOT NULL,
    stdout TEXT,
    stderr TEXT,
    exit_code INT,
    execution_time FLOAT,
    artifacts JSON,
    -- æ–°å¢å­—æ®µï¼šhandler è¿”å›å€¼å’Œæ€§èƒ½æŒ‡æ ‡
    return_value JSON,                  -- handler å‡½æ•°è¿”å›å€¼ï¼ˆJSON å¯åºåˆ—åŒ–ï¼‰
    metrics JSON,                       -- æ€§èƒ½æŒ‡æ ‡ï¼ˆduration_ms, cpu_time_ms, peak_memory_mb ç­‰ï¼‰
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    completed_at TIMESTAMP NULL,
    FOREIGN KEY (session_id) REFERENCES sessions(id) ON DELETE CASCADE,
    INDEX idx_session (session_id),
    INDEX idx_status (status),
    INDEX idx_created (created_at)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

-- æ¨¡æ¿è¡¨
CREATE TABLE templates (
    id VARCHAR(64) PRIMARY KEY,
    name VARCHAR(128) NOT NULL UNIQUE,
    image VARCHAR(256) NOT NULL,
    base_image VARCHAR(256),
    pre_installed_packages JSON,
    default_resources_cpu VARCHAR(16),
    default_resources_memory VARCHAR(16),
    default_resources_disk VARCHAR(16),
    security_context JSON,
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    INDEX idx_name (name)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
```

ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼š
```python
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy import select, update

# SQLAlchemy æ¨¡å‹
from sqlalchemy import Column, String, Enum, DateTime, Integer, Text, JSON, ForeignKey
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class SessionDB(Base):
    __tablename__ = "sessions"

    id = Column(String(64), primary_key=True)
    template_id = Column(String(64), nullable=False)
    status = Column(Enum("creating", "running", "completed", "failed", "timeout", "terminated"), nullable=False)
    runtime_type = Column(Enum("docker", "kubernetes"), nullable=False)
    runtime_node = Column(String(128))
    container_id = Column(String(128))
    pod_name = Column(String(128))
    workspace_path = Column(String(256))  # S3 è·¯å¾„ï¼šs3://bucket/sessions/{session_id}/
    resources_cpu = Column(String(16))
    resources_memory = Column(String(16))
    resources_disk = Column(String(16))
    env_vars = Column(JSON)
    timeout = Column(Integer, default=300)
    last_activity_at = Column(DateTime, nullable=False, default=datetime.now)  # æœ€åæ´»åŠ¨æ—¶é—´
    created_at = Column(DateTime, nullable=False, default=datetime.now)
    updated_at = Column(DateTime, nullable=False, default=datetime.now, onupdate=datetime.now)
    completed_at = Column(DateTime, nullable=True)

class SessionManager:
    def __init__(self, db_url: str = "mysql+aiomysql://sandbox:password@mariadb:3306/sandbox"):
        # åˆ›å»ºå¼‚æ­¥æ•°æ®åº“å¼•æ“
        self.engine = create_async_engine(
            db_url,
            pool_size=20,           # è¿æ¥æ± å¤§å°
            max_overflow=40,        # æœ€å¤§æº¢å‡ºè¿æ¥æ•°
            pool_recycle=3600,      # è¿æ¥å›æ”¶æ—¶é—´ï¼ˆç§’ï¼‰
            pool_pre_ping=True,     # è¿æ¥å‰æ£€æµ‹å¯ç”¨æ€§
            echo=False
        )
        self.async_session = sessionmaker(
            self.engine, class_=AsyncSession, expire_on_commit=False
        )

    async def create_session(self, request: CreateSessionRequest) -> Session:
        # 1. ç”Ÿæˆä¼šè¯ ID
        session_id = self._generate_session_id()

        # 2. è°ƒåº¦è¿è¡Œæ—¶
        runtime_node = await self.scheduler.schedule(request)

        # 3. ç”Ÿæˆ S3 workspace è·¯å¾„
        workspace_path = f"s3://{self.s3_bucket}/sessions/{session_id}/"

        # 4. åˆ›å»ºæ•°æ®åº“äº‹åŠ¡
        async with self.async_session() as db:
            # åˆ›å»ºä¼šè¯è®°å½•
            session_db = SessionDB(
                id=session_id,
                template_id=request.template_id,
                status=SessionStatus.CREATING,
                runtime_type=runtime_node.type,
                runtime_node=runtime_node.id,
                workspace_path=workspace_path,
                resources_cpu=request.resources.cpu,
                resources_memory=request.resources.memory,
                resources_disk=request.resources.disk,
                env_vars=request.env_vars,
                timeout=request.timeout
            )
            db.add(session_db)
            await db.commit()

            # 4. è°ƒç”¨è¿è¡Œæ—¶åˆ›å»ºå®¹å™¨
            try:
                container_id = await runtime_node.create_container(session_id)

                # æ›´æ–°å®¹å™¨ä¿¡æ¯
                session_db.container_id = container_id
                session_db.status = SessionStatus.RUNNING
                await db.commit()

            except Exception as e:
                # åˆ›å»ºå¤±è´¥ï¼Œå›æ»šä¼šè¯çŠ¶æ€
                session_db.status = SessionStatus.FAILED
                await db.commit()
                raise

        return self._db_to_pydantic(session_db)

    async def get_session(self, session_id: str) -> Optional[Session]:
        async with self.async_session() as db:
            result = await db.execute(
                select(SessionDB).where(SessionDB.id == session_id)
            )
            session_db = result.scalar_one_or_none()
            if session_db:
                return self._db_to_pydantic(session_db)
            return None

    async def update_session_status(self, session_id: str, status: SessionStatus):
        async with self.async_session() as db:
            await db.execute(
                update(SessionDB)
                .where(SessionDB.id == session_id)
                .values(status=status.value, updated_at=datetime.now())
            )
            await db.commit()

    async def terminate_session(self, session_id: str):
        async with self.async_session() as db:
            result = await db.execute(
                select(SessionDB).where(SessionDB.id == session_id)
            )
            session_db = result.scalar_one_or_none()

            if not session_db:
                raise ValueError(f"Session {session_id} not found")

            # è·å–å®¹å™¨èŠ‚ç‚¹ä¿¡æ¯
            runtime_node = await self.scheduler.get_node(session_db.runtime_node)

            # è°ƒç”¨è¿è¡Œæ—¶æ¸…ç†èµ„æº
            await runtime_node.destroy_container(session_id, session_db.container_id)

            # æ›´æ–°æ•°æ®åº“çŠ¶æ€
            session_db.status = SessionStatus.TERMINATED
            session_db.completed_at = datetime.now()
            await db.commit()

            # å›æ”¶åˆ° Warm Poolï¼ˆå¦‚æœå®¹å™¨ä»ç„¶å¥åº·ï¼‰
            if await self._is_container_healthy(session_db):
                await self.warm_pool.recycle(self._db_to_pydantic(session_db))

    async def execute_code(self, session_id: str, request: ExecuteRequest) -> str:
        """æ‰§è¡Œä»£ç ï¼Œè‡ªåŠ¨å¤„ç†å®¹å™¨é‡å»º"""
        session = await self.get_session(session_id)

        # æ£€æŸ¥å®¹å™¨æ˜¯å¦å­˜æ´»ï¼Œå¦‚æœå·²é”€æ¯åˆ™è‡ªåŠ¨é‡å»º
        if not await self._is_container_alive(session):
            logger.info(f"Container for session {session_id} not alive, recreating...")
            await self._recreate_container(session)

        # æ›´æ–°æœ€åæ´»åŠ¨æ—¶é—´
        await self._update_last_activity(session_id)

        # è°ƒç”¨è¿è¡Œæ—¶æ‰§è¡Œä»£ç 
        runtime_node = await self.scheduler.get_node(session.runtime_node)
        execution_id = await runtime_node.execute(session_id, request)

        return execution_id

    async def _is_container_alive(self, session: Session) -> bool:
        """æ£€æŸ¥å®¹å™¨æ˜¯å¦å­˜æ´»"""
        try:
            runtime_node = await self.scheduler.get_node(session.runtime_node)
            return await runtime_node.is_container_alive(session.container_id)
        except Exception:
            return False

    async def _is_container_healthy(self, session_db: SessionDB) -> bool:
        """æ£€æŸ¥å®¹å™¨æ˜¯å¦å¥åº·ï¼ˆç”¨äºå›æ”¶åˆ¤æ–­ï¼‰"""
        try:
            runtime_node = await self.scheduler.get_node(session_db.runtime_node)
            return await runtime_node.is_container_healthy(session_db.container_id)
        except Exception:
            return False

    async def _recreate_container(self, session: Session):
        """é‡å»ºå®¹å™¨ï¼ˆå…±äº«åŒä¸€ä¸ª S3 workspaceï¼‰"""
        # é‡æ–°è°ƒåº¦åˆ°æœ€ä¼˜èŠ‚ç‚¹
        runtime_node = await self.scheduler.schedule(
            CreateSessionRequest(
                template_id=session.template_id,
                resources=session.resources,
                env_vars=session.env_vars,
                timeout=session.timeout
            )
        )

        # åˆ›å»ºæ–°å®¹å™¨ï¼ŒæŒ‚è½½åŒä¸€ä¸ª S3 workspace
        container_id = await runtime_node.create_container(
            session_id=session.id,
            workspace_path=session.workspace_path
        )

        # æ›´æ–°æ•°æ®åº“ä¸­çš„å®¹å™¨ä¿¡æ¯
        async with self.async_session() as db:
            await db.execute(
                update(SessionDB)
                .where(SessionDB.id == session.id)
                .values(
                    runtime_node=runtime_node.id,
                    container_id=container_id,
                    status=SessionStatus.RUNNING,
                    updated_at=datetime.now()
                )
            )
            await db.commit()

    async def _update_last_activity(self, session_id: str):
        """æ›´æ–°æœ€åæ´»åŠ¨æ—¶é—´ï¼ˆç”¨äºè‡ªåŠ¨æ¸…ç†ï¼‰"""
        async with self.async_session() as db:
            await db.execute(
                update(SessionDB)
                .where(SessionDB.id == session_id)
                .values(last_activity_at=datetime.now())
            )
            await db.commit()

    def _db_to_pydantic(self, session_db: SessionDB) -> Session:
        """å°† SQLAlchemy æ¨¡å‹è½¬æ¢ä¸º Pydantic æ¨¡å‹"""
        return Session(
            id=session_db.id,
            template_id=session_db.template_id,
            status=SessionStatus(session_db.status),
            runtime_type=session_db.runtime_type,
            runtime_node=session_db.runtime_node,
            container_id=session_db.container_id,
            pod_name=session_db.pod_name,
            workspace_path=session_db.workspace_path,
            resources=ResourceLimit(
                cpu=session_db.resources_cpu,
                memory=session_db.resources_memory,
                disk=session_db.resources_disk
            ),
            env_vars=session_db.env_vars or {},
            created_at=session_db.created_at,
            updated_at=session_db.updated_at,
            timeout=session_db.timeout
        )

    async def cleanup_idle_sessions(self):
        """å®šæœŸæ¸…ç†ç©ºé—²ä¼šè¯ï¼ˆåå°ä»»åŠ¡ï¼‰

        æ¸…ç†ç­–ç•¥ï¼š
        - ç©ºé—²è¶…è¿‡ 30 åˆ†é’Ÿè‡ªåŠ¨é”€æ¯å®¹å™¨
        - åˆ›å»ºè¶…è¿‡ 6 å°æ—¶å¼ºåˆ¶é”€æ¯
        """
        async with self.async_session() as db:
            # ç©ºé—²è¶…æ—¶æ¸…ç†
            idle_threshold = datetime.now() - timedelta(minutes=30)
            idle_sessions = await db.execute(
                select(SessionDB)
                .where(SessionDB.status == SessionStatus.RUNNING)
                .where(SessionDB.last_activity_at < idle_threshold)
            )
            idle_sessions = idle_sessions.scalars().all()

            for session_db in idle_sessions:
                logger.info(f"Cleaning up idle session {session_db.id}")
                await self._destroy_session_container(session_db.id, session_db)

            # æœ€å¤§ç”Ÿå‘½å‘¨æœŸå¼ºåˆ¶æ¸…ç†
            max_lifetime_threshold = datetime.now() - timedelta(hours=6)
            old_sessions = await db.execute(
                select(SessionDB)
                .where(SessionDB.status == SessionStatus.RUNNING)
                .where(SessionDB.created_at < max_lifetime_threshold)
            )
            old_sessions = old_sessions.scalars().all()

            for session_db in old_sessions:
                logger.info(f"Cleaning up old session {session_db.id}")
                await self._destroy_session_container(session_db.id, session_db)

    async def _destroy_session_container(self, session_id: str, session_db: SessionDB):
        """é”€æ¯ä¼šè¯å®¹å™¨"""
        try:
            runtime_node = await self.scheduler.get_node(session_db.runtime_node)
            await runtime_node.destroy_container(session_id, session_db.container_id)
        except Exception as e:
            logger.warning(f"Failed to destroy container for session {session_id}: {e}")

        # æ›´æ–°æ•°æ®åº“çŠ¶æ€
        async with self.async_session() as db:
            await db.execute(
                update(SessionDB)
                .where(SessionDB.id == session_id)
                .values(
                    status=SessionStatus.TERMINATED,
                    completed_at=datetime.now(),
                    container_id=None,  # æ¸…ç©ºå®¹å™¨ ID
                    runtime_node=None
                )
            )
            await db.commit()
```

#### 2.1.4 ç›‘æ§æ¢é’ˆ (Health Probe)
æ¢æµ‹æœºåˆ¶ï¼š

- å¿ƒè·³æ£€æµ‹ï¼šæ¯ 10 ç§’å‘è¿è¡Œæ—¶å‘é€ /health è¯·æ±‚
- è´Ÿè½½é‡‡é›†ï¼šæ¯ 30 ç§’æ”¶é›† CPUã€å†…å­˜ã€ä¼šè¯æ•°
- å¼‚å¸¸æ£€æµ‹ï¼šè¿ç»­ 3 æ¬¡å¿ƒè·³å¤±è´¥åˆ™æ ‡è®°ä¸ºä¸å¥åº·

è‡ªåŠ¨æ‘˜é™¤ï¼š

```python
class HealthProbe:
    async def probe_loop(self):
        while True:
            for node in self.runtime_nodes:
                try:
                    # å‘é€å¿ƒè·³
                    response = await asyncio.wait_for(
                        self.http_client.get(f"{node.url}/health"),
                        timeout=5.0
                    )
                    
                    # æ›´æ–°è´Ÿè½½ä¿¡æ¯
                    node.update_metrics(response.json())
                    node.mark_healthy()
                    
                except asyncio.TimeoutError:
                    node.increment_failure_count()
                    
                    # è¿ç»­å¤±è´¥åˆ™æ‘˜é™¤
                    if node.failure_count >= 3:
                        await self.remove_unhealthy_node(node)
            
            await asyncio.sleep(10)
```

#### 2.1.5 çŠ¶æ€åŒæ­¥æœåŠ¡ (State Sync Service)

**è®¾è®¡åŸåˆ™**ï¼šDocker/K8s æ˜¯å®¹å™¨çŠ¶æ€çš„å”¯ä¸€çœŸå®æ¥æºï¼ŒSession è¡¨åªä¿å­˜å…³è”å…³ç³»ã€‚

çŠ¶æ€åŒæ­¥æœåŠ¡è´Ÿè´£ï¼š
1. **å¯åŠ¨æ—¶å…¨é‡åŒæ­¥**ï¼šControl Plane é‡å¯åæ¢å¤çŠ¶æ€
2. **å®šæ—¶å¥åº·æ£€æŸ¥**ï¼šå®šæœŸæ£€æŸ¥å®¹å™¨çŠ¶æ€å¹¶ä¿®å¤ä¸ä¸€è‡´
3. **å®¹å™¨çŠ¶æ€æ¢å¤**ï¼šç»“åˆé¢„çƒ­æ± è‡ªåŠ¨æ¢å¤ä¸å¥åº·çš„å®¹å™¨

```python
class StateSyncService:
    """
    çŠ¶æ€åŒæ­¥æœåŠ¡

    èŒè´£ï¼š
    1. å¯åŠ¨æ—¶å…¨é‡çŠ¶æ€åŒæ­¥
    2. å®šæ—¶å¥åº·æ£€æŸ¥ï¼ˆæ¯ 30 ç§’ï¼‰
    3. å®¹å™¨çŠ¶æ€æ¢å¤
    """

    def __init__(
        self,
        session_repo: ISessionRepository,
        docker_scheduler: IDockerScheduler,
        warm_pool_manager: WarmPoolManager,
    ):
        self._session_repo = session_repo
        self._docker_scheduler = docker_scheduler
        self._warm_pool_manager = warm_pool_manager

    async def sync_on_startup(self) -> dict:
        """
        å¯åŠ¨æ—¶å…¨é‡åŒæ­¥

        ç­–ç•¥ï¼š
        1. æŸ¥è¯¢æ‰€æœ‰ RUNNING/CREATING çŠ¶æ€çš„ Session
        2. é€šè¿‡ Docker API æ£€æŸ¥æ¯ä¸ªå®¹å™¨æ˜¯å¦çœŸå®å­˜åœ¨ä¸”è¿è¡Œä¸­
        3. æ›´æ–° Session çŠ¶æ€ï¼š
           - å®¹å™¨å­˜åœ¨ä¸”è¿è¡Œ â†’ ä¿æŒ RUNNING
           - å®¹å™¨ä¸å­˜åœ¨/å·²åœæ­¢ â†’ å°è¯•æ¢å¤æˆ–æ ‡è®°ä¸º FAILED
        """
        active_sessions = await self._session_repo.find_by_status("running")
        active_sessions.extend(await self._session_repo.find_by_status("creating"))

        stats = {"healthy": 0, "unhealthy": 0, "recovered": 0, "failed": 0}

        for session in active_sessions:
            if not session.container_id:
                continue

            # ç›´æ¥é€šè¿‡ Docker API æ£€æŸ¥å®¹å™¨çŠ¶æ€
            is_running = await self._docker_scheduler.is_container_running(
                session.container_id
            )

            if is_running:
                stats["healthy"] += 1
            else:
                stats["unhealthy"] += 1
                # å°è¯•æ¢å¤
                recovered = await self._attempt_recovery(session)
                if recovered:
                    stats["recovered"] += 1
                else:
                    stats["failed"] += 1

        return stats

    async def periodic_health_check(self) -> dict:
        """
        å®šæ—¶å¥åº·æ£€æŸ¥ï¼ˆæ¯ 30 ç§’ï¼‰

        åªæ£€æŸ¥ RUNNING çŠ¶æ€çš„ Sessionï¼Œå‡å°‘æŸ¥è¯¢èŒƒå›´
        """
        running_sessions = await self._session_repo.find_by_status("running")

        for session in running_sessions:
            if not session.container_id:
                continue

            is_running = await self._docker_scheduler.is_container_running(
                session.container_id
            )

            if not is_running:
                await self._attempt_recovery(session)

        return {"checked": len(running_sessions)}

    async def _attempt_recovery(self, session: Session) -> bool:
        """
        å°è¯•æ¢å¤ Session

        ç­–ç•¥ï¼š
        1. é¦–å…ˆå°è¯•ä»é¢„çƒ­æ± è·å–å®ä¾‹
        2. å¦‚æœé¢„çƒ­æ± ä¸ºç©ºï¼Œåˆ›å»ºæ–°å®¹å™¨
        3. å¦‚æœåˆ›å»ºå¤±è´¥ï¼Œæ ‡è®° Session ä¸º FAILED
        """
        # 1. å°è¯•ä»é¢„çƒ­æ± è·å–
        warm_entry = await self._warm_pool_manager.acquire(
            session.template_id, session.id
        )
        if warm_entry:
            # åˆ†é…é¢„çƒ­å®ä¾‹
            session.container_id = warm_entry.container_id
            session.runtime_node = warm_entry.node_id
            await self._session_repo.save(session)
            return True

        # 2. åˆ›å»ºæ–°å®¹å™¨
        try:
            container_id = await self._docker_scheduler.create_container_for_session(
                session_id=session.id,
                template_id=session.template_id,
                workspace_path=session.workspace_path,
            )
            session.container_id = container_id
            await self._session_repo.save(session)
            return True
        except Exception as e:
            logger.error(f"Failed to recover session {session.id}: {e}")
            # 3. æ ‡è®°ä¸ºå¤±è´¥
            session.mark_as_failed()
            await self._session_repo.save(session)
            return False
```

**å¯åŠ¨æµç¨‹é›†æˆ**ï¼š

```python
# åœ¨ main.py çš„ lifespan å‡½æ•°ä¸­
@asynccontextmanager
async def lifespan(app: FastAPI):
    # å¯åŠ¨æ—¶
    logger.info("Starting Sandbox Control Plane")

    # åˆå§‹åŒ–ä¾èµ–æ³¨å…¥
    initialize_dependencies(app)

    # æ‰§è¡Œå¯åŠ¨æ—¶çŠ¶æ€åŒæ­¥
    state_sync_service = app.state.state_sync_service
    sync_stats = await state_sync_service.sync_on_startup()
    logger.info(f"Startup sync completed: {sync_stats}")

    # å¯åŠ¨åå°ä»»åŠ¡ç®¡ç†å™¨
    task_manager = BackgroundTaskManager()

    # æ³¨å†Œå®šæ—¶å¥åº·æ£€æŸ¥ä»»åŠ¡ï¼ˆæ¯ 30 ç§’ï¼‰
    task_manager.register_task(
        name="health_check",
        func=state_sync_service.periodic_health_check,
        interval_seconds=30,
        initial_delay_seconds=60,
    )

    await task_manager.start_all()

    yield

    # å…³é—­æ—¶
    await task_manager.stop_all()
```

### 2.2 Container Scheduler æ¨¡å—

è¿è¡Œæ—¶è´Ÿè´£ç®¡ç†æ²™ç®±å®¹å™¨çš„ç”Ÿå‘½å‘¨æœŸã€‚ç³»ç»Ÿé‡‡ç”¨å®¹å™¨éš”ç¦» + Bubblewrap è¿›ç¨‹éš”ç¦»çš„åŒå±‚å®‰å…¨æœºåˆ¶ã€‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å®¿ä¸»æœº (Host)                            â”‚
â”‚  â”œâ”€ Docker Engine / Kubernetes          â”‚
â”‚  â””â”€ è¿è¡Œæ—¶ç®¡ç†å™¨                         â”‚
â”‚     â”œâ”€ åˆ›å»ºå®¹å™¨                          â”‚
â”‚     â””â”€ ç›‘æ§å®¹å™¨                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“ åˆ›å»ºå®¹å™¨
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å®¹å™¨ (Container) - ç¬¬ä¸€å±‚éš”ç¦»            â”‚
â”‚  â”œâ”€ ç‹¬ç«‹æ–‡ä»¶ç³»ç»Ÿ (Union FS)              â”‚
â”‚  â”œâ”€ ç½‘ç»œéš”ç¦» (NetworkMode=none)          â”‚
â”‚  â”œâ”€ èµ„æºé™åˆ¶ (CPU/Memory/PID)            â”‚
â”‚  â”œâ”€ èƒ½åŠ›é™åˆ¶ (CAP_DROP=ALL)              â”‚
â”‚  â””â”€ éç‰¹æƒç”¨æˆ· (sandbox:sandbox)         â”‚
â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ æ‰§è¡Œå™¨è¿›ç¨‹ (Executor)               â”‚ â”‚
â”‚  â”‚  - ç›‘å¬ç®¡ç†ä¸­å¿ƒçš„æ‰§è¡Œè¯·æ±‚           â”‚ â”‚
â”‚  â”‚  - æ¥æ”¶ç”¨æˆ·ä»£ç                      â”‚ â”‚
â”‚  â”‚  - è°ƒç”¨ bwrap å¯åŠ¨ç”¨æˆ·ä»£ç           â”‚ â”‚
â”‚  â”‚  - æ”¶é›†æ‰§è¡Œç»“æœ                     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â†“ è°ƒç”¨ bwrap                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Bubblewrap æ²™ç®± - ç¬¬äºŒå±‚éš”ç¦»       â”‚ â”‚
â”‚  â”‚  â”œâ”€ æ–°çš„å‘½åç©ºé—´ (PID/NET/MNT...)  â”‚ â”‚
â”‚  â”‚  â”œâ”€ åªè¯»æ–‡ä»¶ç³»ç»Ÿ                    â”‚ â”‚
â”‚  â”‚  â”œâ”€ ä¸´æ—¶å·¥ä½œç›®å½• (tmpfs)            â”‚ â”‚
â”‚  â”‚  â”œâ”€ /proc, /dev æœ€å°åŒ–æŒ‚è½½          â”‚ â”‚
â”‚  â”‚  â””â”€ seccomp ç³»ç»Ÿè°ƒç”¨è¿‡æ»¤            â”‚ â”‚
â”‚  â”‚                                     â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚ ç”¨æˆ·ä»£ç è¿›ç¨‹                  â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  - Python/Node.js/Shell      â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  - å— bwrap å®Œå…¨é™åˆ¶          â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
#### 2.2.1 Docker è¿è¡Œæ—¶

å®¹å™¨é…ç½®ï¼ˆç¬¬ä¸€å±‚éš”ç¦»ï¼‰ï¼š

```python
class DockerRuntime:
    def __init__(self):
        self.docker_client = aiodocker.Docker()
    
    async def create_container(self, session: Session) -> str:
        template = await self.get_template(session.template_id)
        
        # å®¹å™¨é…ç½® - ç¬¬ä¸€å±‚éš”ç¦»
        config = {
            "Image": template.image,
            # å®¹å™¨å¯åŠ¨åè¿è¡Œæ‰§è¡Œå™¨
            "Cmd": ["/usr/local/bin/sandbox-executor"],
            "Env": self._build_env_vars(session),
            "WorkingDir": "/workspace",
            
            # ä¸»æœºé…ç½® - å®¹å™¨å±‚éš”ç¦»
            "HostConfig": {
                # èµ„æºé™åˆ¶
                "Memory": session.resources.memory_bytes,
                "MemorySwap": session.resources.memory_bytes,  # ç¦ç”¨ swap
                "CpuQuota": session.resources.cpu_quota,
                "CpuPeriod": 100000,
                "PidsLimit": 128,  # é™åˆ¶æœ€å¤§è¿›ç¨‹æ•°

                # ç½‘ç»œéš”ç¦»
                "NetworkMode": "none",  # é»˜è®¤å®Œå…¨éš”ç¦»ç½‘ç»œ

                # å®‰å…¨é…ç½®
                "CapDrop": ["ALL"],  # åˆ é™¤æ‰€æœ‰ Linux Capabilities
                "SecurityOpt": [
                    "no-new-privileges",  # ç¦æ­¢è¿›ç¨‹è·å–æ–°æƒé™
                    "seccomp=default.json"  # Seccomp é…ç½®
                ],

                # Volume æŒ‚è½½
                "Binds": [
                    f"{session.s3_volume_path}:/workspace",  # S3 å¯¹è±¡å­˜å‚¨é€šè¿‡ FUSE/s3fs æŒ‚è½½
                ],

                # æ–‡ä»¶ç³»ç»Ÿ
                "ReadonlyRootfs": False,  # æ ¹ç›®å½•å¯å†™ï¼ˆæ‰§è¡Œå™¨éœ€è¦ï¼‰
                "Tmpfs": {
                    "/tmp": "rw,noexec,nosuid,size=512m",  # ä¸´æ—¶ç›®å½•
                },

                # æ—¥å¿—é…ç½®
                "LogConfig": {
                    "Type": "json-file",
                    "Config": {
                        "max-size": "10m",
                        "max-file": "3"
                    }
                }
            },
            
            # ç”¨æˆ·é…ç½®
            "User": "sandbox:sandbox",  # éç‰¹æƒç”¨æˆ· (UID:GID = 1000:1000)
        }
        
        container = await self.docker_client.containers.create(config)
        await container.start()
        
        # ç­‰å¾…æ‰§è¡Œå™¨å°±ç»ª
        await self._wait_for_executor_ready(container.id)
        
        return container.id
    
    async def execute(self, session_id: str, request: ExecuteRequest) -> str:
        """å‘å®¹å™¨å†…çš„æ‰§è¡Œå™¨å‘é€æ‰§è¡Œè¯·æ±‚"""
        container = await self.get_container(session_id)
        
        # é€šè¿‡å®¹å™¨å†…çš„ HTTP API ä¸æ‰§è¡Œå™¨é€šä¿¡
        executor_url = f"http://container-{session_id}:8080"
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{executor_url}/execute",
                json={
                    "code": request.code,
                    "language": request.language,
                    "timeout": request.timeout,
                    "stdin": request.stdin
                }
            )
        
        return response.json()["execution_id"]
    
    def _build_env_vars(self, session: Session) -> List[str]:
        """æ„å»ºå®¹å™¨ç¯å¢ƒå˜é‡"""
        env_vars = [
            f"SESSION_ID={session.id}",
            f"CONTROL_PLANE_URL={self.control_plane_url}",
            f"EXECUTION_TIMEOUT={session.timeout}",
        ]
        
        # ç”¨æˆ·è‡ªå®šä¹‰ç¯å¢ƒå˜é‡
        for key, value in session.env_vars.items():
            env_vars.append(f"{key}={value}")
        
        return env_vars
    
    async def _wait_for_executor_ready(self, container_id: str, timeout: int = 10):
        """ç­‰å¾…å®¹å™¨å†…æ‰§è¡Œå™¨å¯åŠ¨å®Œæˆ"""
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            try:
                # æ£€æŸ¥æ‰§è¡Œå™¨æ˜¯å¦å“åº”
                exec_result = await self.docker_client.containers.get(container_id).exec_run(
                    cmd=["curl", "-f", "http://localhost:8080/health"],
                    stdout=True
                )
                
                if exec_result.exit_code == 0:
                    return
            except Exception:
                pass
            
            await asyncio.sleep(0.5)
        
        raise TimeoutError(f"Executor not ready in container {container_id}")

    # ========== çŠ¶æ€æŸ¥è¯¢æ¥å£ï¼ˆçŠ¶æ€åŒæ­¥æœåŠ¡ä¸“ç”¨ï¼‰ ==========

    async def is_container_running(self, container_id: str) -> bool:
        """
        æ£€æŸ¥å®¹å™¨æ˜¯å¦æ­£åœ¨è¿è¡Œ

        ç›´æ¥é€šè¿‡ Docker API æŸ¥è¯¢ï¼Œä¸ä¾èµ–æ•°æ®åº“ã€‚
        æ­¤æ–¹æ³•ä¾› StateSyncService ä½¿ç”¨ã€‚

        Args:
            container_id: å®¹å™¨ ID

        Returns:
            bool: å®¹å™¨æ˜¯å¦è¿è¡Œä¸­
        """
        try:
            container = await self.docker_client.containers.get(container_id)
            info = await container.show()
            return info["State"]["Status"] == "running"
        except Exception:
            return False

    async def get_container_status(self, container_id: str) -> ContainerInfo:
        """
        è·å–å®¹å™¨è¯¦ç»†çŠ¶æ€

        ç›´æ¥é€šè¿‡ Docker API æŸ¥è¯¢ï¼Œä¸ä¾èµ–æ•°æ®åº“ã€‚
        æ­¤æ–¹æ³•ä¾› StateSyncService ä½¿ç”¨ã€‚

        Args:
            container_id: å®¹å™¨ ID

        Returns:
            ContainerInfo: åŒ…å« status, health, created_at ç­‰ä¿¡æ¯
        """
        container = await self.docker_client.containers.get(container_id)
        info = await container.show()

        return ContainerInfo(
            id=container_id,
            status=info["State"]["Status"],
            created_at=info["Created"],
            health=info["State"].get("Health", {}).get("Status", "unknown"),
            image=info["Config"]["Image"],
            labels=info["Config"].get("Labels", {}),
        )
```

å®¹å™¨é•œåƒæ„å»ºï¼š

```
# Dockerfile - æ²™ç®±æ‰§è¡Œç¯å¢ƒé•œåƒ
FROM python:3.11-slim

# å®‰è£…å¿…è¦å·¥å…·
RUN apt-get update && apt-get install -y \
    bubblewrap \
    curl \
    && rm -rf /var/lib/apt/lists/*

# åˆ›å»ºéç‰¹æƒç”¨æˆ·
RUN groupadd -g 1000 sandbox && \
    useradd -m -u 1000 -g sandbox sandbox

# å®‰è£…æ‰§è¡Œå™¨
COPY sandbox-executor /usr/local/bin/sandbox-executor
RUN chmod +x /usr/local/bin/sandbox-executor

# åˆ›å»ºå·¥ä½œç›®å½•
RUN mkdir -p /workspace && chown sandbox:sandbox /workspace

# åˆ‡æ¢åˆ°éç‰¹æƒç”¨æˆ·
USER sandbox

WORKDIR /workspace

# å¯åŠ¨æ‰§è¡Œå™¨ï¼ˆç›‘å¬ 8080 ç«¯å£ï¼‰
CMD ["/usr/local/bin/sandbox-executor"]
```

#### 2.2.2 Kubernetes è¿è¡Œæ—¶
Pod é…ç½®ï¼ˆç¬¬ä¸€å±‚éš”ç¦»ï¼‰ï¼š
```python
class K8sRuntime:
    def _build_pod_spec(self, sandbox: Sandbox) -> V1Pod:
        template = self.get_template(sandbox.spec.templateRef)
        
        return V1Pod(
            metadata=V1ObjectMeta(
                name=f"sandbox-{sandbox.name}",
                labels={
                    "app": "sandbox",
                    "session": sandbox.name,
                    "template": sandbox.spec.templateRef
                }
            ),
            spec=V1PodSpec(
                # å®¹å™¨é…ç½®
                containers=[V1Container(
                    name="executor",
                    image=template.image,
                    command=["/usr/local/bin/sandbox-executor"],
                    
                    # ç¯å¢ƒå˜é‡
                    env=[
                        V1EnvVar(name="SESSION_ID", value=sandbox.name),
                        V1EnvVar(name="CONTROL_PLANE_URL", value=self.control_plane_url)
                    ],
                    
                    # èµ„æºé™åˆ¶
                    resources=V1ResourceRequirements(
                        limits={
                            "cpu": sandbox.spec.resources.cpu,
                            "memory": sandbox.spec.resources.memory,
                            "ephemeral-storage": "1Gi"
                        },
                        requests={
                            "cpu": sandbox.spec.resources.cpu,
                            "memory": sandbox.spec.resources.memory
                        }
                    ),
                    
                    # å®‰å…¨ä¸Šä¸‹æ–‡ - å®¹å™¨å±‚éš”ç¦»
                    security_context=V1SecurityContext(
                        # éç‰¹æƒæ¨¡å¼
                        privileged=False,
                        # é root ç”¨æˆ·
                        run_as_non_root=True,
                        run_as_user=1000,
                        run_as_group=1000,
                        # åªè¯»æ ¹æ–‡ä»¶ç³»ç»Ÿï¼ˆæ‰§è¡Œå™¨ç›®å½•é™¤å¤–ï¼‰
                        read_only_root_filesystem=False,
                        # ç¦æ­¢æƒé™æå‡
                        allow_privilege_escalation=False,
                        # åˆ é™¤æ‰€æœ‰ Capabilities
                        capabilities=V1Capabilities(
                            drop=["ALL"]
                        ),
                        # Seccomp é…ç½®
                        seccomp_profile=V1SeccompProfile(
                            type="RuntimeDefault"
                        )
                    ),
                    
                    # å·æŒ‚è½½
                    volume_mounts=[
                        V1VolumeMount(
                            name="workspace",
                            mount_path="/workspace"
                        )
                    ]
                )],
                
                # Pod å®‰å…¨é…ç½®
                security_context=V1PodSecurityContext(
                    fs_group=1000,
                    run_as_non_root=True,
                    run_as_user=1000,
                    # Sysctl é™åˆ¶
                    sysctls=[
                        V1Sysctl(name="net.ipv4.ping_group_range", value="1000 1000")
                    ]
                ),
                
                # å·å®šä¹‰
                volumes=[
                    V1Volume(
                        name="workspace",
                        # ä½¿ç”¨ PVC æŒ‚è½½ S3 å¯¹è±¡å­˜å‚¨ï¼ˆé€šè¿‡ CSI Driverï¼‰
                        persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(
                            claim_name=f"sandbox-{sandbox.name}-workspace"
                        )
                    )
                ],
                
                # é‡å¯ç­–ç•¥
                restart_policy="Never",
                
                # DNS ç­–ç•¥
                dns_policy="None",  # ç¦ç”¨ DNS
                
                # ä¸»æœºç½‘ç»œé…ç½®
                host_network=False,
                host_pid=False,
                host_ipc=False
            )
        )
```

**S3 CSI Driver é…ç½®è¯´æ˜**ï¼š

Kubernetes ç¯å¢ƒä¸‹ä½¿ç”¨ S3 CSI Driver å°†å¯¹è±¡å­˜å‚¨æŒ‚è½½ä¸º Pod Volumeï¼š

```yaml
# StorageClass å®šä¹‰
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: s3-storage
provisioner: s3.csi.aws.com  # æˆ–å…¶ä»– S3 CSI Driver
parameters:
  mounter: geesefs  # æˆ– goofysã€s3fs ç­‰æŒ‚è½½å·¥å…·
  region: us-east-1
  bucket: sandbox-workspace

# PVC æ¨¡æ¿ï¼ˆç”±æ²™ç®±ç³»ç»ŸåŠ¨æ€åˆ›å»ºï¼‰
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: sandbox-{session-id}-workspace
  namespace: sandbox-system
spec:
  accessModes: [ "ReadWriteOnce" ]
  storageClassName: s3-storage
  resources:
    requests:
      storage: 5Gi  # S3 æ— å®é™…é™åˆ¶ï¼Œä»…ä¸ºæ»¡è¶³ K8s è¦æ±‚
```

**æ³¨æ„äº‹é¡¹**ï¼š
- S3 æŒ‚è½½æ€§èƒ½è¾ƒæœ¬åœ°ç£ç›˜æ…¢ï¼Œé€‚åˆæ–‡ä»¶è¯»å†™ä¸é¢‘ç¹çš„åœºæ™¯
- å¯¹äºé«˜é¢‘è¯»å†™åœºæ™¯ï¼Œå¯è€ƒè™‘ä½¿ç”¨æœ¬åœ°å­˜å‚¨ + å¼‚æ­¥ä¸Šä¼ åˆ° S3 çš„æ–¹æ¡ˆ
- éœ€è¦é…ç½® IAM Role æˆ– Secret æä¾› S3 è®¿é—®å‡­è¯

### 2.3 æ‰§è¡Œå™¨ (Executor)

æ‰§è¡Œå™¨æ˜¯è¿è¡Œåœ¨å®¹å™¨å†…çš„å®ˆæŠ¤è¿›ç¨‹ï¼Œè´Ÿè´£æ¥æ”¶æ‰§è¡Œè¯·æ±‚å¹¶é€šè¿‡ Bubblewrap å¯åŠ¨ç”¨æˆ·ä»£ç ï¼Œå®ç°ç¬¬äºŒå±‚éš”ç¦»ã€‚

#### 2.3.1 æ‰§è¡Œå™¨æ¶æ„
æ‰§è¡Œå™¨èŒè´£ï¼š

- åœ¨å®¹å™¨å¯åŠ¨æ—¶ä½œä¸ºä¸»è¿›ç¨‹è¿è¡Œ
- ç›‘å¬ HTTP è¯·æ±‚ï¼ˆæ¥è‡ªç®¡ç†ä¸­å¿ƒï¼‰
- æ¥æ”¶ç”¨æˆ·ä»£ç å’Œæ‰§è¡Œå‚æ•°
- è°ƒç”¨ bwrap å‘½ä»¤éš”ç¦»æ‰§è¡Œç”¨æˆ·ä»£ç 
- æ”¶é›†æ‰§è¡Œç»“æœï¼ˆstdoutã€stderrã€è¿”å›å€¼ã€æ€§èƒ½æŒ‡æ ‡ï¼‰
- ä¸ŠæŠ¥ç»“æœåˆ°ç®¡ç†ä¸­å¿ƒ

**æ‰§è¡Œæ¨¡å¼**: AWS Lambda-style Handler

æ‰€æœ‰ Python ç”¨æˆ·ä»£ç å¿…é¡»å®šä¹‰ä»¥ä¸‹å…¥å£å‡½æ•°ï¼š

```python
def handler(event: dict) -> dict:
    """
    AWS Lambda-style handler å‡½æ•°

    Args:
        event: ä¸šåŠ¡è¾“å…¥æ•°æ® (JSON å¯åºåˆ—åŒ–ç±»å‹)

    Returns:
        è¿”å›å€¼å¿…é¡»æ”¯æŒ JSON åºåˆ—åŒ–

    Raises:
        Exception: ä¸šåŠ¡é€»è¾‘å¼‚å¸¸ä¼šè¢«æ•è·å¹¶è®°å½•åˆ° stderr
    """
    # ç”¨æˆ·ä¸šåŠ¡é€»è¾‘
    result = process(event)
    return {"status": "ok", "data": result}
```

**Fileless Execution**: æ‰§è¡Œå™¨ä½¿ç”¨ `python3 -c` ç›´æ¥åœ¨å†…å­˜ä¸­æ‰§è¡Œä»£ç ï¼Œé¿å…æ–‡ä»¶ I/O æ“ä½œã€‚

æ ¸å¿ƒå®ç°ï¼š

```python
# sandbox-executor.py
# è¿è¡Œåœ¨å®¹å™¨å†…çš„æ‰§è¡Œå™¨è¿›ç¨‹

import asyncio
import json
import subprocess
import time
from pathlib import Path
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import httpx

app = FastAPI()

class ExecuteRequest(BaseModel):
    code: str
    language: str
    timeout: int = 30
    stdin: str = ""
    execution_id: str  # æ‰§è¡Œ IDï¼Œç”¨äºä¸ŠæŠ¥ç»“æœ

class ExecutionResult(BaseModel):
    status: str
    stdout: str
    stderr: str
    exit_code: int
    execution_time: float
    artifacts: list[str] = []
    return_value: dict | None = None  # æ–°å¢ï¼šhandler è¿”å›å€¼
    metrics: dict | None = None  # æ–°å¢ï¼šæ€§èƒ½æŒ‡æ ‡

class SandboxExecutor:
    def __init__(self):
        self.workspace = Path("/workspace")
        self.workspace.mkdir(exist_ok=True)

        self.session_id = os.environ.get("SESSION_ID")
        self.control_plane_url = os.environ.get("CONTROL_PLANE_URL")
        self.internal_api_token = os.environ.get("INTERNAL_API_TOKEN")

        # Bubblewrap é…ç½®
        self.bwrap_base_args = [
            "bwrap",
            # åªè¯»æŒ‚è½½ç³»ç»Ÿç›®å½•
            "--ro-bind", "/usr", "/usr",
            "--ro-bind", "/lib", "/lib",
            "--ro-bind", "/lib64", "/lib64",
            "--ro-bind", "/bin", "/bin",
            "--ro-bind", "/sbin", "/sbin",

            # å·¥ä½œç›®å½•ï¼ˆå¯å†™ï¼‰
            "--bind", str(self.workspace), "/workspace",
            "--chdir", "/workspace",

            # ä¸´æ—¶ç›®å½•
            "--tmpfs", "/tmp",

            # æœ€å°åŒ–çš„ /proc å’Œ /dev
            "--proc", "/proc",
            "--dev", "/dev",

            # éš”ç¦»æ‰€æœ‰å‘½åç©ºé—´
            "--unshare-all",
            "--unshare-net",

            # è¿›ç¨‹ç®¡ç†
            "--die-with-parent",
            "--new-session",

            # ç¯å¢ƒå˜é‡æ¸…ç†
            "--clearenv",
            "--setenv", "PATH", "/usr/local/bin:/usr/bin:/bin",
            "--setenv", "HOME", "/workspace",
            "--setenv", "TMPDIR", "/tmp",

            # å®‰å…¨é€‰é¡¹
            "--cap-drop", "ALL",
            "--no-new-privs",
        ]

    def _generate_wrapper_code(self, user_code: str) -> str:
        """ç”Ÿæˆ Lambda-style wrapper ä»£ç """
        return f"""
import json
import sys

# User code
{user_code}

# Read event from stdin
try:
    input_data = sys.stdin.read()
    event = json.loads(input_data) if input_data.strip() else {{}}
except json.JSONDecodeError as e:
    print(f"Error parsing event JSON: {{e}}", file=sys.stderr)
    sys.exit(1)

# Call handler
try:
    if 'handler' not in globals():
        raise ValueError("å¿…é¡»å®šä¹‰ handler(event) å‡½æ•°")

    result = handler(event)

    # Output result with markers
    print("\\n===SANDBOX_RESULT===")
    print(json.dumps(result))
    print("\\n===SANDBOX_RESULT_END===")

except Exception as e:
    import traceback
    print("\\n===SANDBOX_ERROR===")
    print(traceback.format_exc())
    print("\\n===SANDBOX_ERROR_END===")
    sys.exit(1)
"""

    async def execute_code(self, request: ExecuteRequest) -> ExecutionResult:
        """æ‰§è¡Œç”¨æˆ·ä»£ç ï¼ˆé€šè¿‡ bwrap éš”ç¦»ï¼‰"""
        execution_id = request.execution_id
        start_time = time.perf_counter()
        start_cpu = time.process_time()

        try:
            # 1. æ ¹æ®è¯­è¨€æ„å»ºæ‰§è¡Œå‘½ä»¤
            if request.language == "python":
                # Fileless execution: ä½¿ç”¨ python3 -c
                wrapper_code = self._generate_wrapper_code(request.code)

                exec_cmd = self.bwrap_base_args + [
                    "--",
                    "python3", "-c", wrapper_code
                ]

            elif request.language == "javascript":
                code_file = self.workspace / "user_code.js"
                code_file.write_text(request.code)

                exec_cmd = self.bwrap_base_args + [
                    "--ro-bind", str(code_file), "/workspace/user_code.js",
                    "--",
                    "node", "/workspace/user_code.js"
                ]

            elif request.language == "shell":
                exec_cmd = self.bwrap_base_args + [
                    "--",
                    "bash", "-c", request.code
                ]
            else:
                raise ValueError(f"Unsupported language: {request.language}")

            # 2. åœ¨ bwrap æ²™ç®±ä¸­æ‰§è¡Œä»£ç 
            result = subprocess.run(
                exec_cmd,
                input=request.stdin,
                capture_output=True,
                text=True,
                timeout=request.timeout,
                cwd=str(self.workspace)
            )

            duration_ms = (time.perf_counter() - start_time) * 1000
            cpu_time_ms = (time.process_time() - start_cpu) * 1000

            # 3. è§£æè¿”å›å€¼ï¼ˆPython handler æ¨¡å¼ï¼‰
            return_value = None
            if request.language == "python":
                return_value = self._parse_return_value(result.stdout)

            # 4. æ„å»ºæ€§èƒ½æŒ‡æ ‡
            metrics = {
                "duration_ms": round(duration_ms, 2),
                "cpu_time_ms": round(cpu_time_ms, 2),
            }

            # 5. æ”¶é›†æ‰§è¡Œç»“æœ
            execution_result = ExecutionResult(
                status="success" if result.returncode == 0 else "failed",
                stdout=result.stdout,
                stderr=result.stderr,
                exit_code=result.returncode,
                execution_time=duration_ms / 1000,  # è½¬æ¢ä¸ºç§’
                artifacts=self._collect_artifacts(),
                return_value=return_value,
                metrics=metrics,
            )

        except subprocess.TimeoutExpired:
            duration_ms = (time.perf_counter() - start_time) * 1000
            execution_result = ExecutionResult(
                status="timeout",
                stdout="",
                stderr=f"Execution timeout after {request.timeout} seconds",
                exit_code=-1,
                execution_time=duration_ms / 1000,
                artifacts=[],
                metrics={"duration_ms": round(duration_ms, 2)},
            )

        except Exception as e:
            duration_ms = (time.perf_counter() - start_time) * 1000
            execution_result = ExecutionResult(
                status="error",
                stdout="",
                stderr=str(e),
                exit_code=-1,
                execution_time=duration_ms / 1000,
                artifacts=[],
                metrics={"duration_ms": round(duration_ms, 2)},
            )

        # 6. ä¸ŠæŠ¥ç»“æœåˆ°ç®¡ç†ä¸­å¿ƒï¼ˆé€šè¿‡å†…éƒ¨ APIï¼‰
        await self._report_result(execution_id, execution_result)

        return execution_result

    def _parse_return_value(self, stdout: str) -> dict | None:
        """ä» stdout è§£æ handler è¿”å›å€¼"""
        try:
            if "===SANDBOX_RESULT===" in stdout:
                start = stdout.find("===SANDBOX_RESULT===") + len("===SANDBOX_RESULT===")
                end = stdout.find("===SANDBOX_RESULT_END===")
                if start > 0 and end > start:
                    json_str = stdout[start:end].strip()
                    return json.loads(json_str)
        except (json.JSONDecodeError, ValueError):
            pass
        return None

    def _collect_artifacts(self) -> list[str]:
        """æ”¶é›†ç”Ÿæˆçš„æ–‡ä»¶"""
        artifacts = []
        for file_path in self.workspace.rglob("*"):
            if file_path.is_file() and not file_path.name.startswith("."):
                artifacts.append(str(file_path.relative_to(self.workspace)))
        return artifacts

    async def _report_result(self, execution_id: str, result: ExecutionResult):
        """ä¸ŠæŠ¥æ‰§è¡Œç»“æœåˆ°ç®¡ç†ä¸­å¿ƒï¼ˆé€šè¿‡å†…éƒ¨ APIï¼‰"""
        try:
            headers = {}
            if self.internal_api_token:
                headers["Authorization"] = f"Bearer {self.internal_api_token}"

            async with httpx.AsyncClient() as client:
                await client.post(
                    f"{self.control_plane_url}/internal/executions/{execution_id}/result",
                    json=result.dict(),
                    headers=headers,
                    timeout=10.0
                )
        except Exception as e:
            logger.error(f"Failed to report result for execution {execution_id}: {e}")

# FastAPI ç«¯ç‚¹
executor = SandboxExecutor()

@app.get("/health")
async def health_check():
    return {"status": "healthy"}

@app.post("/execute")
async def execute(request: ExecuteRequest) -> ExecutionResult:
    """æ¥æ”¶æ‰§è¡Œè¯·æ±‚"""
    return await executor.execute_code(request)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080)
```

#### 2.3.2 æ‰§è¡Œç»“æœæ ¼å¼

æ‰§è¡Œç»“æœåŒ…å«ä»¥ä¸‹å­—æ®µï¼š

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `status` | str | æ‰§è¡ŒçŠ¶æ€ï¼šsuccess/failed/timeout/error |
| `stdout` | str | æ ‡å‡†è¾“å‡º |
| `stderr` | str | æ ‡å‡†é”™è¯¯ |
| `exit_code` | int | è¿›ç¨‹é€€å‡ºç  |
| `execution_time` | float | æ‰§è¡Œè€—æ—¶ï¼ˆç§’ï¼‰ |
| `artifacts` | list[str] | ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨ |
| `return_value` | dict | **æ–°å¢**ï¼šhandler å‡½æ•°è¿”å›å€¼ï¼ˆJSON å¯åºåˆ—åŒ–ï¼‰ |
| `metrics` | dict | **æ–°å¢**ï¼šæ€§èƒ½æŒ‡æ ‡ï¼ˆduration_msã€cpu_time_msã€peak_memory_mb ç­‰ï¼‰ |

**è¿”å›æ ¼å¼ç¤ºä¾‹**:

```json
{
  "status": "success",
  "stdout": "Processing complete.\\n",
  "stderr": "",
  "exit_code": 0,
  "execution_time": 0.07523,
  "artifacts": ["output.csv"],
  "return_value": {
    "status": "ok",
    "data": [1, 2, 3]
  },
  "metrics": {
    "duration_ms": 75.23,
    "cpu_time_ms": 68.12,
    "peak_memory_mb": 42.5
  }
}
```

**metrics å­—æ®µæ ¼å¼**:

```json
{
  "duration_ms": 75.23,     // å¢™é’Ÿè€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
  "cpu_time_ms": 68.12,     // CPU æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
  "peak_memory_mb": 42.5,    // å†…å­˜å³°å€¼ï¼ˆMBï¼‰ï¼Œå¯é€‰
  "io_read_bytes": 1024,     // è¯»å–å­—èŠ‚æ•°ï¼Œå¯é€‰
  "io_write_bytes": 2048     // å†™å…¥å­—èŠ‚æ•°ï¼Œå¯é€‰
}
```

ä¼˜åŠ¿ï¼š
- æ‰©å±•æ€§å¥½ï¼šæ·»åŠ æ–°æŒ‡æ ‡æ— éœ€ä¿®æ”¹è¡¨ç»“æ„
- çµæ´»æ€§é«˜ï¼šä¸åŒæ‰§è¡Œç±»å‹å¯åŒ…å«ä¸åŒæŒ‡æ ‡
- æŸ¥è¯¢æ–¹ä¾¿ï¼šMySQL 5.7+ æ”¯æŒ JSON å­—æ®µç´¢å¼•å’ŒæŸ¥è¯¢

#### 2.3.3 Bubblewrap å®‰å…¨é…ç½®è¯¦è§£
å®Œæ•´çš„ bwrap å‘½ä»¤ç¤ºä¾‹ï¼š

```bash
bwrap \
  # === æ–‡ä»¶ç³»ç»Ÿéš”ç¦» ===
  # åªè¯»æŒ‚è½½ç³»ç»Ÿç›®å½•
  --ro-bind /usr /usr \
  --ro-bind /lib /lib \
  --ro-bind /lib64 /lib64 \
  --ro-bind /bin /bin \
  --ro-bind /sbin /sbin \
  
  # å·¥ä½œç›®å½•ï¼ˆè¯»å†™ï¼‰
  --bind /workspace /workspace \
  --chdir /workspace \
  
  # ä¸´æ—¶ç›®å½•ï¼ˆå†…å­˜æ–‡ä»¶ç³»ç»Ÿï¼‰
  --tmpfs /tmp \
  
  # === å‘½åç©ºé—´éš”ç¦» ===
  --unshare-all \        # éš”ç¦»æ‰€æœ‰å‘½åç©ºé—´ï¼ˆPIDã€NETã€MNTã€IPCã€UTSã€USERï¼‰
  --share-net \          # å¯é€‰ï¼šå¦‚æœéœ€è¦ç½‘ç»œè®¿é—®
  
  # === è¿›ç¨‹éš”ç¦» ===
  --proc /proc \         # æŒ‚è½½ /procï¼ˆåªèƒ½çœ‹åˆ°æ²™ç®±å†…è¿›ç¨‹ï¼‰
  --dev /dev \           # æœ€å°åŒ–çš„ /dev
  --die-with-parent \    # çˆ¶è¿›ç¨‹ç»ˆæ­¢æ—¶è‡ªåŠ¨ç»ˆæ­¢
  --new-session \        # æ–°çš„ä¼šè¯
  
  # === ç¯å¢ƒéš”ç¦» ===
  --clearenv \           # æ¸…é™¤æ‰€æœ‰ç¯å¢ƒå˜é‡
  --setenv PATH /usr/local/bin:/usr/bin:/bin \
  --setenv HOME /workspace \
  --setenv TMPDIR /tmp \
  --unsetenv TERM \      # æ¸…é™¤ç»ˆç«¯ç¯å¢ƒ
  
  # === å®‰å…¨é™åˆ¶ ===
  --cap-drop ALL \       # åˆ é™¤æ‰€æœ‰ Linux Capabilities
  --no-new-privs \       # ç¦æ­¢è·å–æ–°æƒé™
  
  # === èµ„æºé™åˆ¶ï¼ˆå¯é€‰ï¼Œä¸ ulimit é…åˆï¼‰===
  --rlimit NPROC=128 \   # æœ€å¤§è¿›ç¨‹æ•°
  --rlimit NOFILE=1024 \ # æœ€å¤§æ–‡ä»¶æè¿°ç¬¦
  
  # === æ‰§è¡Œå‘½ä»¤ ===
  -- \
  python3 /workspace/user_code.py
```

å®‰å…¨ç‰¹æ€§è¯´æ˜ï¼š

| éš”ç¦»å±‚é¢ | å®¹å™¨éš”ç¦»         | Bubblewrapéš”ç¦»               |
| -------- | ---------------- | ---------------------------- |
| æ–‡ä»¶ç³»ç»Ÿ | Union FS, ç‹¬ç«‹å±‚ | åªè¯»ç»‘å®š, tmpfs              |
| ç½‘ç»œ     | NetworkMode=none | unshare network namespace    |
| è¿›ç¨‹     | PID namespace    | æ–° PID namespaceï¼ˆPID 1ï¼‰|
| IPC      | IPC namespace    | æ–° IPC namespace             |
| ç”¨æˆ·     | éç‰¹æƒç”¨æˆ·       | è¿›ä¸€æ­¥é™åˆ¶ capabilities      |
| ç³»ç»Ÿè°ƒç”¨ | Seccomp è¿‡æ»¤     | é¢å¤–çš„ seccomp è¿‡æ»¤          |
| èµ„æº     | cgroup é™åˆ¶      | ulimit é™åˆ¶                  |


## 3. å…³é”®æµç¨‹è®¾è®¡
### 3.1 ä¼šè¯åˆ›å»ºæµç¨‹
![alt text](image-2.png)
```mermaid
sequenceDiagram
    participant Agent as AI Agent
    participant API as API Gateway
    participant SessionMgr as ä¼šè¯ç®¡ç†å™¨
    participant Scheduler as è°ƒåº¦å™¨
    participant WarmPool as é¢„çƒ­æ± 
    participant Runtime as è¿è¡Œæ—¶
    participant Container as å®¹å™¨å®ä¾‹
    
    Agent->>API: POST /sessions (template_id, resources)
    API->>SessionMgr: create_session()
    SessionMgr->>Scheduler: schedule(request)
    
    Scheduler->>WarmPool: acquire(template_id)
    alt é¢„çƒ­å®ä¾‹å¯ç”¨
        WarmPool-->>Scheduler: è¿”å›é¢„çƒ­å®ä¾‹
        Scheduler-->>SessionMgr: è¿”å›å®¹å™¨èŠ‚ç‚¹
    else æ— é¢„çƒ­å®ä¾‹
        Scheduler->>Runtime: é€‰æ‹©æœ€ä¼˜èŠ‚ç‚¹
        Runtime-->>Scheduler: è¿”å›èŠ‚ç‚¹ä¿¡æ¯
    end
    
    SessionMgr->>Runtime: create_container(session)
    Runtime->>Container: docker run / kubectl create pod
    Container-->>Runtime: å®¹å™¨å¯åŠ¨æˆåŠŸ
    Runtime-->>SessionMgr: è¿”å›å®¹å™¨ ID

    SessionMgr->>MariaDB: INSERT INTO sessions ...
    MariaDB-->>SessionMgr: ç¡®è®¤ä¿å­˜
    SessionMgr-->>API: è¿”å› session_id
    API-->>Agent: è¿”å›ä¼šè¯ä¿¡æ¯
```

### 3.2 ä»£ç æ‰§è¡Œæµç¨‹
![alt text](image-1.png)
```mermaid
sequenceDiagram
    participant Agent as AI Agent
    participant API as API Gateway
    participant SessionMgr as ä¼šè¯ç®¡ç†å™¨
    participant Runtime as è¿è¡Œæ—¶
    participant Executor as æ‰§è¡Œå™¨
    participant DB as MariaDB
    participant Volume as å¯¹è±¡å­˜å‚¨<br/>(Volume æŒ‚è½½)

    Agent->>API: POST /api/v1/sessions/{session_id}/execute (code, language)
    API->>SessionMgr: get_session(id)
    SessionMgr->>DB: SELECT * FROM sessions WHERE id=?
    DB-->>SessionMgr: è¿”å›ä¼šè¯ä¿¡æ¯
    SessionMgr-->>API: è¿”å›ä¼šè¯ä¿¡æ¯

    API->>DB: INSERT INTO executions (ç”Ÿæˆ execution_id)
    DB-->>API: è¿”å› execution_id

    API->>Runtime: execute_code(session_id, execution_id, request)
    Runtime->>Executor: é€šè¿‡å®¹å™¨å†… HTTP API å‘é€æ‰§è¡Œè¯·æ±‚

    Note over Executor,Volume: workspace ç›®å½•å·²é€šè¿‡ Volume æŒ‚è½½å¯¹è±¡å­˜å‚¨

    par å¼‚æ­¥æ‰§è¡Œ
        Executor->>Executor: è¿è¡Œä»£ç  (bwrap + subprocess)
        Executor->>Executor: æ”¶é›† stdout/stderr
        Executor->>Volume: ç”Ÿæˆæ–‡ä»¶å†™å…¥ workspace<br/>(è‡ªåŠ¨æŒä¹…åŒ–åˆ°å¯¹è±¡å­˜å‚¨)
        Executor->>Executor: æ‰«æç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨
    end

    Executor->>API: POST /internal/executions/{execution_id}/result
    API->>DB: UPDATE executions (ä¿å­˜ stdout/stderr/çŠ¶æ€/æ–‡ä»¶åˆ—è¡¨)
    DB-->>API: ç¡®è®¤ä¿å­˜
    API-->>Executor: 200 OK

    API-->>Agent: è¿”å› execution_id (å·²æäº¤)

    Agent->>API: GET /api/v1/executions/{execution_id}/result
    API->>DB: SELECT * FROM executions WHERE id=?
    DB-->>API: è¿”å›ç»“æœæ•°æ® (stdout/stderr/artifacts)
    API-->>Agent: è¿”å›æ‰§è¡Œç»“æœ

    Note over Agent,Volume: å¦‚éœ€ä¸‹è½½æ–‡ä»¶ï¼Œé€šè¿‡æ–‡ä»¶ API ä»å¯¹è±¡å­˜å‚¨è·å–
```
### 3.3 å¥åº·æ£€æŸ¥ä¸æ•…éšœæ¢å¤æµç¨‹
![alt text](image-4.png)
```mermaid
sequenceDiagram
    participant Probe as ç›‘æ§æ¢é’ˆ
    participant Runtime as å®¹å™¨èŠ‚ç‚¹
    participant Scheduler as è°ƒåº¦å™¨
    participant SessionMgr as ä¼šè¯ç®¡ç†å™¨
    
    loop æ¯ 10 ç§’
        Probe->>Runtime: GET /health
        
        alt å¥åº·å“åº”
            Runtime-->>Probe: 200 OK + metrics
            Probe->>Probe: æ›´æ–°èŠ‚ç‚¹çŠ¶æ€
        else è¶…æ—¶æˆ–å¤±è´¥
            Runtime--xProbe: Timeout / Error
            Probe->>Probe: failure_count++
            
            alt failure_count >= 3
                Probe->>Scheduler: mark_unhealthy(node)
                Scheduler->>SessionMgr: migrate_sessions(node)
                
                loop å¯¹è¯¥èŠ‚ç‚¹çš„æ¯ä¸ªä¼šè¯
                    SessionMgr->>Scheduler: schedule(session)
                    Scheduler->>Runtime: åœ¨æ–°èŠ‚ç‚¹åˆ›å»ºä¼šè¯
                end
                
                Probe->>Scheduler: remove_node(node)
            end
        end
    end
```

## 4. æ•°æ®æ¨¡å‹è®¾è®¡


### 4.1 æ ¸å¿ƒå®ä½“æ¨¡å‹
```python
from enum import Enum
from pydantic import BaseModel
from datetime import datetime
from typing import Optional, Dict, List

class SessionStatus(str, Enum):
    CREATING = "creating"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    TIMEOUT = "timeout"
    TERMINATED = "terminated"

class ResourceLimit(BaseModel):
    cpu: str = "1"  # CPU æ ¸å¿ƒæ•°
    memory: str = "512Mi"  # å†…å­˜é™åˆ¶
    disk: str = "1Gi"  # ç£ç›˜é™åˆ¶
    max_processes: int = 128  # æœ€å¤§è¿›ç¨‹æ•°

class Template(BaseModel):
    id: str
    name: str
    image: str  # Docker é•œåƒ
    base_image: str  # åŸºç¡€é•œåƒï¼ˆç”¨äºä¸¤é˜¶æ®µåŠ è½½ï¼‰
    pre_installed_packages: List[str]
    default_resources: ResourceLimit
    security_context: Dict[str, any]
    created_at: datetime

class Session(BaseModel):
    id: str
    template_id: str
    status: SessionStatus
    runtime_type: str  # "docker" or "kubernetes"
    runtime_node: str  # èŠ‚ç‚¹ ID
    container_id: Optional[str]
    pod_name: Optional[str]
    resources: ResourceLimit
    env_vars: Dict[str, str]
    created_at: datetime
    updated_at: datetime
    timeout: int  # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

class Execution(BaseModel):
    id: str
    session_id: str
    code: str
    language: str
    status: str  # "pending", "running", "completed", "failed"
    stdout: str
    stderr: str
    exit_code: int
    execution_time: float  # æ‰§è¡Œè€—æ—¶ï¼ˆç§’ï¼‰
    artifacts: List[Artifact]  # ç”Ÿæˆçš„æ–‡ä»¶å…ƒæ•°æ®åˆ—è¡¨
    # æ–°å¢å­—æ®µï¼šhandler è¿”å›å€¼å’Œæ€§èƒ½æŒ‡æ ‡
    return_value: Optional[dict] = None  # handler å‡½æ•°è¿”å›å€¼ï¼ˆJSON å¯åºåˆ—åŒ–ï¼‰
    metrics: Optional[dict] = None  # æ€§èƒ½æŒ‡æ ‡ï¼ˆduration_ms, cpu_time_ms, peak_memory_mb ç­‰ï¼‰
    created_at: datetime
    completed_at: Optional[datetime]

class Artifact(BaseModel):
    """æ–‡ä»¶å…ƒæ•°æ®æ¨¡å‹"""
    path: str  # ç›¸å¯¹äº workspace çš„æ–‡ä»¶è·¯å¾„ï¼Œå¦‚ "results/output.csv"
    size: int  # æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    mime_type: str  # MIME ç±»å‹ï¼Œå¦‚ "text/csv"
    type: Literal["artifact", "log", "output"]  # æ–‡ä»¶ç±»å‹
    created_at: datetime  # åˆ›å»ºæ—¶é—´
    checksum: Optional[str] = None  # SHA256 æ ¡éªŒå’Œï¼ˆå¯é€‰ï¼‰
    download_url: Optional[str] = None  # ä¸‹è½½ URLï¼ˆé¢„ç­¾å S3 URLï¼‰

class RuntimeNode(BaseModel):
    id: str
    type: str  # "docker" or "kubernetes"
    url: str  # èŠ‚ç‚¹ API åœ°å€
    status: str  # "healthy", "unhealthy", "draining"
    cpu_usage: float  # 0.0 - 1.0
    mem_usage: float  # 0.0 - 1.0
    session_count: int
    max_sessions: int
    cached_templates: List[str]
    last_heartbeat: datetime
    failure_count: int
```

### 4.2 åè®®å®šä¹‰

#### 4.2.1 æ§åˆ¶å¹³é¢ APIï¼ˆå¤–éƒ¨ APIï¼‰

**è¯´æ˜**: ç”± AI Agent æˆ–ä¸Šå±‚æœåŠ¡è°ƒç”¨çš„å…¬å¼€ API æ¥å£ã€‚

```
# ä¼šè¯ç®¡ç†
POST   /api/v1/sessions                           # åˆ›å»ºä¼šè¯
GET    /api/v1/sessions/{id}                      # è·å–ä¼šè¯è¯¦æƒ…
GET    /api/v1/sessions                           # åˆ—å‡ºä¼šè¯
DELETE /api/v1/sessions/{id}                      # ç»ˆæ­¢ä¼šè¯

# ä»£ç æ‰§è¡Œ
POST   /api/v1/sessions/{session_id}/execute      # æäº¤æ‰§è¡Œä»»åŠ¡ï¼Œè¿”å› execution_id
GET    /api/v1/sessions/{session_id}/executions   # åˆ—å‡ºè¯¥ä¼šè¯çš„æ‰€æœ‰æ‰§è¡Œè®°å½•

# æ‰§è¡Œç»“æœæŸ¥è¯¢ï¼ˆåŸºäº execution_idï¼‰
GET    /api/v1/executions/{execution_id}          # è·å–æ‰§è¡Œè¯¦æƒ…ï¼ˆåŒ…å«ç»“æœï¼‰
GET    /api/v1/executions/{execution_id}/status   # è·å–æ‰§è¡ŒçŠ¶æ€ï¼ˆpending/running/completed/failedï¼‰
GET    /api/v1/executions/{execution_id}/result   # è·å–æ‰§è¡Œç»“æœï¼ˆstdout/stderr/exit_codeï¼‰

# æ–‡ä»¶æ“ä½œ
POST   /api/v1/sessions/{id}/files/upload         # ä¸Šä¼ æ–‡ä»¶åˆ°ä¼šè¯å·¥ä½œç›®å½•
GET    /api/v1/sessions/{id}/files/{name}         # ä¸‹è½½ä¼šè¯å·¥ä½œç›®å½•ä¸­çš„æ–‡ä»¶

# æ¨¡æ¿ç®¡ç†
POST   /api/v1/templates                          # åˆ›å»ºæ¨¡æ¿
GET    /api/v1/templates                          # åˆ—å‡ºæ¨¡æ¿
GET    /api/v1/templates/{id}                     # è·å–æ¨¡æ¿è¯¦æƒ…
PUT    /api/v1/templates/{id}                     # æ›´æ–°æ¨¡æ¿
DELETE /api/v1/templates/{id}                     # åˆ é™¤æ¨¡æ¿

# è¿è¡Œæ—¶ç®¡ç†
GET    /api/v1/runtimes                           # åˆ—å‡ºå®¹å™¨èŠ‚ç‚¹
GET    /api/v1/runtimes/{id}/health               # è·å–èŠ‚ç‚¹å¥åº·çŠ¶æ€
GET    /api/v1/runtimes/{id}/metrics              # è·å–èŠ‚ç‚¹æŒ‡æ ‡
```

**è¯·æ±‚/å“åº”ç¤ºä¾‹**ï¼š

```python
# æäº¤æ‰§è¡Œä»»åŠ¡
POST /api/v1/sessions/{session_id}/execute
Request:
{
    "code": "def handler(event):\n    return {'message': 'Hello', 'input': event.get('name', 'World')}",
    "language": "python",
    "timeout": 30,
    "event": {"name": "Alice"}
}

Response:
{
    "execution_id": "exec_1234567890",
    "status": "submitted",
    "submitted_at": "2025-01-04T10:30:00Z"
}

# æŸ¥è¯¢æ‰§è¡ŒçŠ¶æ€
GET /api/v1/executions/{execution_id}/status
Response:
{
    "execution_id": "exec_1234567890",
    "session_id": "sess_abc123",
    "status": "completed",
    "created_at": "2025-01-04T10:30:00Z",
    "completed_at": "2025-01-04T10:30:02Z"
}

# è·å–æ‰§è¡Œç»“æœ
GET /api/v1/executions/{execution_id}/result
Response:
{
    "execution_id": "exec_1234567890",
    "status": "success",
    "stdout": "Processing complete.\\n",
    "stderr": "",
    "exit_code": 0,
    "execution_time": 0.07523,
    "return_value": {
        "message": "Hello",
        "input": "Alice"
    },
    "metrics": {
        "duration_ms": 75.23,
        "cpu_time_ms": 68.12,
        "peak_memory_mb": 42.5
    },
    "artifacts": ["output.txt"]
}
```

#### 4.2.2 å†…éƒ¨å›è°ƒ APIï¼ˆç”± Executor è°ƒç”¨ï¼‰

**è¯´æ˜**: æ‰§è¡Œå™¨ï¼ˆè¿è¡Œåœ¨å®¹å™¨å†…çš„ sandbox-executorï¼‰è°ƒç”¨çš„å†…éƒ¨æ¥å£ï¼Œç”¨äºä¸ŠæŠ¥æ‰§è¡Œç»“æœã€‚

```
# æ‰§è¡Œç»“æœä¸ŠæŠ¥
POST   /internal/executions/{execution_id}/result    # ä¸ŠæŠ¥æ‰§è¡Œç»“æœï¼ˆå®Œæˆ/å¤±è´¥/è¶…æ—¶ï¼‰
POST   /internal/executions/{execution_id}/status    # ä¸ŠæŠ¥æ‰§è¡ŒçŠ¶æ€å˜æ›´ï¼ˆrunning/timeoutï¼‰

# è¯·æ±‚ä½“ç¤ºä¾‹ï¼š
POST /internal/executions/{execution_id}/result
{
    "status": "success",              # success | failed | timeout | error
    "stdout": "æ‰§è¡Œè¾“å‡ºå†…å®¹",
    "stderr": "é”™è¯¯è¾“å‡ºå†…å®¹",
    "exit_code": 0,
    "execution_time": 0.07523,
    "return_value": {                 # handler å‡½æ•°è¿”å›å€¼ï¼ˆJSON å¯åºåˆ—åŒ–ï¼‰
        "message": "Hello",
        "input": "Alice"
    },
    "metrics": {                      # æ€§èƒ½æŒ‡æ ‡
        "duration_ms": 75.23,
        "cpu_time_ms": 68.12,
        "peak_memory_mb": 42.5
    },
    "artifacts": ["generated_file.txt"]
}
```

**å®‰å…¨è¯´æ˜**: å†…éƒ¨ API åº”è¯¥ï¼š
- ä»…åœ¨å†…ç½‘/å®¹å™¨ç½‘ç»œä¸­å¯è®¿é—®
- ä½¿ç”¨è®¤è¯æœºåˆ¶ï¼ˆå¦‚ JWT token æˆ– API Keyï¼‰
- é™åˆ¶ä»…å®¹å™¨èŠ‚ç‚¹å¯è®¿é—®

**è¯´æ˜**: Container Scheduler ä½œä¸º Control Plane å†…éƒ¨æ¨¡å—ï¼Œé€šè¿‡ SDK ç›´æ¥è°ƒç”¨ Docker/K8s APIï¼Œæ— éœ€ç‹¬ç«‹çš„ HTTP APIã€‚

### 4.3 æ‰§è¡Œè¯­ä¹‰ä¸å¹‚ç­‰æ€§æ¨¡å‹

#### 4.3.1 execution_id ç”Ÿå‘½å‘¨æœŸ

æ¯ä¸ªä»£ç æ‰§è¡Œè¯·æ±‚éƒ½ä¼šè¢«åˆ†é…ä¸€ä¸ªå”¯ä¸€çš„ `execution_id`ï¼Œç”¨äºè¿½è¸ªæ•´ä¸ªæ‰§è¡Œè¿‡ç¨‹ã€‚

**execution_id ç”Ÿæˆè§„åˆ™**:
```python
execution_id = f"exec_{timestamp}_{uuid4()[:8]}"
# ç¤ºä¾‹: exec_20240115_abc12345
```

**ç”Ÿå‘½å‘¨æœŸçŠ¶æ€æœº**:
```mermaid
stateDiagram-v2
    [*] --> Pending: åˆ›å»º execution_id
    Pending --> Running: Executor å¼€å§‹æ‰§è¡Œ
    Running --> Completed: æ‰§è¡ŒæˆåŠŸ
    Running --> Failed: æ‰§è¡Œå¤±è´¥ï¼ˆè¯­æ³•é”™è¯¯/è¿è¡Œæ—¶é”™è¯¯ï¼‰
    Running --> Timeout: è¶…æ—¶
    Running --> Crashed: Executor å´©æºƒ
    Crashed --> Running: è‡ªåŠ¨é‡è¯•ï¼ˆæœ€å¤š 3 æ¬¡ï¼‰
    Crashed --> Failed: é‡è¯•æ¬¡æ•°è€—å°½
    Completed --> [*]
    Failed --> [*]
    Timeout --> [*]
```

**çŠ¶æ€è¯´æ˜**:

| çŠ¶æ€ | è¯´æ˜ | å¯å¦é‡è¯• |
|------|------|----------|
| `pending` | å·²åˆ›å»ºï¼Œç­‰å¾… Executor æ¥æ”¶ | æ˜¯ |
| `running` | Executor æ­£åœ¨æ‰§è¡Œ | å¦ |
| `completed` | æ‰§è¡ŒæˆåŠŸå®Œæˆ | å¦ |
| `failed` | æ‰§è¡Œå¤±è´¥ï¼ˆç”¨æˆ·ä»£ç é”™è¯¯ï¼‰ | å¦ |
| `timeout` | æ‰§è¡Œè¶…æ—¶ | å¯é€‰ï¼ˆç”±è°ƒç”¨æ–¹å†³å®šï¼‰ |
| `crashed` | Executor è¿›ç¨‹å´©æºƒ | æ˜¯ï¼ˆè‡ªåŠ¨é‡è¯•ï¼‰ |

#### 4.3.2 å¹‚ç­‰æ€§ä¿è¯

**At-Least-Once è¯­ä¹‰**:
- ç³»ç»Ÿä¿è¯æ¯ä¸ªæ‰§è¡Œè¯·æ±‚**è‡³å°‘è¢«æ‰§è¡Œä¸€æ¬¡**
- åœ¨ç½‘ç»œæ•…éšœã€Executor å´©æºƒç­‰åœºæ™¯ä¸‹å¯èƒ½ä¼šæ‰§è¡Œå¤šæ¬¡
- è°ƒç”¨æ–¹åº”è®¾è®¡å¹‚ç­‰å¤„ç†é€»è¾‘

**Exactly-Once è¯­ä¹‰ï¼ˆæœ‰é™ä¿è¯ï¼‰**:
- åœ¨æ­£å¸¸æƒ…å†µä¸‹ï¼ˆæ— å´©æºƒã€æ— ç½‘ç»œåˆ†åŒºï¼‰ï¼Œæ¯ä¸ª execution_id åªæ‰§è¡Œä¸€æ¬¡
- é€šè¿‡æ•°æ®åº“å”¯ä¸€çº¦æŸå’ŒçŠ¶æ€æœºä¿è¯ï¼š
  ```sql
  CREATE UNIQUE INDEX idx_execution_id ON executions(id);
  ```

**å¹‚ç­‰æ€§å»ºè®®**:
1. **è°ƒç”¨æ–¹å±‚é¢**:
   - å¯¹äºæœ‰å‰¯ä½œç”¨çš„æ“ä½œï¼ˆå¦‚å†™æ–‡ä»¶ï¼‰ï¼Œåº”å…ˆæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
   - ä½¿ç”¨å¹‚ç­‰é”®ï¼ˆidempotency keyï¼‰å»é‡

2. **å¹³å°å±‚é¢**:
   - ç›¸åŒ execution_id çš„é‡å¤æäº¤è¿”å›å·²æœ‰ç»“æœ
   - æ–‡ä»¶å†™å…¥ä½¿ç”¨åŸå­æ“ä½œï¼ˆé‡å‘½åè€Œéè¦†ç›–ï¼‰

```python
# ç¤ºä¾‹ï¼šå¹‚ç­‰æ–‡ä»¶å†™å…¥
def write_output(filename: str, content: str):
    tmp_file = f"{filename}.tmp.{uuid4()}"
    with open(tmp_file, 'w') as f:
        f.write(content)
    os.rename(tmp_file, filename)  # åŸå­æ“ä½œ
```

#### 4.3.3 é‡è¯•æœºåˆ¶

**è‡ªåŠ¨é‡è¯•æ¡ä»¶**:
- Executor è¿›ç¨‹å´©æºƒï¼ˆexit_code = -1 æˆ–ä¿¡å·ç»ˆæ­¢ï¼‰
- ç½‘ç»œé€šä¿¡å¤±è´¥ï¼ˆè¶…è¿‡ 3 æ¬¡å¿ƒè·³è¶…æ—¶ï¼‰
- å®¹å™¨å¼‚å¸¸é€€å‡ºï¼ˆéç”¨æˆ·ä»£ç å¯¼è‡´çš„å¤±è´¥ï¼‰

**é‡è¯•ç­–ç•¥**:
```python
class RetryPolicy:
    max_attempts: int = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
    backoff_base: float = 1.0  # é€€é¿åŸºæ•°ï¼ˆç§’ï¼‰
    backoff_factor: float = 2.0  # é€€é¿å› å­
    max_backoff: float = 10.0  # æœ€å¤§é€€é¿æ—¶é—´

    def get_delay(attempt: int) -> float:
        """è®¡ç®—ç¬¬ N æ¬¡é‡è¯•çš„å»¶è¿Ÿæ—¶é—´"""
        delay = backoff_base * (backoff_factor ** (attempt - 1))
        return min(delay, max_backoff)

# é‡è¯•å»¶è¿Ÿåºåˆ—: 1s, 2s, 4s, 8s, 10s, 10s, ...
```

**ä¸é‡è¯•çš„åœºæ™¯**:
- ç”¨æˆ·ä»£ç é”™è¯¯ï¼ˆè¯­æ³•é”™è¯¯ã€ImportErrorã€NameError ç­‰ï¼‰
- è¶…æ—¶ï¼ˆtimeout çŠ¶æ€ï¼‰
- æ˜¾å¼å–æ¶ˆï¼ˆè°ƒç”¨æ–¹ä¸»åŠ¨ç»ˆæ­¢ï¼‰
- é‡è¯•æ¬¡æ•°å·²è¾¾ä¸Šé™

**é‡è¯•æ‰§è¡Œæµç¨‹**:
```python
async def retry_execution_if_needed(execution_id: str) -> bool:
    """åˆ¤æ–­å¹¶æ‰§è¡Œé‡è¯•"""
    execution = await db.get_execution(execution_id)

    if execution.status != ExecutionStatus.CRASHED:
        return False

    if execution.retry_count >= MAX_RETRY_ATTEMPTS:
        await mark_failed(execution_id, reason="Max retries exceeded")
        return False

    # è®¡ç®—é€€é¿å»¶è¿Ÿ
    delay = RetryPolicy.get_delay(execution.retry_count + 1)
    await asyncio.sleep(delay)

    # é‡æ–°è°ƒåº¦åˆ°ç›¸åŒ sessionï¼ˆå¤ç”¨ workspaceï¼‰
    await scheduler.resubmit(execution.session_id, execution_id)

    # æ›´æ–°é‡è¯•è®¡æ•°
    execution.retry_count += 1
    await db.commit()

    return True
```

#### 4.3.4 Executor å´©æºƒå¤„ç†

**å´©æºƒæ£€æµ‹æœºåˆ¶**:

1. **å¿ƒè·³æ£€æµ‹**:
   ```python
   # Executor æ¯ 5 ç§’å‘é€ä¸€æ¬¡å¿ƒè·³
   async def heartbeat_loop(execution_id: str):
       while True:
           await api.post(f"/internal/executions/{execution_id}/heartbeat")
           await asyncio.sleep(5)

   # Control Plane 15 ç§’æœªæ”¶åˆ°å¿ƒè·³åˆ™åˆ¤å®šä¸ºå´©æºƒ
   HEARTBEAT_TIMEOUT = 15
   ```

2. **å®¹å™¨çŠ¶æ€ç›‘æ§**:
   ```python
   # å¥åº·æ¢é’ˆæ£€æŸ¥å®¹å™¨çŠ¶æ€
   async def check_container_health(container_id: str):
       container = docker.containers.get(container_id)
       status = container.status

       if status in {"exited", "dead"}:
           return "crashed"
       elif status == "running":
           return "healthy"
       else:
           return "unknown"
   ```

**å´©æºƒæ¢å¤æµç¨‹**:
```mermaid
flowchart TD
    A[æ£€æµ‹åˆ°å´©æºƒ] --> B{å´©æºƒç±»å‹}
    B -->|å®¹å™¨é€€å‡º| C[æ ‡è®°ä¸º crashed]
    B -->|å¿ƒè·³è¶…æ—¶| D[æ£€æŸ¥å®¹å™¨çŠ¶æ€]

    D -->|å®¹å™¨æ­£å¸¸è¿è¡Œ| E[æ¢å¤å¿ƒè·³]
    D -->|å®¹å™¨å·²é€€å‡º| C

    C --> F{é‡è¯•æ¬¡æ•° < 3?}
    F -->|æ˜¯| G[å»¶è¿Ÿåé‡è¯•]
    F -->|å¦| H[æ ‡è®°ä¸º failed]

    G --> I[åˆ›å»ºæ–°å®¹å™¨/å¤ç”¨ session]
    I --> J[é‡æ–°æ‰§è¡Œç›¸åŒä»£ç ]
    J --> K{æ‰§è¡ŒæˆåŠŸ?}
    K -->|æ˜¯| L[æ ‡è®°ä¸º completed]
    K -->|å¦| C
```

**æ•°æ®ä¸€è‡´æ€§ä¿è¯**:

1. **æ‰§è¡Œç»“æœå¹‚ç­‰ä¸ŠæŠ¥**:
   ```python
   # Executor ä½¿ç”¨å¹‚ç­‰é”®ä¸ŠæŠ¥ç»“æœ
   async def report_result(execution_id: str, result: ExecutionResult):
       await api.post(
           f"/internal/executions/{execution_id}/result",
           json=result.dict(),
           headers={"Idempotency-Key": f"{execution_id}_result"}
       )
   ```

2. **Artifact æ–‡ä»¶åŸå­åŒ–**:
   - æ–‡ä»¶å…ˆå†™å…¥ä¸´æ—¶ç›®å½• `.tmp/{execution_id}/`
   - æ‰§è¡Œå®ŒæˆååŸå­ç§»åŠ¨åˆ° workspace
   - å´©æºƒæ—¶ä¸´æ—¶æ–‡ä»¶è‡ªåŠ¨æ¸…ç†

3. **æ•°æ®åº“äº‹åŠ¡éš”ç¦»**:
   ```sql
   -- ä½¿ç”¨ä¹è§‚é”é˜²æ­¢å¹¶å‘æ›´æ–°
   UPDATE executions
   SET status = 'completed',
       version = version + 1
   WHERE id = ? AND version = ?;
   ```

#### 4.3.5 æ‰§è¡Œç»“æœæŸ¥è¯¢

**æœ€ç»ˆä¸€è‡´æ€§**:
- æ‰§è¡Œå®Œæˆåç»“æœé€šå¸¸åœ¨ 100ms å†…å¯æŸ¥è¯¢
- åœ¨é«˜è´Ÿè½½ä¸‹å¯èƒ½æœ‰ 1-2 ç§’å»¶è¿Ÿ
- è°ƒç”¨æ–¹åº”ä½¿ç”¨è½®è¯¢æˆ– Webhook è·å–ç»“æœ

**æ¨èæŸ¥è¯¢æ¨¡å¼**:
```python
async def wait_for_result(execution_id: str, timeout: int = 60) -> ExecutionResult:
    """ç­‰å¾…æ‰§è¡Œç»“æœï¼ˆå¸¦è¶…æ—¶ï¼‰"""
    start = time.time()

    while True:
        result = await api.get(f"/api/v1/executions/{execution_id}")

        if result["status"] in {"completed", "failed", "timeout"}:
            return result

        if time.time() - start > timeout:
            raise TimeoutError(f"Execution {execution_id} query timeout")

        await asyncio.sleep(0.5)  # é€€é¿è½®è¯¢
```

#### 4.3.6 å¤±è´¥æ¢å¤è·¯å¾„

æœ¬èŠ‚æè¿°å„ç§æ•…éšœåœºæ™¯ä¸‹çš„æ¢å¤æœºåˆ¶ï¼Œç¡®ä¿ç³»ç»Ÿåœ¨å„ç§å¼‚å¸¸æƒ…å†µä¸‹çš„å¯ç”¨æ€§å’Œæ•°æ®ä¸€è‡´æ€§ã€‚

**æ•…éšœåˆ†ç±»**:

| æ•…éšœç±»å‹ | å½±å“èŒƒå›´ | æ¢å¤ç­–ç•¥ | æ•°æ®ä¸€è‡´æ€§ |
|----------|----------|----------|------------|
| Control Plane é‡å¯ | æ‰€æœ‰æ­£åœ¨è¿›è¡Œçš„è¯·æ±‚ | æ•°æ®åº“çŠ¶æ€æ¢å¤ + è¿è¡Œæ—¶é‡è¿ | å¼ºä¸€è‡´æ€§ |
| Executor å´©æºƒ | å•ä¸ªæ‰§è¡Œä»»åŠ¡ | è‡ªåŠ¨é‡è¯•ï¼ˆæœ€å¤š 3 æ¬¡ï¼‰ | At-Least-Once |
| Pod Eviction | æ•´ä¸ªæ²™ç®± Pod | é€æ˜é‡å»º + å¤ç”¨ workspace | At-Least-Once |
| ç½‘ç»œåˆ†åŒº | éƒ¨åˆ†èŠ‚ç‚¹ä¸å¯è¾¾ | è‡ªåŠ¨é‡è·¯ç”± + è¶…æ—¶é‡è¯• | æœ€ç»ˆä¸€è‡´æ€§ |
| èŠ‚ç‚¹æ•…éšœ | èŠ‚ç‚¹ä¸Šæ‰€æœ‰ Pod | è°ƒåº¦åˆ°å…¶ä»–èŠ‚ç‚¹ + é‡å»º | æœ€ç»ˆä¸€è‡´æ€§ |
| æ•°æ®åº“æ•…éšœ | æ‰€æœ‰å…ƒæ•°æ®æ“ä½œ | åªè¯»æ¨¡å¼ + é‡è¯• | å¼ºä¸€è‡´æ€§ |

**åœºæ™¯ 1: Control Plane é‡å¯**

```mermaid
flowchart TD
    A[Control Plane é‡å¯] --> B[ä»æ•°æ®åº“æ¢å¤çŠ¶æ€]
    B --> C{æ£€æŸ¥ running çŠ¶æ€çš„ executions}

    C --> D[æŸ¥è¯¢ last_heartbeat_at]
    D --> E{å¿ƒè·³è¶…æ—¶?}

    E -->|æ˜¯| F[æ ‡è®°ä¸º crashed]
    E -->|å¦| G[ä¿æŒ running çŠ¶æ€]

    F --> H[è§¦å‘é‡è¯•é€»è¾‘]
    G --> I[ç­‰å¾… Executor ä¸ŠæŠ¥ç»“æœ]

    H --> J[æ£€æŸ¥ session å®¹å™¨çŠ¶æ€]
    J --> K{å®¹å™¨å­˜åœ¨?}
    K -->|æ˜¯| L[å¤ç”¨ç°æœ‰å®¹å™¨]
    K -->|å¦| M[åˆ›å»ºæ–°å®¹å™¨]

    L --> N[é‡æ–°æ‰§è¡Œä»£ç ]
    M --> N
```

**æ¢å¤æµç¨‹**:

1. **å¯åŠ¨æ—¶çŠ¶æ€æ¢å¤**:
   ```python
   async def recover_on_startup():
       """Control Plane å¯åŠ¨æ—¶æ¢å¤çŠ¶æ€"""

       # 1. æŸ¥æ‰¾æ‰€æœ‰ running çŠ¶æ€çš„æ‰§è¡Œ
       running_executions = await db.query(
           SELECT * FROM executions
           WHERE status = 'running'
       )

       for execution in running_executions:
           # 2. æ£€æŸ¥å¿ƒè·³æ—¶é—´
           if execution.last_heartbeat_at < heartbeat_threshold():
               # å¿ƒè·³è¶…æ—¶ï¼Œæ ‡è®°ä¸ºå´©æºƒ
               await mark_crashed(execution.id)
               # è§¦å‘é‡è¯•
               await retry_execution_if_needed(execution.id)

       # 3. æ£€æŸ¥ session å®¹å™¨çŠ¶æ€
       sessions = await db.query(
           SELECT * FROM sessions
           WHERE status = 'running'
       )

       for session in sessions:
           is_alive = await check_container_status(session.container_id)
           if not is_alive:
               # å®¹å™¨å·²æ¶ˆå¤±ï¼Œæ ‡è®°ä¸ºå¾…é‡å»º
               await mark_session_unhealthy(session.id)
   ```

2. **è¿è¡Œæ—¶è¿æ¥æ¢å¤**:
   ```python
   async def reconnect_runtime_nodes():
       """é‡æ–°è¿æ¥æ‰€æœ‰å®¹å™¨èŠ‚ç‚¹"""
       nodes = await db.query(SELECT * FROM runtime_nodes)

       for node in nodes:
           try:
               # å‘é€å¥åº·æ£€æŸ¥
               await node.health_check()
               node.status = "healthy"
           except Exception:
               node.status = "unhealthy"

       await db.commit()
   ```

**åœºæ™¯ 2: Pod Eviction / èŠ‚ç‚¹ Drain**

å½“ Kubernetes èŠ‚ç‚¹éœ€è¦ç»´æŠ¤ï¼ˆå¦‚å‡çº§å†…æ ¸ï¼‰æ—¶ï¼ŒPod ä¼šè¢«ä¸»åŠ¨é©±é€ã€‚

```mermaid
flowchart TD
    A[Kubernetes å‘å‡º Eviction ä¿¡å·] --> B[Pod æ”¶åˆ° SIGTERM]
    B --> C[Executor å°è¯•ä¼˜é›…å…³é—­]

    C --> D{æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡?}
    D -->|æ˜¯| E[æ ‡è®°å½“å‰æ‰§è¡Œä¸º crashed]
    D -->|å¦| F[ç›´æ¥é€€å‡º]

    E --> F
    F --> G[Pod è¢«åˆ é™¤]

    G --> H[Control Plane æ£€æµ‹åˆ° Pod æ¶ˆå¤±]
    H --> I[ä»æ•°æ®åº“æŸ¥è¯¢ session ä¿¡æ¯]

    I --> J[è°ƒåº¦åˆ°æ–°èŠ‚ç‚¹]
    J --> K[åˆ›å»ºæ–° Pod + æŒ‚è½½åŒä¸€ S3 workspace]

    K --> L[æ¢å¤ crashed çŠ¶æ€çš„ executions]
    L --> M[è‡ªåŠ¨é‡è¯•æ‰§è¡Œ]
```

**æ¢å¤æœºåˆ¶**:

1. **ä¼˜é›…å…³é—­å¤„ç†**:
   ```python
   # Executor æ”¶åˆ° SIGTERM æ—¶çš„å¤„ç†
   async def handle_shutdown():
       # 1. æ ‡è®°æ‰€æœ‰æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡ä¸º crashed
       running_executions = get_running_executions()
       for exec_id in running_executions:
           await mark_crashed_via_callback(exec_id)

       # 2. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
       cleanup_temp_files()

       # 3. æœ€å¤šç­‰å¾… 10 ç§’åå¼ºåˆ¶é€€å‡º
       await asyncio.sleep(10)
       sys.exit(143)  # SIGTERM exit code
   ```

2. **è·¨èŠ‚ç‚¹ä¼šè¯æ¢å¤**:
   ```python
   async def recover_session_on_eviction(session_id: str):
       """Pod é©±é€åæ¢å¤ä¼šè¯"""
       session = await db.get_session(session_id)

       # 1. è°ƒåº¦åˆ°æ–°çš„å¥åº·èŠ‚ç‚¹
       new_node = await scheduler.select_best_node(session.template_id)

       # 2. åœ¨æ–°èŠ‚ç‚¹ä¸Šåˆ›å»ºå®¹å™¨ï¼ŒæŒ‚è½½åŒä¸€ S3 workspace
       new_container_id = await new_node.create_container(
           session_id=session_id,
           workspace_path=session.workspace_path,  # å¤ç”¨ S3 è·¯å¾„
           template_id=session.template_id
       )

       # 3. æ›´æ–° session è®°å½•
       session.runtime_node = new_node.id
       session.container_id = new_container_id
       await db.commit()

       # 4. æ¢å¤æ‰€æœ‰ crashed çŠ¶æ€çš„æ‰§è¡Œ
       crashed_executions = await db.query(
           SELECT * FROM executions
           WHERE session_id = ? AND status = 'crashed'
       )

       for execution in crashed_executions:
           await retry_execution_if_needed(execution.id)
   ```

**åœºæ™¯ 3: ç½‘ç»œåˆ†åŒº**

ç½‘ç»œåˆ†åŒºå¯èƒ½å¯¼è‡´ Control Plane ä¸ å®¹å™¨èŠ‚ç‚¹ã€Executor ä¹‹é—´é€šä¿¡ä¸­æ–­ã€‚

```mermaid
flowchart TD
    A[ç½‘ç»œåˆ†åŒºå‘ç”Ÿ] --> B{å“ªäº›èŠ‚ç‚¹å—å½±å“?}

    B -->|å®¹å™¨èŠ‚ç‚¹ä¸å¯è¾¾| C[æ ‡è®°èŠ‚ç‚¹ä¸º unhealthy]
    B -->|Executor å¿ƒè·³ä¸¢å¤±| D[æ ‡è®°æ‰§è¡Œä¸º crashed]
    B -->|Control Plane ä¸å¯è¾¾| E[Executor æœ¬åœ°æŒä¹…åŒ–ç»“æœ]

    C --> F[åœæ­¢å‘è¯¥èŠ‚ç‚¹è°ƒåº¦æ–°ä»»åŠ¡]
    D --> G[è§¦å‘é‡è¯•é€»è¾‘]
    E --> H[ç½‘ç»œæ¢å¤åé‡è¯•ä¸ŠæŠ¥]

    F --> I[ç­‰å¾…ç½‘ç»œæ¢å¤]
    G --> I
    H --> I

    I --> J{ç½‘ç»œæ¢å¤?}
    J -->|æ˜¯| K[æ¢å¤æ­£å¸¸è°ƒåº¦]
    J -->|å¦| L[è¶…æ—¶åæ ‡è®°ä¸º failed]
```

**å¤„ç†ç­–ç•¥**:

1. **è¶…æ—¶ä¸é‡è¯•é…ç½®**:
   ```python
   class NetworkConfig:
       # HTTP å®¢æˆ·ç«¯é…ç½®
       connect_timeout: float = 5.0  # è¿æ¥è¶…æ—¶
       read_timeout: float = 30.0    # è¯»å–è¶…æ—¶
       max_retries: int = 3          # æœ€å¤§é‡è¯•æ¬¡æ•°

       # å¿ƒè·³é…ç½®
       heartbeat_interval: float = 5.0    # å¿ƒè·³é—´éš”
       heartbeat_timeout: float = 15.0    # å¿ƒè·³è¶…æ—¶

       # èŠ‚ç‚¹å¥åº·æ£€æŸ¥
       health_check_interval: float = 10.0
       node_unhealthy_threshold: int = 3  # è¿ç»­å¤±è´¥æ¬¡æ•°é˜ˆå€¼
   ```

2. **Executor æœ¬åœ°æŒä¹…åŒ–**:
   ```python
   # Executor åœ¨ Control Plane ä¸å¯è¾¾æ—¶æœ¬åœ°ä¿å­˜ç»“æœ
   async def report_result_with_fallback(execution_id: str, result: ExecutionResult):
       try:
           await api.post(f"/internal/executions/{execution_id}/result", json=result)
       except Exception as e:
           # ç½‘ç»œå¤±è´¥ï¼Œæœ¬åœ°æŒä¹…åŒ–
           local_path = f"/tmp/results/{execution_id}.json"
           with open(local_path, 'w') as f:
               json.dump(result.dict(), f)

           # åå°é‡è¯•ä»»åŠ¡
           asyncio.create_task(retry_report_when_available(execution_id, local_path))

   async def retry_report_when_available(execution_id: str, local_path: str):
       while True:
           try:
               with open(local_path, 'r') as f:
                   result = json.load(f)
               await api.post(f"/internal/executions/{execution_id}/result", json=result)
               os.remove(local_path)  # ä¸ŠæŠ¥æˆåŠŸï¼Œåˆ é™¤æœ¬åœ°æ–‡ä»¶
               break
           except Exception:
               await asyncio.sleep(5)  # 5 ç§’åé‡è¯•
   ```

**åœºæ™¯ 4: æ•°æ®åº“æ•…éšœ**

```mermaid
flowchart TD
    A[æ£€æµ‹åˆ°æ•°æ®åº“æ•…éšœ] --> B{æ•…éšœç±»å‹}

    B -->|è¿æ¥å¤±è´¥| C[åˆ‡æ¢åˆ°å¤‡ç”¨æ•°æ®åº“]
    B -->|ä¸»ä»åˆ‡æ¢| D[æ›´æ–°è¿æ¥æ± æŒ‡å‘æ–°ä¸»åº“]
    B -->|å®Œå…¨ä¸å¯ç”¨| E[è¿›å…¥åªè¯»é™çº§æ¨¡å¼]

    C --> F[é‡è¯•å¤±è´¥çš„æ“ä½œ]
    D --> F
    E --> G[æ‹’ç»å†™æ“ä½œ<br/>å…è®¸è¯»ç¼“å­˜]

    F --> H{æœåŠ¡æ¢å¤?}
    H -->|æ˜¯| I[æ¢å¤æ­£å¸¸æœåŠ¡]
    H -->|å¦| J[è¿”å› 503 Service Unavailable]

    G --> K[ç­‰å¾…æ•°æ®åº“æ¢å¤]
    K --> I
```

**é™çº§ç­–ç•¥**:

1. **åªè¯»æ¨¡å¼**:
   ```python
   class DatabaseManager:
       def __init__(self):
           self.read_only_mode = False
           self.cache = TTLCache(maxsize=1000, ttl=60)  # 1 åˆ†é’Ÿç¼“å­˜

       async def execute_write(self, query, params):
           if self.read_only_mode:
               raise ServiceUnavailable("Database in read-only mode")

           return await self.db.execute(query, params)

       async def execute_read(self, query, params):
           # ä¼˜å…ˆä»ç¼“å­˜è¯»å–
           cache_key = f"{query}:{params}"
           if cached := self.cache.get(cache_key):
               return cached

           result = await self.db.execute(query, params)
           self.cache[cache_key] = result
           return result
   ```

**åœºæ™¯ 5: S3 å¯¹è±¡å­˜å‚¨æ•…éšœ**

```mermaid
flowchart TD
    A[S3 ä¸ŠæŠ¥/ä¸‹è½½å¤±è´¥] --> B{æ“ä½œç±»å‹}

    B -->|Executor ä¸ŠæŠ¥ç»“æœ| C[æœ¬åœ°æŒä¹…åŒ– + é‡è¯•]
    B -->|ä¸‹è½½ artifact| D[è¿”å›é¢„ç­¾å URL<br/>å®¢æˆ·ç«¯ç›´æ¥ä¸‹è½½]
    B -->|åˆ›å»º workspace| E[ä½¿ç”¨æœ¬åœ°ä¸´æ—¶å­˜å‚¨]

    C --> F{é‡è¯•æˆåŠŸ?}
    F -->|æ˜¯| G[æ¸…ç†æœ¬åœ°å‰¯æœ¬]
    F -->|å¦| H[ä¿ç•™ 24 å°æ—¶ååˆ é™¤]

    D --> I[ç»•è¿‡ Control Plane]
    E --> J[é™çº§è­¦å‘Š]
```

**å®¹é”™æœºåˆ¶**:

1. **æœ¬åœ°ä¸´æ—¶å­˜å‚¨**:
   ```python
   # S3 ä¸å¯ç”¨æ—¶ä½¿ç”¨æœ¬åœ°å­˜å‚¨
   class ArtifactStorage:
       def __init__(self):
           self.s3_client = boto3.client('s3')
           self.fallback_path = "/tmp/artifacts"

       async def upload(self, file_path: str, s3_path: str):
           try:
               await self.s3_client.upload_file(file_path, bucket, s3_path)
           except Exception:
               # é™çº§åˆ°æœ¬åœ°å­˜å‚¨
               local_path = os.path.join(self.fallback_path, s3_path)
               os.makedirs(os.path.dirname(local_path), exist_ok=True)
               shutil.copy(file_path, local_path)
               logger.warning(f"S3 unavailable, using local storage: {local_path}")
   ```

2. **é¢„ç­¾å URL ç›´æ¥ä¸‹è½½**:
   ```python
   # ç»•è¿‡ Control Planeï¼Œå®¢æˆ·ç«¯ç›´æ¥ä» S3 ä¸‹è½½
   async def get_artifact_download_url(session_id: str, file_path: str) -> str:
       s3_path = f"sessions/{session_id}/{file_path}"
       url = s3_client.generate_presigned_url(
           'get_object',
           Params={'Bucket': S3_BUCKET, 'Key': s3_path},
           ExpiresIn=3600  # 1 å°æ—¶æœ‰æ•ˆæœŸ
       )
       return url
   ```

**æ¢å¤æ—¶é—´ç›®æ ‡ (RTO)**:

| æ•…éšœåœºæ™¯ | RTO | RPO | è¯´æ˜ |
|----------|-----|-----|------|
| Control Plane é‡å¯ | < 30s | 0 | å†…å­˜çŠ¶æ€å¯ä»æ•°æ®åº“æ¢å¤ |
| Executor å´©æºƒ | < 10s | 0 | è‡ªåŠ¨é‡è¯•ï¼Œæœ€å¤š 3 æ¬¡ |
| Pod Eviction | < 60s | 0 | è·¨èŠ‚ç‚¹æ¢å¤ï¼Œå¤ç”¨ S3 workspace |
| ç½‘ç»œåˆ†åŒº | < 30s | 0 | è¶…æ—¶é‡è¯• + è‡ªåŠ¨é‡è·¯ç”± |
| æ•°æ®åº“æ•…éšœ | < 60s | 0 | ä¸»ä»åˆ‡æ¢ |
| S3 æ•…éšœ | N/A | > 0 | é™çº§åˆ°æœ¬åœ°å­˜å‚¨ |

**æœ€ä½³å®è·µå»ºè®®**:

1. **å®šæœŸå¥åº·æ£€æŸ¥**:
   - æ¯ 10 ç§’æ£€æŸ¥ä¸€æ¬¡ å®¹å™¨èŠ‚ç‚¹å¥åº·çŠ¶æ€
   - æ¯ 5 ç§’æ£€æŸ¥ä¸€æ¬¡ Executor å¿ƒè·³
   - ä½¿ç”¨ Kubernetes liveness/readiness probe

2. **ä¼˜é›…å…³é—­**:
   - Control Plane æ”¶åˆ° SIGTERM æ—¶ï¼š
     - åœæ­¢æ¥å—æ–°è¯·æ±‚
     - ç­‰å¾…æ­£åœ¨å¤„ç†çš„è¯·æ±‚å®Œæˆï¼ˆæœ€å¤š 30 ç§’ï¼‰
     - æŒä¹…åŒ–å†…å­˜çŠ¶æ€åˆ°æ•°æ®åº“

3. **ç›‘æ§å‘Šè­¦**:
   - ç›‘æ§å´©æºƒé‡è¯•ç‡ï¼ˆåº” < 1%ï¼‰
   - ç›‘æ§å¿ƒè·³è¶…æ—¶æ¬¡æ•°ï¼ˆåº” < 0.1%ï¼‰
   - ç›‘æ§èŠ‚ç‚¹ä¸å¥åº·æ¯”ä¾‹ï¼ˆåº” < 10%ï¼‰
   - å‘Šè­¦é˜ˆå€¼ï¼šè¿ç»­ 3 æ¬¡é‡è¯•å¤±è´¥

## 5. Python ä¾èµ–é…ç½®

### 5.1 æ ¸å¿ƒä¾èµ–

ä½¿ç”¨ MariaDB éœ€è¦ä»¥ä¸‹ Python åŒ…ï¼š

```txt
# requirements.txt

# Web æ¡†æ¶
fastapi>=0.104.0
uvicorn[standard]>=0.24.0
pydantic>=2.5.0
pydantic-settings>=2.1.0

# æ•°æ®åº“ç›¸å…³
sqlalchemy[asyncio]>=2.0.23
aiomysql>=0.2.0          # å¼‚æ­¥ MySQL/MariaDB é©±åŠ¨
alembic>=1.12.0          # æ•°æ®åº“è¿ç§»å·¥å…·

# HTTP å®¢æˆ·ç«¯
httpx>=0.25.0

# å®¹å™¨è¿è¡Œæ—¶
aiodocker>=0.21.0        # Docker API
kubernetes>=28.0.0       # K8s Python å®¢æˆ·ç«¯

# å¯¹è±¡å­˜å‚¨
boto3>=1.29.0            # S3 å…¼å®¹å­˜å‚¨

# å·¥å…·åº“
python-jose[cryptography]>=3.3.0  # JWT
python-multipart>=0.0.6
structlog>=23.2.0        # ç»“æ„åŒ–æ—¥å¿—
```

### 5.2 å¼€å‘ä¾èµ–

```txt
# requirements-dev.txt

# æµ‹è¯•
pytest>=7.4.0
pytest-asyncio>=0.21.0
pytest-cov>=4.1.0
httpx>=0.25.0            # ç”¨äºæµ‹è¯• API

# ä»£ç è´¨é‡
black>=23.11.0
flake8>=6.1.0
mypy>=1.7.0
isort>=5.12.0

# ç±»å‹å­˜æ ¹
types-redis>=4.6.0.11    # å¦‚æœéœ€è¦ä½¿ç”¨ Redis ä½œä¸ºç¼“å­˜å±‚
```

### 5.3 æ•°æ®åº“è¿ç§» (Alembic)

```python
# alembic/env.py
from asyncio import run
from sqlalchemy import pool
from sqlalchemy.ext.asyncio import async_engine_from_config
from alembic import context

# this is the Alembic Config object
config = context.config

# add your model's MetaData object here for 'autogenerate' support
from sandbox_control_plane.db.models import Base
target_metadata = Base.metadata

def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode."""
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
        compare_type=True,
    )

    with context.begin_transaction():
        context.run_migrations()


def do_run_migrations(connection):
    context.configure(
        connection=connection,
        target_metadata=target_metadata,
        compare_type=True,
    )

    with context.begin_transaction():
        context.run_migrations()


async def run_async_migrations():
    """Run migrations in 'online' mode with async connection."""
    connectable = async_engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    async with connectable.connect() as connection:
        await connection.run_sync(do_run_migrations)

    await connectable.dispose()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode."""
    run(run_async_migrations())


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
```

### 5.4 æ•°æ®åº“é…ç½®

```python
# config.py
from pydantic_settings import BaseSettings

class DatabaseSettings(BaseSettings):
    url: str = "mysql+aiomysql://sandbox:sandbox_pass@localhost:3306/sandbox"
    pool_size: int = 50
    max_overflow: int = 100
    pool_recycle: int = 3600
    pool_pre_ping: bool = True
    echo: bool = False

    class Config:
        env_prefix = "DB_"

class Settings(BaseSettings):
    database: DatabaseSettings = DatabaseSettings()
    s3_endpoint: str = "http://localhost:9000"
    runtime_mode: str = "docker"

    class Config:
        env_file = ".env"
```

## 6. å®‰å…¨è®¾è®¡
### 6.1 å¤šå±‚éš”ç¦»ç­–ç•¥

1. **å®¹å™¨çº§éš”ç¦»**
   - æ¯ä¸ªä¼šè¯ç‹¬ç«‹å®¹å™¨
   - ç¦ç”¨ç‰¹æƒæ¨¡å¼
   - åˆ é™¤æ‰€æœ‰ Linux Capabilities
   - é root ç”¨æˆ·è¿è¡Œ

2. **è¿›ç¨‹çº§éš”ç¦» (Bubblewrap)**
   - Namespace éš”ç¦»ï¼ˆPID, NET, MNT, IPC, UTSï¼‰
   - åªè¯»æ–‡ä»¶ç³»ç»Ÿ
   - ä¸´æ—¶ç›®å½• tmpfs
   - èµ„æºé™åˆ¶ï¼ˆulimitï¼‰

3. **ç½‘ç»œéš”ç¦»**
   - é»˜è®¤ NetworkMode=none
   - å¯é€‰ç™½åå•ç½‘ç»œç­–ç•¥
   - ä»£ç†æ‹¦æˆªæ•æ„Ÿè¯·æ±‚

4. **æ•°æ®éš”ç¦»**
   - ä¼šè¯é—´å®Œå…¨éš”ç¦»
   - æ•æ„Ÿæ•°æ®ç¯å¢ƒå˜é‡ä¼ é€’
   - æ‰§è¡Œç»“æœåŠ å¯†å­˜å‚¨

### 6.2 å®‰å…¨é…ç½®ç¤ºä¾‹
```yaml
# Docker å®‰å…¨é…ç½®
security_opt:
  - no-new-privileges
  - seccomp=default.json
cap_drop:
  - ALL
read_only_root_filesystem: true
user: "1000:1000"

# Bubblewrap é…ç½®
bwrap_args:
  - --ro-bind /usr /usr
  - --ro-bind /lib /lib
  - --tmpfs /tmp
  - --proc /proc
  - --dev /dev
  - --unshare-all
  - --die-with-parent
  - --new-session

# èµ„æºé™åˆ¶
resources:
  limits:
    cpu: "1"
    memory: "512Mi"
    ephemeral-storage: "1Gi"
  ulimits:
    nofile: 1024
    nproc: 128
```

## 7. æ€§èƒ½ä¼˜åŒ–

### 7.1 å¯åŠ¨ä¼˜åŒ–

**ä¸¤é˜¶æ®µé•œåƒåŠ è½½ï¼š**
```dockerfile
# Stage 1: åŸºç¡€é•œåƒï¼ˆé¢„çƒ­æ± ä½¿ç”¨ï¼‰
FROM python:3.11-slim as base
RUN apt-get update && apt-get install -y bubblewrap
COPY sandbox-executor /usr/local/bin/

# Stage 2: ç”¨æˆ·ä¾èµ–ï¼ˆè¿è¡Œæ—¶åŠ è½½ï¼‰
FROM base
COPY requirements.txt /tmp/
RUN pip install -r /tmp/requirements.txt
```

**é¢„çƒ­æ± é…ç½®ï¼š**
```python
WARM_POOL_CONFIG = {
    "default_template": {
        "target_size": 10,  # ç›®æ ‡æ± å¤§å°
        "min_size": 5,      # æœ€å°ä¿ç•™
        "max_idle_time": 300,  # æœ€å¤§ç©ºé—²æ—¶é—´ï¼ˆç§’ï¼‰
    },
    "high_frequency_template": {
        "target_size": 50,
        "min_size": 20,
    }
}
```

### 7.2 å¹¶å‘ä¼˜åŒ–

**å¼‚æ­¥å¤„ç†ï¼š**
```python
# FastAPI å¼‚æ­¥ç«¯ç‚¹
@app.post("/api/v1/sessions/{session_id}/execute")
async def execute_code(session_id: str, request: ExecuteRequest):
    session = await session_manager.get_session(session_id)
    
    # å¼‚æ­¥æ‰§è¡Œï¼Œç«‹å³è¿”å›
    execution_id = await executor.submit(session, request)
    
    return {"execution_id": execution_id, "status": "submitted"}

# æ‰¹é‡å¤„ç†
async def batch_create_sessions(requests: List[CreateSessionRequest]):
    tasks = [session_manager.create_session(req) for req in requests]
    return await asyncio.gather(*tasks)
```

**è¿æ¥æ± ï¼š**
```python
# HTTP è¿æ¥æ± 
http_client = httpx.AsyncClient(
    limits=httpx.Limits(max_connections=1000, max_keepalive_connections=100),
    timeout=httpx.Timeout(10.0)
)

# MariaDB è¿æ¥æ± ï¼ˆSQLAlchemy å¼‚æ­¥å¼•æ“ï¼‰
from sqlalchemy.ext.asyncio import create_async_engine

db_engine = create_async_engine(
    "mysql+aiomysql://sandbox:password@mariadb:3306/sandbox",
    pool_size=50,              # å¸¸é©»è¿æ¥æ± å¤§å°
    max_overflow=100,          # æœ€å¤§æº¢å‡ºè¿æ¥æ•°
    pool_recycle=3600,         # è¿æ¥å›æ”¶æ—¶é—´ï¼ˆé˜²æ­¢è¿æ¥è¢«æœåŠ¡ç«¯å…³é—­ï¼‰
    pool_pre_ping=True,        # è¿æ¥å‰ ping æ£€æµ‹å¯ç”¨æ€§
    pool_timeout=30,           # è·å–è¿æ¥è¶…æ—¶æ—¶é—´
    echo=False                 # ä¸è¾“å‡º SQL æ—¥å¿—
)
```

---

## 8. ç›‘æ§ä¸å¯è§‚æµ‹æ€§

### 8.1 æŒ‡æ ‡å®šä¹‰

**ç³»ç»ŸæŒ‡æ ‡ï¼š**
- `sandbox_sessions_total`: ä¼šè¯æ€»æ•°
- `sandbox_sessions_active`: æ´»è·ƒä¼šè¯æ•°
- `sandbox_executions_total`: æ‰§è¡Œæ€»æ•°
- `sandbox_execution_duration_seconds`: æ‰§è¡Œè€—æ—¶
- `sandbox_warm_pool_size`: é¢„çƒ­æ± å¤§å°
- `sandbox_runtime_cpu_usage`: è¿è¡Œæ—¶ CPU ä½¿ç”¨ç‡
- `sandbox_runtime_memory_usage`: è¿è¡Œæ—¶å†…å­˜ä½¿ç”¨ç‡

**ä¸šåŠ¡æŒ‡æ ‡ï¼š**
- `sandbox_cold_start_duration`: å†·å¯åŠ¨è€—æ—¶
- `sandbox_warm_start_duration`: çƒ­å¯åŠ¨è€—æ—¶
- `sandbox_failure_rate`: å¤±è´¥ç‡
- `sandbox_timeout_rate`: è¶…æ—¶ç‡

### 8.2 ç›‘æ§é›†æˆ

**æ—¥å¿—ç»“æ„åŒ–ï¼š**
```python
import structlog

logger = structlog.get_logger()

logger.info(
    "session_created",
    session_id=session.id,
    template_id=session.template_id,
    runtime_node=session.runtime_node,
    duration_ms=100
)
```

## 9. éƒ¨ç½²æ–¹æ¡ˆ

### 9.1 Docker Compose éƒ¨ç½²ï¼ˆå¼€å‘/å°è§„æ¨¡ï¼‰
```yaml
version: '3.8'

services:
  control-plane:
    build: ./control-plane
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=mysql+aiomysql://sandbox:sandbox_pass@mariadb:3306/sandbox
      - S3_ENDPOINT=http://minio:9000
      - RUNTIME_MODE=docker
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - mariadb
      - minio

  mariadb:
    image: mariadb:11.2
    ports:
      - "3306:3306"
    environment:
      - MYSQL_ROOT_PASSWORD=root_password
      - MYSQL_DATABASE=sandbox
      - MYSQL_USER=sandbox
      - MYSQL_PASSWORD=sandbox_pass
    volumes:
      - mariadb_data:/var/lib/mysql
      - ./init.sql:/docker-entrypoint-initdb.d/01-init.sql:ro
    command:
      - --character-set-server=utf8mb4
      - --collation-server=utf8mb4_unicode_ci
      - --max-connections=500
      - --innodb-buffer-pool-size=256M

  minio:
    image: minio/minio
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password
    volumes:
      - minio_data:/data

volumes:
  mariadb_data:
  minio_data:
```

æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬ `init.sql`:
```sql
-- åˆ›å»ºæ•°æ®åº“
CREATE DATABASE IF NOT EXISTS sandbox CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;

USE sandbox;

-- ä¼šè¯è¡¨
CREATE TABLE IF NOT EXISTS sessions (
    id VARCHAR(64) PRIMARY KEY,
    template_id VARCHAR(64) NOT NULL,
    status ENUM('creating', 'running', 'completed', 'failed', 'timeout', 'terminated') NOT NULL,
    runtime_type ENUM('docker', 'kubernetes') NOT NULL,
    runtime_node VARCHAR(128),           -- å½“å‰è¿è¡Œçš„èŠ‚ç‚¹ï¼ˆå¯ä¸ºç©ºï¼Œæ”¯æŒä¼šè¯è¿ç§»ï¼‰
    container_id VARCHAR(128),           -- å½“å‰å®¹å™¨ ID
    pod_name VARCHAR(128),               -- å½“å‰ Pod åç§°
    workspace_path VARCHAR(256),         -- S3 è·¯å¾„ï¼šs3://bucket/sessions/{session_id}/
    resources_cpu VARCHAR(16),
    resources_memory VARCHAR(16),
    resources_disk VARCHAR(16),
    env_vars JSON,
    timeout INT NOT NULL DEFAULT 300,
    last_activity_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,  -- æœ€åæ´»åŠ¨æ—¶é—´ï¼ˆç”¨äºè‡ªåŠ¨æ¸…ç†ï¼‰
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    completed_at TIMESTAMP NULL,
    INDEX idx_status (status),
    INDEX idx_template (template_id),
    INDEX idx_created (created_at),
    INDEX idx_runtime_node (runtime_node),  -- æ”¯æŒèŠ‚ç‚¹æ•…éšœæ—¶æŸ¥è¯¢ä¼šè¯
    INDEX idx_last_activity (last_activity_at)  -- æ”¯æŒè‡ªåŠ¨æ¸…ç†æŸ¥è¯¢
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- æ‰§è¡Œè®°å½•è¡¨
CREATE TABLE IF NOT EXISTS executions (
    id VARCHAR(64) PRIMARY KEY,
    session_id VARCHAR(64) NOT NULL,
    code TEXT NOT NULL,
    language VARCHAR(16) NOT NULL,
    status ENUM('pending', 'running', 'completed', 'failed', 'timeout', 'crashed') NOT NULL,
    stdout MEDIUMTEXT,
    stderr MEDIUMTEXT,
    exit_code INT,
    execution_time FLOAT,
    artifacts JSON,  -- Artifact å¯¹è±¡æ•°ç»„: [{"path": "...", "size": 123, "mime_type": "...", ...}]
    retry_count INT DEFAULT 0,  -- é‡è¯•æ¬¡æ•°
    last_heartbeat_at TIMESTAMP NULL,  -- æœ€åå¿ƒè·³æ—¶é—´
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    completed_at TIMESTAMP NULL,
    FOREIGN KEY (session_id) REFERENCES sessions(id) ON DELETE CASCADE,
    INDEX idx_session (session_id),
    INDEX idx_status (status),
    INDEX idx_created (created_at),
    INDEX idx_last_heartbeat (last_heartbeat_at)  -- æ”¯æŒå¿ƒè·³è¶…æ—¶æ£€æµ‹
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- æ¨¡æ¿è¡¨
CREATE TABLE IF NOT EXISTS templates (
    id VARCHAR(64) PRIMARY KEY,
    name VARCHAR(128) NOT NULL UNIQUE,
    image VARCHAR(256) NOT NULL,
    base_image VARCHAR(256),
    pre_installed_packages JSON,
    default_resources_cpu VARCHAR(16),
    default_resources_memory VARCHAR(16),
    default_resources_disk VARCHAR(16),
    security_context JSON,
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    INDEX idx_name (name)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- æ’å…¥é»˜è®¤æ¨¡æ¿
INSERT INTO templates (id, name, image, base_image, default_resources_cpu, default_resources_memory, default_resources_disk) VALUES
('python-basic', 'Python Basic', 'sandbox-python:3.11-basic', 'python:3.11-slim', '1', '512Mi', '1Gi'),
('python-datascience', 'Python Data Science', 'sandbox-python:3.11-datascience', 'python:3.11-slim', '2', '2Gi', '5Gi'),
('nodejs-basic', 'Node.js Basic', 'sandbox-nodejs:20-basic', 'node:20-alpine', '1', '512Mi', '1Gi')
ON DUPLICATE KEY UPDATE name=VALUES(name);
```


### 9.2 Kubernetes éƒ¨ç½²ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰

**MariaDB éƒ¨ç½²ï¼š**
```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mariadb
  namespace: sandbox-system
spec:
  serviceName: mariadb
  replicas: 1
  selector:
    matchLabels:
      app: mariadb
  template:
    metadata:
      labels:
        app: mariadb
    spec:
      containers:
      - name: mariadb
        image: mariadb:11.2
        ports:
        - containerPort: 3306
          name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mariadb-secret
              key: root-password
        - name: MYSQL_DATABASE
          value: sandbox
        - name: MYSQL_USER
          value: sandbox
        - name: MYSQL_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mariadb-secret
              key: user-password
        volumeMounts:
        - name: mariadb-storage
          mountPath: /var/lib/mysql
        - name: init-scripts
          mountPath: /docker-entrypoint-initdb.d
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "2"
            memory: "4Gi"
        livenessProbe:
          exec:
            command:
            - mysqladmin
            - ping
            - -h
            - localhost
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command:
            - mysql
            - -h
            - localhost
            - -u
            - sandbox
            - -p${MYSQL_PASSWORD}
            - -e
            - SELECT 1
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: init-scripts
        configMap:
          name: mariadb-init-scripts
  volumeClaimTemplates:
  - metadata:
      name: mariadb-storage
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 50Gi
---
apiVersion: v1
kind: Service
metadata:
  name: mariadb
  namespace: sandbox-system
spec:
  selector:
    app: mariadb
  ports:
  - port: 3306
    targetPort: 3306
  clusterIP: None
---
apiVersion: v1
kind: Secret
metadata:
  name: mariadb-secret
  namespace: sandbox-system
type: Opaque
data:
  root-password: cm9vdF9wYXNzd29yZF9jaGFuZ2VfbWU=  # Base64 encoded
  user-password: c2FuZGJveF9wYXNzd29yZF9jaGFuZ2VfbWU=
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: mariadb-init-scripts
  namespace: sandbox-system
data:
  01-init.sql: |
    CREATE DATABASE IF NOT EXISTS sandbox CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
    USE sandbox;

    CREATE TABLE IF NOT EXISTS sessions (
        id VARCHAR(64) PRIMARY KEY,
        template_id VARCHAR(64) NOT NULL,
        status ENUM('creating', 'running', 'completed', 'failed', 'timeout', 'terminated') NOT NULL,
        runtime_type ENUM('docker', 'kubernetes') NOT NULL,
        runtime_node VARCHAR(128),           -- å½“å‰è¿è¡Œçš„èŠ‚ç‚¹ï¼ˆå¯ä¸ºç©ºï¼Œæ”¯æŒä¼šè¯è¿ç§»ï¼‰
        container_id VARCHAR(128),           -- å½“å‰å®¹å™¨ ID
        pod_name VARCHAR(128),               -- å½“å‰ Pod åç§°
        workspace_path VARCHAR(256),         -- S3 è·¯å¾„ï¼šs3://bucket/sessions/{session_id}/
        resources_cpu VARCHAR(16),
        resources_memory VARCHAR(16),
        resources_disk VARCHAR(16),
        env_vars JSON,
        timeout INT NOT NULL DEFAULT 300,
        last_activity_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,  -- æœ€åæ´»åŠ¨æ—¶é—´ï¼ˆç”¨äºè‡ªåŠ¨æ¸…ç†ï¼‰
        created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
        completed_at TIMESTAMP NULL,
        INDEX idx_status (status),
        INDEX idx_template (template_id),
        INDEX idx_created (created_at),
        INDEX idx_runtime_node (runtime_node),  -- æ”¯æŒèŠ‚ç‚¹æ•…éšœæ—¶æŸ¥è¯¢ä¼šè¯
        INDEX idx_last_activity (last_activity_at)  -- æ”¯æŒè‡ªåŠ¨æ¸…ç†æŸ¥è¯¢
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
```

**ç®¡ç†ä¸­å¿ƒéƒ¨ç½²ï¼š**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sandbox-control-plane
  namespace: sandbox-system
spec:
  replicas: 3
  selector:
    matchLabels:
      app: control-plane
  template:
    metadata:
      labels:
        app: control-plane
    spec:
      serviceAccountName: sandbox-controller
      initContainers:
      - name: wait-for-mariadb
        image: mariadb:11.2
        command:
        - sh
        - -c
        - |
          until mysql -h mariadb -u sandbox -p${MYSQL_PASSWORD} -e "SELECT 1" 2>/dev/null; do
            echo "Waiting for MariaDB..."
            sleep 2
          done
        env:
        - name: MYSQL_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mariadb-secret
              key: user-password
      containers:
      - name: control-plane
        image: sandbox-control-plane:v1.0.0
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          value: "mysql+aiomysql://sandbox:$(MYSQL_PASSWORD)@mariadb:3306/sandbox"
        - name: MYSQL_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mariadb-secret
              key: user-password
        - name: RUNTIME_MODE
          value: "kubernetes"
        resources:
          requests:
            cpu: "500m"
            memory: "512Mi"
          limits:
            cpu: "2"
            memory: "2Gi"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: control-plane-service
  namespace: sandbox-system
spec:
  selector:
    app: control-plane
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
```

**RBAC é…ç½®ï¼š**
```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: sandbox-controller
  namespace: sandbox-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: sandbox-controller-role
rules:
- apiGroups: ["sandbox.ai"]
  resources: ["sandboxes", "sandboxtemplates"]
  verbs: ["get", "list", "watch", "create", "update", "delete"]
- apiGroups: [""]
  resources: ["pods", "pods/log"]
  verbs: ["get", "list", "watch", "create", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: sandbox-controller-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: sandbox-controller-role
subjects:
- kind: ServiceAccount
  name: sandbox-controller
  namespace: sandbox-system
```

**HPA è‡ªåŠ¨æ‰©ç¼©å®¹ï¼š**
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: control-plane-hpa
  namespace: sandbox-system
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: sandbox-control-plane
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

---